{"meta":{"title":"SylensHub","subtitle":"","description":"Blog from Sylens Wong","author":"Sylens Wong","url":"https://silenwang.github.io/en","root":"/en/"},"pages":[{"title":"","date":"2026-02-10T14:03:13.692Z","updated":"2026-02-10T14:03:13.692Z","comments":true,"path":"about/index.html","permalink":"https://silenwang.github.io/en/about/index.html","excerpt":"","text":"About MeThe About Me page has been rewrited! Now it uses tabs to collapse the content… to save space from being wasted too often~ Personal Profile Self-proclaimed: Silen Wang &#x2F; Sylens Wong, is a rosted egg that is gradually becoming happier! Interests: Playing games, playing games, and still damn playing! Games! Also watch anime, but recently I’m less and less interested in series…, In 2024, I started chasing series again! ダンジョン飯 is really amazing! Go watch it! Learning&#x2F;Working Path: Public Health -&gt; Molecular Epidemiology -&gt; Bioinformatics -&gt; Neoantigens -&gt; Organoids -&gt; All kinds of things emmmm… becoming more and more eclectic Word cloud updateded! But not as much differenct as I had imagined … GitHub: Education 2009.09-2014.06 School: Guangdong Pharmaceutical University (formerly Guangdong Pharmaceutical College)College: School of Public HealthMajor: Preventive MedicineDegree: Bachelor 2014.09-2017.06 School: Southern Medical UniversityCollege: School of Public HealthMajor: Epidemiology and Health StatisticsDegree: Master Work Experience2017.07-2018.082018.08-2019.072019.07-2024.042024.05-Present Employment: Bioinformatics Analysis Engineer at a bioinformatics service company in Tianjin Work Content: Human genomic next-generation sequencing data (WES &#x2F; WGS) quality control and analysis Variant result screening based on frequency, gene region, pathogenicity software prediction, and family inheritance patterns Localization of potentially harmful variants based on genome-wide association study (GWAS) results Customization of screening plans and presentation of results according to customer requirements Research on the current status of specific diseases Research and learn the basic knowledge and current research status of specific diseases to prepare for the development of related analysis modules Maintenance and upgrade of data delivery processes Employment: Bioinformatics Engineer at a genetic technology company in Hangzhou Work Content: Data Processing NGS data processing of cfDNA&#x2F;FFPE&#x2F;white blood cell samples Construction and testing of NGS analysis processes Development of tools for NGS analysis (Python &#x2F; R &#x2F; Julia &#x2F; Shell) and quality control of cell sample sequencing data Alignment and quality control of raw sequencing data (fastq) Processing of UMI data &#x2F; Molecular Barcode data Analysis Process Related Debugging of existing bioinformatics analysis processes Writing new analysis modules and integrating them into the existing pipeline Testing and debugging of analysis modules Deployment and dependency management of analysis software Performance testing of analysis software and comparison of analysis results Writing and maintaining process documentation Construction and testing of single-cell sequencing analysis processes Algorithm and Tool Development: Comparison and replication of UMI deduplication algorithms Development of tools for drawing quality control plots Development of variant result screening tools based on ML Performance optimization of existing tools (parallelization) 2019.07-2020.03 Bioinformatics Engineer at a biotechnology company in Hangzhou Research on NGS-based cfDNA analysis plans, construction of analysis workflows Statistical analysis of neoantigen-related data Maintenance of bioinformatics analysis programs Maintenance of the medication guidance report database 2020.03-2024.04 Bioinformatics Supervisor at a biotechnology company in Hangzhou Development of NGS-based HLA typing&#x2F;quantification modules Backend development of bioinformatics analysis tools based on flask Development of neoantigen screening modules Maintenance and upgrade of the main bioinformatics analysis workflow Writing and revising materials as required 2024.05-Present Senior Bioinformatics Analysis Engineer at a medical technology company in Hangzhou Development of WES&#x2F;RNA-Seq&#x2F;single‑cell analysis workflows Research, interpretation, formulation and execution of bioinformatics analysis plans according to customer requirements Construction, operation and maintenance of bioinformatics databases Operation and maintenance of bioinformatics servers Operation and maintenance of websites and system cloud servers Development, operation, maintenance, migration and automated testing of websites and internal laboratory management systems Operation and maintenance of WeChat mini‑programs Work-related SkillsCommonly Used Languages &amp; ToolsProgramming Languages Data Science &#x2F; Database Web IDE &#x2F; Dev DevOps Visualization Bioinformatics Operating Systems Programming SkillsPythonRWorkflow Setup Parallelization: Multiprocess Next‑generation sequencing file handling: Pysam, PyVCF, CyVCF, Biopython, Scanpy Web scraping: scrapy: Several small‑scale scraping project experiences, capable of using scrapy for static and non‑encrypted dynamic page scraping and data extraction Able to use splash, selenium with scrapy to scrape dynamic page data Data processing &#x2F; statistical computing: Pandas, numpy, math, scipy, statsmodels Automated testing: Pytest Plotting: Plotly, altair Interactive data display: Dash Image processing: pillow WebUI&#x2F;API development: FastAPI, Taipy, flask, gradio, nicegui Office automation: docxtpl, openpyxl, python‑docx Data processing: base, dplyr Parallelization: Parallel Plotting: ggplot2, ggpubr, ggthemes Web development: Shiny, plumber Linux system installation&#x2F;configuration management: Red‑hat family distribution operation&#x2F;maintenance, Debian family distribution installation&#x2F;configuration&#x2F;maintenance, Arch family distribution installation&#x2F;configuration&#x2F;maintenance HPC cluster management system: SUN Grid Engine use and maintenance experience Snakemake: Proficient in writing Snakefiles, proficient in using Snakemake for workflow construction and management, rich practical Snakemake experience Nextflow: Able to understand nextflow configuration files, experience in converting nextflow workflows to Snakemake workflows Container technology: Containerized development experience, able to configure development containers compliant with Devcontainer specifications for local or remote development Rich container usage and deployment experience, able to use mainstream container software such as Docker, Podman, Apptainer (formerly Singularity), able to write Dockerfiles and config files to build Docker or Sif containers (Example) Pixi&#x2F;Conda&#x2F;Mamba: Sufficient practical experience, able to use conda to quickly deploy analysis software and workflows Git: Experience using Git for project code&#x2F;document management, understands basic creation&#x2F;push&#x2F;merge methods, has used hook features to run some automated tasks DevOps: Understand basic DevOps concepts, able to use Github actions &#x2F; Azure Pipeline platforms for code management, automated testing and deployment Mastered Bioinformatics Software&#x2F;Modules emmm… I’ve used quite a few, and I haven’t used many of them in the past year… But I should be able to get reacquainted with them all within a week, hahahhahaha Data File ProcessingSingle‑Cell AnalysisVariant Detection & AnnotationOther Analyses Next‑generation sequencing data splitting: bcl2fastq BED file manipulation: bedtools Data quality control: fastp, fastQC, MultiQC Sequence alignment: bwa, bowtie, STAR Expression quantification&#x2F;analysis: DESeq2, DEXseq, Salmon, kallisto, htseq, Hisat2 Sequence processing: seqtk, seqkit BAM&#x2F;VCF file manipulation: Picard, Samtools, Bcftools, Sambamba, Pysam, Biogo UMI processing: UMI‑tools, fgbio Analysis frameworks: cellranger, rhapsody, Scanpy, Seurat Cellular composition differences: Milopy, miloR Cell annotation: SingleR, celltypist, Azimuth Pseudotime: Monocle, scvelo, cellrank, pymonocle Cell‑cell communication: Cellchat SNP&#x2F;INDEL detection: Samtools, GATK(Mutect2), Vardict, Vacscan, Strelka DNA structural variation &#x2F; fusion gene detection: CREAST, lumpy‑sv, Smoove, SViCT, SvABA, Manta, Delly, GeneFuse, Factera, sv‑tools CNV detection: CNVkit, PyLOH Variant annotation: ANNOVAR, snpEff, VEP Clonality: PyClone, SciClone, FastClone Sequencing data genetic consistency checking: plink, NGSCheckMate Mass spectrometry identification: pFind, maxquant Other Work Skills Patent draft writing Software copyright writing Research paper writing Static blog&#x2F;document writing hexo mkdocs gitbook Displayable ProjectsGithub Projects Published ArticlesI generally only take responsibility for the bioinformatics related parts in the following articles: Ye Feng et al., “Multi-Epitope Vaccine Design Using an Immunoinformatic Approach for SARS-CoV-2” Pathogens 10 (June 11, 2021) 737. Zheling Chen et al., “A Neoantigen-Based Peptide Vaccine for Patients With Advanced Pancreatic Cancer Refractory to Standard Treatment” Front. Immunol. 12 (2021) 691605. Jiawei Shou et al., “Combination Treatment of Radiofrequency Ablation and Peptide Neoantigen Vaccination: Promising Modality for Future Cancer Immunotherapy” Front. Immunol. 13 (September 29, 2022) 1000681."},{"title":"Category","date":"2026-02-10T14:03:13.692Z","updated":"2026-02-10T14:03:13.692Z","comments":true,"path":"categories/index.html","permalink":"https://silenwang.github.io/en/categories/index.html","excerpt":"","text":""},{"title":"My Friends","date":"2026-02-10T14:03:13.692Z","updated":"2026-02-10T14:03:13.692Z","comments":true,"path":"friends/index.html","permalink":"https://silenwang.github.io/en/friends/index.html","excerpt":"","text":""},{"title":"Tags","date":"2026-02-10T14:03:13.692Z","updated":"2026-02-10T14:03:13.692Z","comments":true,"path":"tags/index.html","permalink":"https://silenwang.github.io/en/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Trying to Package Opencode and Hitting Another Snag","slug":"想打包个opencode结果又遇坑了","date":"2026-01-31T15:54:18.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2026/01/31/想打包个opencode结果又遇坑了/","permalink":"https://silenwang.github.io/en/2026/01/31/%E6%83%B3%E6%89%93%E5%8C%85%E4%B8%AAopencode%E7%BB%93%E6%9E%9C%E5%8F%88%E9%81%87%E5%9D%91%E4%BA%86/","excerpt":"Last time I successfully packaged my own program, so I wanted to try something else. Recently opencode has been extremely popular, and I’ve been using it too. Since it’s not yet available on conda, I decided to package it.","text":"Last time I successfully packaged my own program, so I wanted to try something else. Recently opencode has been extremely popular, and I’ve been using it too. Since it’s not yet available on conda, I decided to package it. Packaging went smoothly, but…Opencode is written in TypeScript and uses the relatively new bun.js for management, which is not yet available on conda. Additionally, opencode itself already provides various installation packages to run in different environments, so this time I planned to follow the rattler‑build documentation and adopt a repackaging strategy—that is, instead of compiling from source, unpacking the pre‑built binary and repackaging it, similar to how some AUR packages convert .deb packages to Arch packages. Because the program is a single‑file executable, packaging is especially simple: just extract the executable and place it in the appropriate directory. 123456789101112131415161718192021222324252627282930313233343536373839package: name: opencode version: 1.1.35source: - if: target_platform == &quot;linux-64&quot; then: url: https://github.com/anomalyco/opencode/releases/download/v1.1.35/opencode-linux-x64.tar.gz sha256: 451f5a36e2875b5540adf55e8cc9e144902b44959a6f31899fc21876b38b31ae file_name: opencode.tar.gz - if: target_platform == &quot;linux-aarch64&quot; then: url: https://github.com/anomalyco/opencode/releases/download/v1.1.35/opencode-linux-arm64.tar.gz sha256: e7544ae14afb10e75d28a3623b1fd33d60e17f372106665566ca4e085c2b157b file_name: opencode.tar.gzbuild: number: 0 script: | tar xvf opencode.tar.gz mkdir -p $PREFIX/bin mv opencode $PREFIX/bin/opencode chmod 755 $PREFIX/bin/opencode requirements: build: - tartests: - script: - opencode -habout: homepage: https://opencode.ai/ license: MIT summary: &#x27;The open source coding agent.&#x27; description: | The open source coding agent. repository: https://github.com/anomalyco/opencode But here’s the problem: although the packaging itself went smoothly, unlike the previous dependency‑free Go program, the generated conda package couldn’t be used after installation—it reported a segmentation fault. Investigating the issueThus began a long discussion with AI. Eventually, guided by AI, I used rattler-build to unpack the conda package and discovered that the binary had been altered (md5sum didn’t match). Further investigation revealed that both rattler-build and conda-build have a special mechanism that adds extra library path settings to binary files. Using readelf -d opencode you can see that, compared to the original binary, the opencode inside the conda package had an extra Library rpath line, and this change made the program unusable. 12345678Dynamic section at offset 0xd588 contains 32 entries: Tag Type Name/Value 0x000000000000000f (RPATH) Library rpath: [$ORIGIN/../lib] 0x0000000000000001 (NEEDED) Shared library: [libc.so.6] 0x0000000000000001 (NEEDED) Shared library: [ld-linux-aarch64.so.1] 0x0000000000000001 (NEEDED) Shared library: [libpthread.so.0] 0x0000000000000001 (NEEDED) Shared library: [libdl.so.2] 0x0000000000000001 (NEEDED) Shared library: [libm.so.6] According to the documentation, this rpath modification behavior can be turned off: 1234567891011121314151617181920212223242526272829303132333435363738394041424344package: name: opencode version: 1.1.35source: - if: target_platform == &quot;linux-64&quot; then: url: https://github.com/anomalyco/opencode/releases/download/v1.1.35/opencode-linux-x64.tar.gz sha256: 451f5a36e2875b5540adf55e8cc9e144902b44959a6f31899fc21876b38b31ae file_name: opencode.tar.gz - if: target_platform == &quot;linux-aarch64&quot; then: url: https://github.com/anomalyco/opencode/releases/download/v1.1.35/opencode-linux-arm64.tar.gz sha256: e7544ae14afb10e75d28a3623b1fd33d60e17f372106665566ca4e085c2b157b file_name: opencode.tar.gzbuild: number: 0 script: | tar xvf opencode.tar.gz mkdir -p $PREFIX/bin mv opencode $PREFIX/bin/opencode chmod 755 $PREFIX/bin/opencode prefix_detection: ignore_binary_files: true dynamic_linking: binary_relocation: falserequirements: build: - tartests: - script: - opencode -habout: homepage: https://opencode.ai/ license: MIT summary: &#x27;The open source coding agent.&#x27; description: | The open source coding agent. repository: https://github.com/anomalyco/opencode But here’s the weird part: after making this change, the binary inside the package matched the original md5, yet installing the conda package still resulted in a segmentation fault. Checking the package installed in the environment showed that the rpath modification was still present. Because pixi itself is relatively new, I couldn’t find any discussion about this issue on my own, so I began a week‑long battle with AI… The final conclusion was that pixi’s global install mechanism works this way and there’s no way around it. However, that wasn’t actually the case… I suddenly realized that no matter how I changed the Recipe file, the generated package name seemed to stay the same… Both conda and pixi have caching mechanisms, so I tried clearing the cache… And then, it worked… Although I wasted quite a bit of time, at least the outcome is good… For now, relying solely on AI still isn’t entirely realistic…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[]},{"title":"Packaging an AI-Assisted Program with pixi and rattler-build","slug":"使用-pixi-和-rattler-build-打包AI帮我写的程序","date":"2026-01-25T20:50:24.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2026/01/25/使用-pixi-和-rattler-build-打包AI帮我写的程序/","permalink":"https://silenwang.github.io/en/2026/01/25/%E4%BD%BF%E7%94%A8-pixi-%E5%92%8C-rattler-build-%E6%89%93%E5%8C%85AI%E5%B8%AE%E6%88%91%E5%86%99%E7%9A%84%E7%A8%8B%E5%BA%8F/","excerpt":"Previously, I had tried modifying and adding recipes to conda channels. This time, I want to package my DevSSH and upload it to my own channel. I decided to try building a conda package myself.","text":"Previously, I had tried modifying and adding recipes to conda channels. This time, I want to package my DevSSH and upload it to my own channel. I decided to try building a conda package myself. Why choose pixi and rattler-build?This was actually a coincidence… I originally wanted to register an Anaconda account to use conda-build, but their website was acting up and I couldn’t complete the registration. So I switched directly to prefix.dev, which gave me a chance to learn about newer tools. Introduction to rattler-buildrattler-build is a next-generation conda package builder developed by the team behind pixi. Written in Rust, it naturally offers faster build speeds. It is somewhat compatible with conda-forge’s build system and has a certain level of integration with pixi. Configuring pixi.tomlpixi‘s core functionality is to set up development environments, but it can also be used to configure build environments for software packages. To do this, you need to use the package keyword and enable the experimental pixi-build feature within the [workspace] section. 12345678910111213141516171819202122232425[workspace]authors = [&quot;Sylens Wong &lt;qiumin14@163.com&gt;&quot;]channels = [&quot;conda-forge&quot;]name = &quot;DevSSH&quot;platforms = [&quot;linux-aarch64&quot;, &quot;linux-64&quot;]version = &quot;0.1.1&quot;preview = [&quot;pixi-build&quot;] # Need to enable pixi-build[tasks]build = &#123;cmd = &quot;go build -o bin/devssh cmd/devssh/main.go&quot;, cwd = &quot;./&quot;&#125;[activation.env]CGO_ENABLED = &quot;0&quot;[dependencies]go = &quot;&gt;=1.25.4,&lt;2&quot;# Below is the configuration for building the package[package]name = &quot;DevSSH&quot;version = &quot;0.1.1&quot;[package.build.backend]name = &quot;pixi-build-rattler-build&quot; # Building a Go program, no specialized builder, so use rattler-buildversion = &quot;0.3.*&quot; Writing a rattler-build reciperattler-build uses YAML‑format recipe files to define how a package is built. It is compatible with recipes from conda-build, though it does not support all conda-build recipe syntax. Here is the recipe I wrote for DevSSH: 123456789101112131415161718192021222324252627package: name: devssh version: 0.1.1source: path: . use_gitignore: true build: number: 0 script: | # Build the binary go build -o &quot;$PREFIX/bin/devssh&quot; cmd/devssh/main.gorequirements: build: - $&#123;&#123; compiler(&#x27;go-nocgo&#x27;) &#125;&#125; # My program is pure Go with no C dependencies, so choose the nocgo compiler - patchelf # Required for post‑processingabout: homepage: https://github.com/SilenWang/DevSSH license: MPL-2.0 license_file: LICENSE summary: &#x27;A CLI tool to quickly set up remote development tools over SSH&#x27; description: | A CLI tool to quickly set up remote development tools over SSH repository: https://github.com/SilenWang/DevSSH Building and uploading the package1. Local buildIf the configuration files are set up correctly, running the build command should produce a conda‑format package. 12345# Build with pixipixi build# Build for a specific target platformpixi build -t linux-64 If you encounter issues, you can ask AI step by step. However, note that rattler-build is not yet mainstream, so answers you get may often be based on conda-build. You’ll need to cross‑check with the documentation and experiment. 2. Setting up a prefix.dev account and uploadingBefore uploading, you need to log in, create a channel under your account, and set up an API_TOKEN. 12# Set the API tokenpixi auth login --token YOUR_API_TOKEN prefix.dev Once configured, you can directly upload the built package. With a good internet connection, it completes quickly. 1pixi upload prefix devssh-0.1.1-hb0f4dca_0.conda -c sylens 3. Installing the package via the channelYou can install it with either pixi or conda. 123pixi global install -c https://prefix.dev/sylens devsshconda install -c https://prefix.dev/sylens devssh","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"pixi","slug":"pixi","permalink":"https://silenwang.github.io/en/tags/pixi/"},{"name":"rattler-build","slug":"rattler-build","permalink":"https://silenwang.github.io/en/tags/rattler-build/"},{"name":"conda-build","slug":"conda-build","permalink":"https://silenwang.github.io/en/tags/conda-build/"},{"name":"Go","slug":"Go","permalink":"https://silenwang.github.io/en/tags/Go/"},{"name":"devssh","slug":"devssh","permalink":"https://silenwang.github.io/en/tags/devssh/"}]},{"title":"Cherry-pick is Everywhere","slug":"到处都有cherry-pick","date":"2026-01-20T00:05:43.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2026/01/20/到处都有cherry-pick/","permalink":"https://silenwang.github.io/en/2026/01/20/%E5%88%B0%E5%A4%84%E9%83%BD%E6%9C%89cherry-pick/","excerpt":"I wanted to contribute to the Fydetab Duo Wiki, but to preview the blog locally, I needed to add pixi or other configurations to the project. These changes were not suitable for the original repository. So I learned how to selectively submit commits, which is where git cherry-pick comes in.","text":"I wanted to contribute to the Fydetab Duo Wiki, but to preview the blog locally, I needed to add pixi or other configurations to the project. These changes were not suitable for the original repository. So I learned how to selectively submit commits, which is where git cherry-pick comes in. Git’s cherry-pickIn the Git version control system, the cherry-pick command is used to apply a specific commit to the current branch without merging the entire branch. This perfectly fits my needs: I forked the source code, created a branch (dev) for modifications, where the first two commits were files for local configuration, and the later ones were actual changes to the documentation. When I finished the modifications and was ready to submit, I could create a dedicated submission branch based on the unmodified original branch, then use cherry-pick to select the last three commits to include: 12345678# Apply specified commits to the current branchgit cherry-pick COMMIT3 COMMIT4 COMMIT5# If conflicts occur, resolve them and continuegit cherry-pick --continue# Or abort the cherry-pickgit cherry-pick --abort Then, using this branch to submit a PR ensures that unnecessary content isn’t pushed to the original project. At the same time, after the upstream merges my PR, I can pull the changes from upstream into the dev branch, allowing me to continue updating, and then modify and use the dedicated merge branch again. Cherry-picking in data analysisWhen I first saw cherry-pick, I found it quite interesting because I had also encountered this term in data‑analysis discussions, where it carries a more negative connotation—referring to “selective presentation.” Honestly, I’ve done quite a bit of this after starting work… It’s one of those “charming” moments in statistics… Also, I asked an AI, and it seems the phrase “cherry pick” has been around for a long time… Hmmm… Indeed, there’s nothing new under the sun…","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://silenwang.github.io/en/tags/Git/"},{"name":"Version Control","slug":"Version-Control","permalink":"https://silenwang.github.io/en/tags/Version-Control/"},{"name":"cherry-pick","slug":"cherry-pick","permalink":"https://silenwang.github.io/en/tags/cherry-pick/"}]},{"title":"Wait... Has Devpod Really Been Abandoned by Its Maintainers?","slug":"啊这-原来devpod已经被官方放弃了么","date":"2026-01-17T02:54:00.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2026/01/17/啊这-原来devpod已经被官方放弃了么/","permalink":"https://silenwang.github.io/en/2026/01/17/%E5%95%8A%E8%BF%99-%E5%8E%9F%E6%9D%A5devpod%E5%B7%B2%E7%BB%8F%E8%A2%AB%E5%AE%98%E6%96%B9%E6%94%BE%E5%BC%83%E4%BA%86%E4%B9%88/","excerpt":"It all started when I tried to use Devpod to set up a container based on Docker Compose, only to find that Devpod didn’t seem to be able to call Docker Compose correctly to create the container. I didn’t think much of it until I searched online… Wait, what? Has this project really been dropped by loft‑sh?","text":"It all started when I tried to use Devpod to set up a container based on Docker Compose, only to find that Devpod didn’t seem to be able to call Docker Compose correctly to create the container. I didn’t think much of it until I searched online… Wait, what? Has this project really been dropped by loft‑sh? According to issue 1946 and issue 1915, vCluster is loft‑sh’s most commercially successful and promising project, so all their energy is temporarily focused on it, leaving no time to maintain community projects. This situation feels similar to the Fyde team—they also mentioned on Discord that they would prioritize work that keeps the company running… Fortunately, a community hero has created a community fork of Devpod and is updating it at an impressive pace (already at version 0.9.*)! Using skevetter’s fork, Docker Compose works normally, which is a silver lining in an otherwise unfortunate situation. However, so far, the vast majority of the code has been contributed by skevetter alone, which isn’t really healthy. Just like with onelist before, the project became quite popular, but almost all development was concentrated on the original author. Relying on pure passion isn’t sustainable in the long run and partly led to the eventual sale and abandonment of that project. I hope Devpod can follow the path of Gitea, which originated from Gogs but eventually grew a complete community, allowing a good project to continue thriving.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"Devpod","slug":"Devpod","permalink":"https://silenwang.github.io/en/tags/Devpod/"},{"name":"Open Source Maintenance","slug":"Open-Source-Maintenance","permalink":"https://silenwang.github.io/en/tags/Open-Source-Maintenance/"},{"name":"Community Fork","slug":"Community-Fork","permalink":"https://silenwang.github.io/en/tags/Community-Fork/"},{"name":"Project Sustainability","slug":"Project-Sustainability","permalink":"https://silenwang.github.io/en/tags/Project-Sustainability/"},{"name":"Docker Compose","slug":"Docker-Compose","permalink":"https://silenwang.github.io/en/tags/Docker-Compose/"},{"name":"Open Source Software","slug":"Open-Source-Software","permalink":"https://silenwang.github.io/en/tags/Open-Source-Software/"}]},{"title":"Seriously, Bro, You Actually Wrote It?","slug":"不是哥们-你真写出来了","date":"2026-01-14T22:12:22.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2026/01/14/不是哥们-你真写出来了/","permalink":"https://silenwang.github.io/en/2026/01/14/%E4%B8%8D%E6%98%AF%E5%93%A5%E4%BB%AC-%E4%BD%A0%E7%9C%9F%E5%86%99%E5%87%BA%E6%9D%A5%E4%BA%86/","excerpt":"The new year has brought another AI surprise…","text":"The new year has brought another AI surprise… Last week I kept seeing recommendations for opencode, claiming it’s currently the best open‑source Claude Code alternative, with a pretty impressive success rate in writing executable code. Of course I was skeptical at first. After all, I’ve used aider and openhands—they’re both good tools, but when paired with DeepSeek, the results were often… let’s say mixed. Many times they couldn’t quite grasp what I meant. Even after tweaking prompts with other AIs, they still struggled with tasks that were just a little more complex. But with so many people praising it, I figured I’d give it a try. And wow—it actually exceeded my expectations. As I mentioned last year, I really wanted a tool that could quickly spin up VS Code on a remote machine, but due to limited time and skill, I hadn’t started building it before I discovered devpod, which solved about 70% of my needs… so I lost motivation to write my own… Back then I tried both aider and openhands to give me a basic framework. My requirements were fairly simple: automatically install VS Code server on a remote machine, forward the target port to my local machine, and try to follow devpod’s source code or directly use its interfaces. Neither worked well. I ended up having to read the code myself with Copilot’s help, but I understood it very slowly, and I couldn’t even get devpod’s interfaces to work properly… But openencode—it produced a working demo in one shot, using the same model, DeepSeek. Although it didn’t fully meet the requirement of closely following devpod’s approach, the fact that it delivered a runnable program that matched my functional description on the first attempt is, in my book, seriously impressive. After the first version, I asked openencode to make three more modifications. Except for one time when it stopped before finishing, the other two went smoothly. That really surprised me. Sure, it consumes tokens like crazy, but I’m using DeepSeek‑Chat—over the past year I’ve only spent about 25 yuan… This test cost me just two or three yuan, which I’m totally fine with. Also, after this trial, my opinion of Manus has changed a bit. Maybe they really do have something! It turns out that prompt and engineering optimizations can indeed bring noticeable improvements… As always, practice reveals the truth… It’s just… I wish the cost of trying things out could be a bit lower…","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"aider","slug":"aider","permalink":"https://silenwang.github.io/en/tags/aider/"},{"name":"opencode","slug":"opencode","permalink":"https://silenwang.github.io/en/tags/opencode/"},{"name":"AI programming","slug":"AI-programming","permalink":"https://silenwang.github.io/en/tags/AI-programming/"},{"name":"code generation","slug":"code-generation","permalink":"https://silenwang.github.io/en/tags/code-generation/"},{"name":"remote development","slug":"remote-development","permalink":"https://silenwang.github.io/en/tags/remote-development/"},{"name":"devpod","slug":"devpod","permalink":"https://silenwang.github.io/en/tags/devpod/"},{"name":"openhands","slug":"openhands","permalink":"https://silenwang.github.io/en/tags/openhands/"},{"name":"DeepSeek","slug":"DeepSeek","permalink":"https://silenwang.github.io/en/tags/DeepSeek/"},{"name":"automation tools","slug":"automation-tools","permalink":"https://silenwang.github.io/en/tags/automation-tools/"}]},{"title":"Trying to Submit a New Package to Bioconda/Conda‑Forge","slug":"尝试向bioconda-conda-forge提交新的包","date":"2026-01-11T17:15:43.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2026/01/11/尝试向bioconda-conda-forge提交新的包/","permalink":"https://silenwang.github.io/en/2026/01/11/%E5%B0%9D%E8%AF%95%E5%90%91bioconda-conda-forge%E6%8F%90%E4%BA%A4%E6%96%B0%E7%9A%84%E5%8C%85/","excerpt":"After my previous small contribution to a conda‑forge package, I wanted to take on something more advanced: trying to publish the singler‑py Python package into the conda ecosystem. Little did I know that it would turn out to be… somewhat troublesome.","text":"After my previous small contribution to a conda‑forge package, I wanted to take on something more advanced: trying to publish the singler‑py Python package into the conda ecosystem. Little did I know that it would turn out to be… somewhat troublesome. Rome Wasn’t Built in a DayI originally thought that setting up a recipe, submitting it once, and getting it approved would be enough. However, I encountered the following issues: I initially planned to submit directly to conda‑forge, but after reading the contribution guidelines, I discovered that software specifically aimed at bioinformatics should be submitted to the Bioconda channel first. After reading the Bioconda documentation and trying step by step, I found that many of the packages singler‑py depends on are not available in any conda channel. The AI informed me that the official recommendation is not to submit multiple new packages in the same PR, so I had to start from the bottom of the dependency tree, one by one. Parsing the dependencies level by level, I realized that a whole series of packages under BiocPy are not yet in the Conda ecosystem… After sorting out the dependencies, the build work required for singler‑py actually forms a layered dependency tree. Starting from the lowest level, biocutils, then moving up to biocframe, summarizedexperiment, and singlecellexperiment, and finally singler‑py itself. So far, the known dependency tree looks like this: 12345flowchart TD A[biocutils] --&gt; B[biocframe] B --&gt; E[summarizedexperiment] E --&gt; F[singlecellexperiment] F --&gt; G[singler‑py] As you can see, completing the conda release of singler‑py requires at least 4 levels of build work. Assuming a review cycle of 2‑3 working days per level, the entire chain would take about 3‑4 weeks. In reality, more dependent packages are likely to be discovered along the way… probably even longer. Different Recipe Workflows for conda‑forge and BiocondaSince I actually got to understand the workflows of both sides, I’ll document them here. A Brief Introduction to conda‑forge and Biocondaconda‑forge is a community‑led conda software package repository that covers a large number of open‑source software in general domains. Anyone can submit new recipes to its staged‑recipes repository via GitHub. After automated checks and maintainer review, the new package appears in the conda‑forge channel, available worldwide via conda install -c conda-forge &lt;package&gt;. In nature, it should be similar to the AUR. Bioconda is a conda channel focused on bioinformatics software, a repository specifically dedicated to biology‑related packages. The documentation also emphasizes that if a package does not specifically serve a biological purpose, it should be submitted to the conda‑forge channel. The tools for managing and generating recipes overlap between the two, but the actual recipe organization differs. conda‑forge provides a recipe template; after submitting a recipe, the CI workflow generates a feedstock repository, and subsequent maintenance is done in that feedstock repository—hence a “one‑package‑one‑repo” model. Bioconda, on the other hand, keeps all recipes in a single centralized repository, and you simply submit a PR directly to that repository. The difference probably stems from the number of packages: bioinformatics is a branch of data science, so the number of tools is much smaller than that of conda‑forge, and the number of people willing to maintain and contribute… is likely even smaller (having usable code is already good enough… don’t expect too much standardization and packaging). Tool PreparationFor this attempt, I temporarily used the conda toolset—after all, I’m a beginner following the official documentation… All the tools can be obtained via pixi by running: 1pixi global install --environment conda -c conda-forge -c bioconda conda conda-build bioconda-utils greyskull grayskull There’s a small Easter egg here: both greyskull and grayskull exist, but they actually point to the same grayskull. One page of the Bioconda documentation mentions greyskull—I wonder if there was a small mistake at the beginning of the project… Brief Steps for Submitting a New Package to conda‑forgeThe conda‑forge approach is to first submit a recipe, generate a feedstock, and then maintain the feedstock. Fork the RepositoryVisit conda‑forge&#x2F;staged‑recipes and click the “Fork” button to copy the repository to your own GitHub account. Prepare the Local EnvironmentClone your forked repository and create a new branch: 123git clone https://github.com/&lt;your-username&gt;/staged-recipes.gitcd staged-recipesgit checkout -b add-&lt;package-name&gt; Generate the Recipe FilesSince the package I submitted is already on PyPI, simply run grayskull pypi --strict-conda-forge &lt;package-name&gt;. Local ValidationUse the conda smithy tool to run lint and build tests locally (requires installing conda‑smithy first): 12conda smithy recipe-lint recipes/&lt;package-name&gt;/conda build recipes/&lt;package-name&gt;/ Submit a PRPush your changes to your fork, then open a Pull Request to conda‑forge/staged‑recipes via the GitHub interface. The CI will automatically run multi‑platform build tests, and maintainers will provide feedback in the comments until all checks pass and the PR is merged. 3. Brief Steps for Submitting a New Package to Bioconda Fork the RepositoryVisit bioconda&#x2F;bioconda‑recipes and fork it to your account. Prepare the Local EnvironmentClone the repository and create a branch (again, it’s recommended to include the package name in the branch name): 123git clone https://github.com/&lt;your-username&gt;/bioconda-recipes.gitcd bioconda-recipesgit checkout -b add-&lt;package-name&gt; Generate the Recipe FilesSimply run grayskull pypi &lt;package-name&gt;. Validate with bioconda‑utilsBioconda provides a dedicated toolchain for testing recipes: 12345# Run lint checksbioconda-utils lint recipes/&lt;package-name&gt;/# Build testconda build recipes/&lt;package-name&gt;/ Submit a PRPush to your fork and open a PR to bioconda/bioconda‑recipes. Bioconda’s CI will perform stricter bioinformatics software compatibility tests, and you’ll need to wait for maintainer review and approval. A Small DiscoveryWhile checking the syntax of the package build files, I noticed that conda has now introduced parallelism. Previously, downloading package configuration files for different platforms was serial; even with acceleration in mainland China, it would take several minutes, not to mention the subsequent dependency calculations. The introduction of parallel mechanisms is definitely an improvement (though I’ll still keep using Pixi). Additionally, I recall that Pixi uses newer package‑building tools, which might be faster than conda. Next time I submit something else, I’ll definitely give it a try. AfterwordBack when I was working in Tianjin (seven years ago already…), I thought about compiling conda packages myself to solve the problem of installing bioinformatics software without root privileges. However, the documentation at that time was completely incomprehensible to me… let alone writing it. Now, with highly automated tools and AI assistance… it’s finally possible to make some contributions. Practical Submission Attempt for BiocUtils 0.3.3Applying the aforementioned content, I attempted to submit BiocUtils to Bioconda. During the process, I gained a few somewhat awkward experiences: The documentation stating the need to specify a software license does not mean attaching the license file to the recipe itself, but rather specifying its location within the original project. Bioconda does not provide packages for older versions of Python, so there is no need to ensure compatibility for versions below 3.9. The content automatically parsed by conda-build is not always accurate. For example, BiocUtils’ original PyPI dependency specifies python &gt;= 3.9, but this constraint was not captured in the parsed recipe. Additionally, once a package is submitted to Bioconda, subsequent new versions can be automatically parsed based on updates to the source code. Therefore, unless there are specific issues, there is no need to update the recipe for every new release.","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"conda-forge","slug":"conda-forge","permalink":"https://silenwang.github.io/en/tags/conda-forge/"},{"name":"conda-build","slug":"conda-build","permalink":"https://silenwang.github.io/en/tags/conda-build/"},{"name":"Bioconda","slug":"Bioconda","permalink":"https://silenwang.github.io/en/tags/Bioconda/"},{"name":"singler-py","slug":"singler-py","permalink":"https://silenwang.github.io/en/tags/singler-py/"},{"name":"biocutils","slug":"biocutils","permalink":"https://silenwang.github.io/en/tags/biocutils/"},{"name":"biocframe","slug":"biocframe","permalink":"https://silenwang.github.io/en/tags/biocframe/"},{"name":"summarizedexperiment","slug":"summarizedexperiment","permalink":"https://silenwang.github.io/en/tags/summarizedexperiment/"},{"name":"singlecellexperiment","slug":"singlecellexperiment","permalink":"https://silenwang.github.io/en/tags/singlecellexperiment/"},{"name":"package management","slug":"package-management","permalink":"https://silenwang.github.io/en/tags/package-management/"},{"name":"dependency resolution","slug":"dependency-resolution","permalink":"https://silenwang.github.io/en/tags/dependency-resolution/"},{"name":"grayskull","slug":"grayskull","permalink":"https://silenwang.github.io/en/tags/grayskull/"}]},{"title":"Making a Small Contribution to Conda-forge","slug":"为conda-forge做一点小贡献","date":"2026-01-09T10:03:56.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2026/01/09/为conda-forge做一点小贡献/","permalink":"https://silenwang.github.io/en/2026/01/09/%E4%B8%BAconda-forge%E5%81%9A%E4%B8%80%E7%82%B9%E5%B0%8F%E8%B4%A1%E7%8C%AE/","excerpt":"Finally, aider-chat is available as a conda package, which means it can theoretically be installed globally via pixi global. However, during actual installation, you’ll find that one of its dependencies, tree_sitter_languages, doesn’t have a corresponding aarch64 version, causing the installation to fail. This made me wonder: could I rely on AI to solve this?","text":"Finally, aider-chat is available as a conda package, which means it can theoretically be installed globally via pixi global. However, during actual installation, you’ll find that one of its dependencies, tree_sitter_languages, doesn’t have a corresponding aarch64 version, causing the installation to fail. This made me wonder: could I rely on AI to solve this? What is Conda‑forge?To be honest, before this, I never fully understood the difference between Anaconda and the conda‑forge channel… After reading the documentation, I learned that Conda‑forge is a community‑driven Conda package distribution channel, while Anaconda is the official resource for conda (and if you use Anaconda casually in a commercial company, you might get warnings or be asked to pay…).Every package on Conda‑forge corresponds to a feedstock repository, which contains all the files needed to build that package, with the most crucial part being the recipe. What is a recipe?A recipe is a configuration file that instructs Conda on how to build a software package. It’s aptly named a “recipe,” and you can find more details in the official documentation. Since I wasn’t building a conda package from scratch this time, I didn’t actually need to modify the recipe. Preparing the feedstock repositoryAs mentioned, each software package corresponds to a feedstock repository. The tree_sitter_languages‑feedstock repository is under the conda‑forge organization. According to the official documentation, contributing to a repository is best done by first forking it to your own account, then creating a branch for modifications, and finally pushing changes back to the original repository. Modifying the filesBased on Aider’s analysis of this feedstock, there’s nothing particularly unsupported about aarch64 for this library. It’s likely that aarch64 support simply wasn’t considered when it was initially set up, and by default, feedstocks don’t automatically support aarch64 (probably because there are too few users to justify the effort…). Therefore, the modification is straightforward: just edit conda-forge.yml and add: 12provider: linux_aarch64: azure Afterward, following the documentation, I needed to manually run conda-smithy render -c auto. Conda would then automatically update the build scripts (truly automated—this is what professional CI&#x2F;CD looks like!). Submitting a PR to the original project for updatesThe final step was to commit the code to my forked repository, then submit a Pull Request (PR) from GitHub to the original project. The PR triggers the original repository’s CI, which automatically handles building and testing. Once the tests pass and the package maintainer accepts the PR, the package becomes searchable on conda. With this package, I can now directly run pixi global install aider-chat on my Fydetab Duo, and so far, I haven’t encountered any issues!","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"conda","slug":"conda","permalink":"https://silenwang.github.io/en/tags/conda/"},{"name":"conda-forge","slug":"conda-forge","permalink":"https://silenwang.github.io/en/tags/conda-forge/"},{"name":"tree_sitter_languages","slug":"tree-sitter-languages","permalink":"https://silenwang.github.io/en/tags/tree-sitter-languages/"},{"name":"PR","slug":"PR","permalink":"https://silenwang.github.io/en/tags/PR/"},{"name":"feedstock","slug":"feedstock","permalink":"https://silenwang.github.io/en/tags/feedstock/"}]},{"title":"Word Cloud Update","slug":"词云更新","date":"2026-01-08T01:22:04.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2026/01/08/词云更新/","permalink":"https://silenwang.github.io/en/2026/01/08/%E8%AF%8D%E4%BA%91%E6%9B%B4%E6%96%B0/","excerpt":"Those Bioinfo projects at my new workplace are mostly personalized, so each project inevitably involves reading some new papers. Once again, I’ve been feeling a headache from reading so many english papers. It occurred to me that the last time I created a word cloud was back in 2022. Three years have passed, and it’s time to welcome… well, just an update.","text":"Those Bioinfo projects at my new workplace are mostly personalized, so each project inevitably involves reading some new papers. Once again, I’ve been feeling a headache from reading so many english papers. It occurred to me that the last time I created a word cloud was back in 2022. Three years have passed, and it’s time to welcome… well, just an update. This update is relatively straightforward. After exporting the bibliography, I generated the word cloud again, but with a slight modification to the text used. Previously, I only extracted the abstract. This time, after reviewing the bibliography, I noticed that many entries only have titles and no abstracts. Moreover, paper abstracts tend to be somewhat formulaic. Therefore, this time I used keywords and titles: if keywords are available, use them; otherwise, use the title. 1234567with open(filepath, &#x27;r&#x27;) as bibliography_file: entries = rispy.load(bibliography_file) for entry in entries: if &#x27;keywords&#x27; in entry: text += &#x27; &#x27;.join(entry[&#x27;keywords&#x27;]) + &#x27; &#x27; else: text += entry[&#x27;title&#x27;] + &#x27; &#x27; Additionally, thanks to AI suggestions, I discovered the reason why the drawing with a mask failed previously. After adding Image.open(&quot;Sylens_Happy_Background.png&quot;).convert(&quot;L&quot;), the mask can now take effect correctly. The new version is shown below: For reference, the original version: Surprisingly… Hmm, the changes aren’t as significant as I expected. It seems that over the past year and a half, I’ve indeed been doing more IT-related work… The feeling of being overwhelmed by papers might be because projects often come in clusters.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"Word Cloud","slug":"Word-Cloud","permalink":"https://silenwang.github.io/en/tags/Word-Cloud/"},{"name":"Literature Management","slug":"Literature-Management","permalink":"https://silenwang.github.io/en/tags/Literature-Management/"},{"name":"Data Visualization","slug":"Data-Visualization","permalink":"https://silenwang.github.io/en/tags/Data-Visualization/"}]},{"title":"How AI Has Changed My Life This Year","slug":"这一年来AI给我带来的改变","date":"2026-01-06T22:30:10.000Z","updated":"2026-02-10T14:03:13.692Z","comments":true,"path":"2026/01/06/这一年来AI给我带来的改变/","permalink":"https://silenwang.github.io/en/2026/01/06/%E8%BF%99%E4%B8%80%E5%B9%B4%E6%9D%A5AI%E7%BB%99%E6%88%91%E5%B8%A6%E6%9D%A5%E7%9A%84%E6%94%B9%E5%8F%98/","excerpt":"From late 2022 to early 2023, ChatGPT exploded in popularity, and I started using LLMs to assist with script writing. Although it was quite useful, the price was relatively high, and payment was always an issue, so its application was limited to simple coding problems. From late 2024 to early 2025, DeepSeek became a hit. While its answers often weren’t entirely satisfactory, and the addition of a “thinking mode” made the response time a bit slower, it was really cheap! A year later, DeepSeek remains one of the most affordable models in terms of tokens while still delivering decent output. I paid 50 RMB at the beginning of the last year, and now, almost exactly a year later, I still have 27 RMB left… As a result, I’ve become much bolder in applying LLMs to various other areas.","text":"From late 2022 to early 2023, ChatGPT exploded in popularity, and I started using LLMs to assist with script writing. Although it was quite useful, the price was relatively high, and payment was always an issue, so its application was limited to simple coding problems. From late 2024 to early 2025, DeepSeek became a hit. While its answers often weren’t entirely satisfactory, and the addition of a “thinking mode” made the response time a bit slower, it was really cheap! A year later, DeepSeek remains one of the most affordable models in terms of tokens while still delivering decent output. I paid 50 RMB at the beginning of the last year, and now, almost exactly a year later, I still have 27 RMB left… As a result, I’ve become much bolder in applying LLMs to various other areas. From “Luxury” to “Daily Necessity”If using AI in the previous two years felt like an occasional luxury, this year it has completely transformed into a daily necessity in my workflow. DeepSeek’s low cost allows me to use it without hesitation for all sorts of tasks: from simple command queries, to entire script refactoring, to exploring completely unfamiliar domains. While it may not have achieved a leap in efficiency, it has genuinely expanded my capabilities, enabling me to tackle work that I previously only had conceptual ideas about but never practiced (and then became an even more diligent workhorse). Successful Use Cases This YearBatch Translation of Blog PostsUsing the command-line AI tool Aider, I wrote a simple shell script that automatically reads each article from the Chinese blog directory, translates it into English, and saves it to the corresponding English directory. The script completed the translation in just an hour and a half, but I spent a whole week of evenings manually proofreading… This is also why, despite having the idea to translate my blog into English in the past, I never followed through—without a script tool, copying and pasting article by article would have been exhausting. Additionally, that week of proofreading English translations reminded me of the time I spent preparing English clinical trial materials… Even though I’ve now read thousands of English professional papers and materials… I still find the process quite torturous… I guess that’s just a lack of language talent… Contributing Chinese Translations to the FydeTab Duo WikiBesides my own blog, I also contributed Chinese translations to the FydeTab Duo Wiki several times. Again, I used Aider for the initial translation, then performed a round of proofreading myself. This also helped me learn about the FydeTab Duo Wiki, as I hadn’t previously used the Wiki framework it use. Drafting Blog PostsStarting in the second half of this year, the number of blog posts I uploaded began to increase exponentially, thanks to the combination of Aider + DeepSeek. Since I write blogs mainly for my own records, without much stylistic requirement—the purpose of documentation far outweighs that of presentation—I can just outline the points, throw the relevant code or Notebook at them, generate a draft, and then edit it. This really saves time. Previously, I could spend an entire evening on a single blog post… Now, as long as I have a topic, writing five posts in one evening isn’t a problem… Combined with the earlier translation work, simultaneous Chinese and English publication is also feasible. Batch Generation of API TestsFaced with Swagger description files from multiple service groups, manually writing pytest test cases used to be a not‑difficult but quite time‑consuming task. Again, with the help of Aider, I only needed to provide the Swagger description files, and it could generate a runnable basic test code framework within minutes. I just needed to fine‑tune on that basis to quickly improve test coverage. When building a CI system for a system I took over in the first half of this year, the Aider + DeepSeek combination allowed me to complete the work much earlier than expected. Fixing Internationalization Issues in a Hexo ThemeWhile setting up the English site for my blog, I discovered that a small portion of the text in the Volantis theme was actually hard‑coded in configuration files or project code, and some interactive features failed under sub‑directory routing. By using DeepSeek to analyze browser console errors and the theme source code, I quickly pinpointed the root causes: Incorrect CDN path configuration caused app.js to fail loading; after fixing this, interactive functionality returned to normal. I replaced the fixed Chinese titles, descriptions, and other fields in the theme configuration file one by one with English equivalents. In the end, I resolved the bilingual support issue by generating and applying patches, without directly modifying the theme source repository. This would have been very hard in the past, as I haven’t really done full‑scale JS project development; reading code to find problems would have been quite laborious, but AI helped narrow down the inspection scope, making it possible to solve the problem in a short time. Configuring GitHub Actions for CI and CDThis is another good example. This year I gained experience with automation workflows on two platforms: Azure Pipeline and GitHub Actions. For the former, I wrote the configuration manually by following the documentation, which took about a full day, because the documentation was entirely in English, and the translation was rather poor (many of Microsoft’s Chinese documents feel as if there are no Chinese users at all, so they are all machine‑translated without optimization). GitHub Actions, on the other hand, was largely completed under AI guidance: generating a rough example and then me modifying it, basically taking 1–2 hours to finalize. Contributing ARM64 Packages to conda‑forgeWhen I discovered that tree_sitter_languages lacked a conda package for the ARM64 architecture, preventing aider‑chat from being installed on the Fydetab Duo (an ARM device), I decided to supplement it myself. Guided by AI, I: Learned about the feedstock mechanism of conda‑forge. Added linux_aarch64 build support in the corresponding conda‑forge.yml. Followed the community process to submit a PR, which successfully passed CI tests. Failures or Partial Failures This YearAI is obviously not omnipotent; its proposed solutions often require professional judgment, otherwise you can end up spinning your wheels, and many times I didn’t even know whether I was spinning or just hadn’t reached the end yet. Attempting to Modify Theia’s Remote ModuleI wanted to add remote development support to the browser‑based Theia IDE because using VSCode in the Linux subsystem on my FydeTab Duo was too laggy… Both Aider and OpenHands could provide code modification suggestions, but since I knew nothing about large Electron&#x2F;Node.js projects, I couldn’t judge whether those modifications were correct, nor how to debug them. In the end, this attempt came to nothing. Attempting to Update the FydeOS Kernel for Vulkan SupportTo improve graphics performance, I considered upgrading the FydeTab Duo’s kernel to a newer version in hopes of supporting Vulkan drivers. AI helped me find new kernel source code suitable for RK3588, but I couldn’t understand the overlay replacement mechanism during compilation. Eventually, I took a crude replacement approach, which resulted in bricking the device. Attempting to Write a Custom Application Based on DevPod’s Source CodeDevPod’s functionality fits my requirement of creating a development environment independent of FydeOS’s Linux subsystem, except that it must be based on Docker or Podman, and I currently cannot run Docker&#x2F;Podman outside the subsystem. So I once tried to have AI directly reference and reuse DevPod’s source code to write an application that connects via SSH, installs VSCode on a remote machine, and performs port forwarding. However, AI didn’t reference or reuse any of DevPod’s existing content; instead, it rewrote everything based on my description, and the resulting code didn’t run. Attempting to Compile and Run Podman on FydeOSI successfully used Pixi to manage dependencies and compiled an ARM64 version of Podman and its dependent libraries. However, at runtime, due to the SafeSetID security restriction in the FydeOS&#x2F;ChromeOS kernel, user namespace mapping couldn’t be completed, causing rootless Podman to ultimately fail to run. AI explained the origin of this issue to me, but to this day I’m not sure whether it was correct or not, nor how I should solve the problem. SummaryLooking back over this year, AI has become thoroughly integrated into my daily work and study. It has helped me complete a huge amount of repetitive labor, solved countless specific problems, and even given me the courage to venture into areas I previously wouldn’t have dared to touch casually. But at the same time, I’ve become more acutely aware that the effectiveness of AI depends heavily on the user’s knowledge base and judgment. Within an existing knowledge framework, AI can bring a certain efficiency boost. While it’s absolutely not to the level of being able to be a hands‑off manager as some vendors claim, there is definitely a real efficiency improvement. In areas where I have partial understanding, AI can greatly shorten the time from 0 to 1 breakthrough. This would have been hard to imagine in the past, but this year, getting started with many new topics actually took only half a day. In completely unfamiliar domains, without sufficient background knowledge to verify and filter AI’s output, one can only passively try each suggestion AI gives, which easily leads to spending a lot of time on fruitless efforts. Simply put, my own ceiling determines AI’s ceiling. In the end, the user still needs to study hard and improve themselves.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://silenwang.github.io/en/tags/AI/"},{"name":"DeepSeek","slug":"DeepSeek","permalink":"https://silenwang.github.io/en/tags/DeepSeek/"},{"name":"Automation","slug":"Automation","permalink":"https://silenwang.github.io/en/tags/Automation/"}]},{"title":"Building a Container for Resume Generation with RenderCV","slug":"搭一个用rendercv生成简历的容器","date":"2026-01-06T21:17:55.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2026/01/06/搭一个用rendercv生成简历的容器/","permalink":"https://silenwang.github.io/en/2026/01/06/%E6%90%AD%E4%B8%80%E4%B8%AA%E7%94%A8rendercv%E7%94%9F%E6%88%90%E7%AE%80%E5%8E%86%E7%9A%84%E5%AE%B9%E5%99%A8/","excerpt":"Preparing a resume may not sound difficult, but in practice, it can be quite time-consuming. Nowadays, many jobs have specific requirements for niche knowledge or projects. With so many job seekers, a resume that doesn’t highlight the key points relevant to the job requirements might be overlooked by employers. Therefore, to achieve better results, it’s best to tailor your resume for each position… This is something AI should be good at, but at least for now, I haven’t found a good free tool for this. Recently, I came across RenderCV, a tool that generates resumes from YAML configuration files. It enables a “configuration-as-resume” approach. Combining it with an AI coding assistant like Aider essentially creates a rapid resume preparation environment.","text":"Preparing a resume may not sound difficult, but in practice, it can be quite time-consuming. Nowadays, many jobs have specific requirements for niche knowledge or projects. With so many job seekers, a resume that doesn’t highlight the key points relevant to the job requirements might be overlooked by employers. Therefore, to achieve better results, it’s best to tailor your resume for each position… This is something AI should be good at, but at least for now, I haven’t found a good free tool for this. Recently, I came across RenderCV, a tool that generates resumes from YAML configuration files. It enables a “configuration-as-resume” approach. Combining it with an AI coding assistant like Aider essentially creates a rapid resume preparation environment. Components of the ContainerThe container’s components are straightforward: Pixi: Used to install dependencies, namely RenderCV and Aider Aider: Used to generate a rough draft of the resume configuration file, and can also be used directly for translation RenderCV: Generates the resume from the configuration file PDF Preview Extension: Allows viewing the generated PDF resume directly in VSCode. Combined with RenderCV’s --watch parameter, it also enables real-time preview of changes. Launching the Container Using GitHub Codespace: The most convenient way is to directly leverage GitHub’s Codespace. After forking my container project, you can prepare your resume in the web version of VSCode. Once generated, the resume can also be downloaded directly from the container to your local machine. Using Devpod: Using Devpod is also simple. With any provider set up, run devpod up https://github.com/SilenWang/RenderCV_Pod to start writing. Notes on Using Aider for Resume Configuration GenerationCurrently, I’m using the DeepSeek API for resume generation. Possibly because the RenderCV project itself is relatively new, DeepSeek clearly doesn’t know what the RenderCV configuration file should look like. It can produce files that appear plausible, but the section parts are completely incorrect. You still need to refer to RenderCV’s documentation to make corrections. Of course, RenderCV is designed to be simple and fast, so learning these adjustments takes only about 10 minutes and isn’t a major issue. Additionally, providing a complete example to the AI can largely avoid the aforementioned problems. I’ll supplement the project with examples when I have time.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Pixi","slug":"Pixi","permalink":"https://silenwang.github.io/en/tags/Pixi/"},{"name":"Development Environment","slug":"Development-Environment","permalink":"https://silenwang.github.io/en/tags/Development-Environment/"},{"name":"RenderCV","slug":"RenderCV","permalink":"https://silenwang.github.io/en/tags/RenderCV/"},{"name":"Container","slug":"Container","permalink":"https://silenwang.github.io/en/tags/Container/"},{"name":"Aider","slug":"Aider","permalink":"https://silenwang.github.io/en/tags/Aider/"}]},{"title":"Updating My Avatar","slug":"更新我的头像","date":"2025-12-28T17:34:00.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2025/12/28/更新我的头像/","permalink":"https://silenwang.github.io/en/2025/12/28/%E6%9B%B4%E6%96%B0%E6%88%91%E7%9A%84%E5%A4%B4%E5%83%8F/","excerpt":"I’ve been using my current avatar for over ten years. Ever since I started working in bioinformatics, I’ve wanted to add more complexity to it—half with circuit‑board patterns, the other half with DNA patterns, connected by a smooth transition in the middle to symbolize the transformation from biology to information, which fits my professional field. However, I’m not very skilled at photo editing, so I couldn’t achieve the desired effect. This year, the newly released image‑editing models gave me hope to realize my idea.","text":"I’ve been using my current avatar for over ten years. Ever since I started working in bioinformatics, I’ve wanted to add more complexity to it—half with circuit‑board patterns, the other half with DNA patterns, connected by a smooth transition in the middle to symbolize the transformation from biology to information, which fits my professional field. However, I’m not very skilled at photo editing, so I couldn’t achieve the desired effect. This year, the newly released image‑editing models gave me hope to realize my idea. Actually, when image‑generation models first became popular and the internet was flooded with AI‑generated beauty images, I tried every open‑source and commercial model I could get my hands on, attempting to draw at least a picture that transitions from DNA to circuit‑board patterns. In the end, only the image provided by Gemini was decent enough, and it has since become the banner of my blog. This year, I accidentally came across an introduction to Alibaba’s qwen‑image‑edit on WeChat and learned that there are models specifically designed for image editing, capable of combining different elements to create new images. This was perfect for my plan to update my avatar. So I found a simple front‑end on GitHub (I felt my needs didn’t require something as complex as ComfyUI), forked it myself, added model selection and multi‑image input, and started modifying my avatar. Before officially starting the image modifications, I used my modified little tool to test the effects of qwen‑image‑edit and wan2.5‑i2i‑preview. The actual tests showed that Wanxiang (万象) performed better, so all subsequent work was done using Wanxiang. The One-Step MethodThe examples in the qwen‑image‑edit project demonstrate quite powerful editing capabilities—from replacing clothes and modifying text to blending graphics. Therefore, I expected that simply providing a description would allow me to complete the modifications in one go. But, as expected, marketing is one thing, actual usage is another; the “seller’s show” and the “buyer’s show” are always different. Below is a collection of my all‑in‑one attempt results. The leftmost image in the first row is my original avatar, and the other three are results I obtained by writing simple prompts. My prompt was roughly a simple description like this: Figure 1 is a logo. I now want to turn the shape of this logo into a shape composed of specific patterns. The left half of the graphic should have circuit‑board patterns, and the right half should have DNA double‑helix patterns. After several attempts, the results were far from what I expected, so I turned to ChatGPT to optimize the prompt. The resulting prompt looked roughly like this: While strictly keeping the overall outline, proportions, and structure of the original icon completely unchanged, reconstruct the icon’s texture.The icon consists of stylized letters S and W, where the S is formed by upper and lower feather‑like arcs, and the W is located in the center, presenting a ring‑like dynamic structure.Modification requirements:The left half of the icon (including the corresponding S and W area outlines) should be filled with fine circuit‑board patterns:Include PCB traces, chip pads, and micro‑electronic lines.Lines should be clear, with a strong technological feel and engineering precision.The right half of the icon (including the corresponding S and W area outlines) should be filled with DNA double‑helix structure patterns:Show regular double‑helix twists.Include base‑pair ladder structures, conveying a bio‑tech and life‑science texture.Overall style requirements:All textures serve only as fill patterns and must not alter the original outer contour.The two textures on the left and right should transition naturally along the central axis without mixing.Maintain the icon’s simplicity and recognizability, suitable for a tech &#x2F; bioinformatics &#x2F; AI‑related brand logo.Keep the background solid or transparent.High contrast, vector‑like, clean, professional.Output a clear, modern icon effect that can be used in a brand visual system. With this more professional prompt, I could obtain images in the second row that came much closer to the effect I wanted. However, two issues arose during the process: The original graphic has two arcs that are lighter in color, and the model mostly seemed to ignore modifying those parts. The model couldn’t clearly distinguish the upper and lower parts I described—after all, these two parts aren’t strictly vertical or strictly horizontal… Step‑by‑Step MethodAfter failing to accomplish the task in one go, I eventually chose to complete it step by step (well, no wonder professional workflows need ComfyUI). First, I manually changed the colors of the original image to avoid the model skipping those areas. Then I separated the two parts that needed different textures myself, to prevent the model from misidentifying the two parts I mentioned, which would lead to inaccurate texture modifications. Finally, after obtaining the two parts separately, I combined them. During this process, another issue appeared: when drawing the DNA pattern on the lower part, the model couldn’t color it correctly, so I had to add a final coloring step. SummaryAlthough the process wasn’t very smooth and the final result isn’t 100% what I envisioned, it’s about 85% there. Moreover, the whole process was much faster than I expected—excluding the time spent modifying the tool, the graphical part actually took only about two hours. I’ve watched Naing, who creates artwork for Vtubers, work live; crafting a small pattern isn’t that simple… especially for something as intricate as the circuit‑board and DNA patterns I wanted. Given that, I’m quite satisfied with what the model could achieve.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://silenwang.github.io/en/tags/AI/"},{"name":"qwen-image-edit","slug":"qwen-image-edit","permalink":"https://silenwang.github.io/en/tags/qwen-image-edit/"},{"name":"wan2.5-i2i-preview","slug":"wan2-5-i2i-preview","permalink":"https://silenwang.github.io/en/tags/wan2-5-i2i-preview/"},{"name":"Image Editing","slug":"Image-Editing","permalink":"https://silenwang.github.io/en/tags/Image-Editing/"}]},{"title":"I always stumble on whatever I use – several issues with Devcontainer","slug":"我真是用什么都会踩坑-Devcontainer使用的若干问题","date":"2025-12-19T22:27:38.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2025/12/19/我真是用什么都会踩坑-Devcontainer使用的若干问题/","permalink":"https://silenwang.github.io/en/2025/12/19/%E6%88%91%E7%9C%9F%E6%98%AF%E7%94%A8%E4%BB%80%E4%B9%88%E9%83%BD%E4%BC%9A%E8%B8%A9%E5%9D%91-Devcontainer%E4%BD%BF%E7%94%A8%E7%9A%84%E8%8B%A5%E5%B9%B2%E9%97%AE%E9%A2%98/","excerpt":"I just wanted to set up a devcontainer environment to maintain the company website more efficiently, but I never expected to encounter three pitfalls in a single task…","text":"I just wanted to set up a devcontainer environment to maintain the company website more efficiently, but I never expected to encounter three pitfalls in a single task… BackgroundDevcontainer is a feature introduced by Visual Studio Code that allows us to quickly set up a development environment matching the project using Docker containers. This “code-as-environment” approach can greatly unify the team collaboration experience and reduce the “it works on my machine” embarrassment. The devpod I wrote earlier is an open‑source solution based on this tech. However, the ideal is plump, while reality always trips you up on the details. While configuring a Devcontainer for my company’s official website project recently, I ran into three unexpected problems in a row, each costing a fair amount of time to solve. Below I’ll document these issues and their solutions for future reference. Pitfall 1: Missing Git subtree commandProblem descriptionThe base Devcontainer images provided by Microsoft (Debian and Ubuntu) both come with Git, but the version is quite basic. It does not include the subtree subcommand… Well, this was the first time I learned that Git has so many features that some relatively new commands are made optional… But that’s not the real problem. The Git packages in Ubuntu and Debian repositories should be fully functional—outside the Devcontainer I’ve never seen a “subtree command not found” message. The issue appeared even after trying apt-get install git-man or git-all; the subtree functionality still wasn’t available. Why? Actually it’s simple: the Git that comes inside the Devcontainer is not from the distribution repositories; it’s located in /usr/local/bin, while the system‑installed Git would be placed in /usr/bin. Since /usr/local/bin has higher priority in the PATH, the Git we install via apt never gets used, and subtree remains missing. SolutionGiven the above, the fix is straightforward: install the distribution’s Git via apt and then remove the one that came with the image: 12345678910FROM mcr.microsoft.com/devcontainers/base:ubuntuRUN apt-get update &amp;&amp; \\ add-apt-repository -y ppa:git-core/ppa &amp;&amp; \\ apt-get update &amp;&amp; \\ apt-get install -y git vim &amp;&amp; \\ # remember to remove the original git from the container rm -r /usr/local/bin/git* &amp;&amp; \\ apt-get autoremove -y &amp;&amp; \\ apt-get clean -y Pitfall 2: Automatic line‑ending conversion on WindowsProblem descriptionWhen you run a Devcontainer on a Windows host using Docker Desktop, Git will automatically convert line endings from LF to CRLF according to the default setting of core.autocrlf (usually true). This is a common behavior in Windows development environments, but inside the container the file system is Linux‑style, so Git performs the conversion during checkout, causing almost every file to be marked as “modified”. The concrete symptom is: executing git status inside the container shows a large number of files as changed (even though you did nothing). This noise is not only confusing but may also affect subsequent commit operations. SolutionWell… I didn’t actually solve this problem, because tweaking Git configuration doesn’t stop VS Code from performing the conversion… This issue only occurs when using Docker Desktop on Windows. Anyway, compiling a website with Docker Desktop can even freeze the whole desktop, so just don’t use it… Stick to using a linux server as an SSH Provider. Pitfall 3: Devcontainer Desktop and remote SSH Provider disconnectionsProblem descriptionWhen trying to connect to a remote SSH Provider through Devpod Desktop, the connection turns out to be extremely unstable—it often drops within a few minutes, and the reconnection process is painfully slow. It disconnects again shortly after re‑establishing. Since Devpod Desktop doesn’t provide any runtime logs, I have no clue what’s wrong… SolutionSimple and brutal: use Devpod Desktop only for configuring Providers and workspaces, and let the DevPod CLI handle the actual connection. SummaryGood news: all problems were eventually solved.Bad news: I lost another seven or eight hours of sleep this week. May I not drop dead in 2025…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"git","slug":"git","permalink":"https://silenwang.github.io/en/tags/git/"},{"name":"docker","slug":"docker","permalink":"https://silenwang.github.io/en/tags/docker/"},{"name":"devpod","slug":"devpod","permalink":"https://silenwang.github.io/en/tags/devpod/"},{"name":"devcontainer","slug":"devcontainer","permalink":"https://silenwang.github.io/en/tags/devcontainer/"}]},{"title":"Pixi Advanced: Using Pixi as a Universal Management Tool for Data Science/Bioinformatics/Web Development","slug":"Pixi进阶-使用Pixi作为项目的万能管理工具","date":"2025-12-17T21:56:40.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2025/12/17/Pixi进阶-使用Pixi作为项目的万能管理工具/","permalink":"https://silenwang.github.io/en/2025/12/17/Pixi%E8%BF%9B%E9%98%B6-%E4%BD%BF%E7%94%A8Pixi%E4%BD%9C%E4%B8%BA%E9%A1%B9%E7%9B%AE%E7%9A%84%E4%B8%87%E8%83%BD%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/","excerpt":"Last year, when I joined my current company, I was tasked with preparing a teaching analysis environment for bioinformatics training. This opportunity introduced me to Pixi. Later, as my responsibilities shifted, I not only continued with bioinformatics analysis but also took on full‑stack maintenance work. The projects I handled spanned front‑end and back‑end, involving languages and frameworks beyond the data‑science staples R and Python. Pixi has still been able to serve as a cross‑language&#x2F;stack development environment management tool (Conda offers an extremely rich resource pool, covering mainstream programming languages and common frameworks). Therefore, I’d like to summarize the practical Pixi features that have proven useful in my daily work.","text":"Last year, when I joined my current company, I was tasked with preparing a teaching analysis environment for bioinformatics training. This opportunity introduced me to Pixi. Later, as my responsibilities shifted, I not only continued with bioinformatics analysis but also took on full‑stack maintenance work. The projects I handled spanned front‑end and back‑end, involving languages and frameworks beyond the data‑science staples R and Python. Pixi has still been able to serve as a cross‑language&#x2F;stack development environment management tool (Conda offers an extremely rich resource pool, covering mainstream programming languages and common frameworks). Therefore, I’d like to summarize the practical Pixi features that have proven useful in my daily work. 1. Handling Shared Dependencies Across Multiple EnvironmentsCurrently, about 70‑80% of the bioinformatic projects I work on involve single‑cell analysis. To this day, this field still has two dominant analysis ecosystems: Scanpy in Python and Seurat in R. Both ecosystems continue to produce cutting‑edge new analysis techniques, so using both is unavoidable, and consequently, multiple analysis environments are inevitable. All my projects are highly customized, so installing and testing tools from both ecosystems is a must. Single‑cell data are large, making interactive testing with tools like Jupyter Notebook essential. To avoid repeatedly configuring common packages every time a new environment is created, it’s necessary to set Jupyter and the required kernels as shared dependencies. Setting a Default EnvironmentAll packages configured under the [dependencies] key are placed in the default environment. Every package recorded in this environment will be included in dependency resolution for any other environment. 1234567891011121314151617181920212223[workspace]authors = [&quot;Sylens Wong &lt;silenseek14@gmail.com&gt;&quot;]channels = [&quot;conda-forge&quot;, &quot;bioconda&quot;]name = &quot;DEMO1&quot;platforms = [&quot;linux-64&quot;]version = &quot;0.1.0&quot;[environments]scanpy = [&#x27;scanpy&#x27;]seurat = [&#x27;seurat&#x27;][dependencies]ipykernel = &#x27;*&#x27;r-irkernel = &#x27;*&#x27;jupyterlab = &#x27;*&#x27;[feature.scanpy.dependencies]python = &#x27;3.*&#x27;scanpy = &#x27;*&#x27;[feature.seurat.dependencies]r-base = &#x27;4.*&#x27;r-seurat = &#x27;*&#x27; Setting a Common FeatureAnother approach is to define a common feature. In pixi.toml, we can define multiple environments ([environments]), each composed of a set of features. If different environments share common dependencies (e.g., ipykernel, r-irkernel, jupyterlab), we can place them in a feature named kernel and then reference that feature in each analysis environment. 1234567891011121314151617181920212223[workspace]authors = [&quot;Sylens Wong &lt;silenseek14@gmail.com&gt;&quot;]channels = [&quot;conda-forge&quot;, &quot;bioconda&quot;]name = &quot;DEMO1&quot;platforms = [&quot;linux-64&quot;]version = &quot;0.1.0&quot;[environments]scanpy = [&#x27;kernel&#x27;, &#x27;scanpy&#x27;]seurat = [&#x27;kernel&#x27;, &#x27;seurat&#x27;][feature.kernel.dependencies]ipykernel = &#x27;*&#x27;r-irkernel = &#x27;*&#x27;jupyterlab = &#x27;*&#x27;[feature.scanpy.dependencies]python = &#x27;3.*&#x27;scanpy = &#x27;*&#x27;[feature.seurat.dependencies]r-base = &#x27;4.*&#x27;r-seurat = &#x27;*&#x27; This way, both the scanpy and seurat environments automatically include the Jupyter kernel support provided by kernel, avoiding duplicate definitions. Moreover, this configuration is more modular: you can configure multiple package components and freely combine them into the environments you need. 2. Managing Simple Deployment Tasks and Task DependenciesBeyond dependency management, Pixi can also define tasks and specify dependencies between them. This essentially incorporates a very simple Make‑like workflow system, which is especially useful in projects that require sequential steps such as compilation, packaging, and deployment. In the [tasks] or [feature.xxx.tasks] section of pixi.toml, we can set up a series of tasks. Each task can declare dependencies using depends-on. Pixi ensures that all dependent tasks are executed in order. Below is an example from a bioinformatics analysis project where several Bioconductor data packages need to be installed sequentially: 12345[feature.rplot.tasks]GenomeInfoDbData = &#123;cmd = &#x27;Rscript -e &quot;BiocManager::install(\\&quot;GenomeInfoDbData\\&quot;)&quot;&#x27;&#125;BSgenome = &#123;cmd = &#x27;Rscript -e &quot;BiocManager::install(\\&quot;BSgenome.Hsapiens.UCSC.hg38\\&quot;)&quot;&#x27;&#125;EnsDb = &#123;cmd = &#x27;Rscript -e &quot;BiocManager::install(\\&quot;EnsDb.Hsapiens.v86\\&quot;)&quot;&#x27;&#125;r_dep = &#123;cmd = &#x27;echo &quot;bio dep for R done&quot;&#x27;, depends-on=[&#x27;GenomeInfoDbData&#x27;, &#x27;BSgenome&#x27;, &#x27;EnsDb&#x27;]&#125; When you run pixi run r_dep, Pixi will automatically execute the three installation tasks first, then output the completion message. Besides this approach of naming tasks as dependencies, Pixi also supports using files as markers to skip already‑completed steps—though the functionality is quite basic, and I haven’t used it much myself. 3. Specifying Sources for Particular PackagesSometimes we need to install a specific package from a particular channel (e.g., an internal channel) while other packages still come from the default channels. Pixi allows you to specify a channel attribute directly in the dependency declaration. Simply append &#123;version = &quot;*&quot;, channel = &quot;channel-name&quot;&#125; after the dependency. The only caveat is that channel-name must have been defined earlier in the channels list. 123456789[workspace]name = &quot;DEMO1&quot;version = &quot;1.1&quot;channels = [&quot;conda-forge&quot;, &quot;bioconda&quot;, &quot;dnachun&quot;]platforms = [&quot;linux-64&quot;][feature.label.dependencies]r-seurat = &#x27;5.2.*&#x27;r-SeuratDisk = &#123;version = &quot;*&quot;, channel = &quot;dnachun&quot;&#125; Here, r-SeuratDisk will be installed from the dnachun channel, while other packages follow the default order defined in the channels list. 4. Mixing PyPI DependenciesThe Conda ecosystem provides a very rich collection of Python resources, but some niche or extremely cutting‑edge packages may still be missing. In such cases, you can additionally specify dependencies from PyPI. Set the PyPI mirror in [pypi-options], list Conda packages under [dependencies], and list PyPI packages under [pypi-dependencies]. If you are using workspace mode, you can also use [feature.xxx.pypi-dependencies] within a feature. 12345678910111213141516[project]name = &quot;DEMO1&quot;version = &quot;1.0&quot;channels = [&quot;conda-forge&quot;, &quot;bioconda&quot;]platforms = [&quot;linux-64&quot;][pypi-options]index-url = &quot;https://pypi.tuna.tsinghua.edu.cn/simple&quot;[dependencies]snakemake = &#x27;*&#x27;scanpy = &#x27;*&#x27;[pypi-dependencies]singler = &#x27;*&#x27;celldex = &#x27;*&#x27; In the above configuration, singler and celldex will be installed via PyPI, while the remaining packages come from Conda. 5. Cross‑Platform Dependency ManagementIn team collaboration or CI&#x2F;CD scenarios, we may need to run the same environment on Linux‑64, Linux‑aarch64 (my Fydetab Duo uses the Arm‑based RK3588), etc. Pixi can specify different dependency versions per platform, and even use different package sources on different platforms to meet specific requirements (some packages are only available on Conda for Linux‑64). Use the [target.&lt;platform&gt;.dependencies] section to define dependencies for a particular platform. At the same time, you can restrict the platforms an environment supports via the platforms field in [environments]. 123456789101112131415[workspace]channels = [&quot;conda-forge&quot;]platforms = [&quot;linux-64&quot;, &quot;linux-aarch64&quot;][target.linux-64.dependencies]git = &#x27;*&#x27;git-subtree = &#x27;*&#x27;[feature.frontend.dependencies]nodejs = &#x27;18.*&#x27;pnpm = &#x27;*&#x27;[feature.frontend.tasks]front_init = &#123; cmd = &quot;pnpm install&quot;, cwd = &quot;app/Frontend_Admin&quot; &#125;front_build = &#123; cmd = &quot;pnpm run build:prod&quot;, depends-on=[&#x27;front_init&#x27;]&#125; Here, git and git-subtree are installed only on linux-64; the frontend environment is limited to the linux-64 platform (for example, some front‑end toolchains may not be compatible with ARM). 6. Setting Environment Variables for Sub‑environmentsCertain tools require environment variables to be set in specific contexts (e.g., Node.js’s NODE_OPTIONS, proxy settings, etc.). Pixi allows you to define environment variables in a feature via [feature.xxx.activation.env]. These variables are automatically set when entering that environment. 1234567891011121314[workspace]channels = [&quot;conda-forge&quot;]platforms = [&quot;linux-64&quot;, &quot;linux-aarch64&quot;][environments]frontend = [&quot;frontend&quot;]test = [&quot;test&quot;][feature.frontend.activation.env]NODE_OPTIONS = &quot;--openssl-legacy-provider&quot;[feature.frontend.dependencies]nodejs = &#x27;18.*&#x27;pnpm = &#x27;*&#x27; This way, when running tasks inside the frontend environment, the NODE_OPTIONS environment variable automatically takes effect, avoiding some Node.js version compatibility issues. SummaryThrough the six typical use cases above, we can see that Pixi is not just a package manager—it’s a unified project environment and task coordination hub. With a single, concise pixi.toml file, it solves dependency management and automation for multi‑language, multi‑platform, multi‑stage tasks, greatly reducing the complexity and maintenance cost of project configuration. Currently, all the projects I write or maintain use Pixi for unified dependency management and task setup. The only limitation is that system‑level services like MySQL cannot be managed through it (but SQLite works fine—the projects aren’t that large anyway…).","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Pixi","slug":"Pixi","permalink":"https://silenwang.github.io/en/tags/Pixi/"},{"name":"Dependency Management","slug":"Dependency-Management","permalink":"https://silenwang.github.io/en/tags/Dependency-Management/"},{"name":"Data Science","slug":"Data-Science","permalink":"https://silenwang.github.io/en/tags/Data-Science/"},{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/tags/Bioinformatics/"},{"name":"Cross‑Platform","slug":"Cross‑Platform","permalink":"https://silenwang.github.io/en/tags/Cross%E2%80%91Platform/"}]},{"title":"Still Confused_Why Taipy's on_action Fails Without Wrapping","slug":"还是搞不懂-为什么taipy的on-action不嵌套一下就报错","date":"2025-12-17T21:45:09.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2025/12/17/还是搞不懂-为什么taipy的on-action不嵌套一下就报错/","permalink":"https://silenwang.github.io/en/2025/12/17/%E8%BF%98%E6%98%AF%E6%90%9E%E4%B8%8D%E6%87%82-%E4%B8%BA%E4%BB%80%E4%B9%88taipy%E7%9A%84on-action%E4%B8%8D%E5%B5%8C%E5%A5%97%E4%B8%80%E4%B8%8B%E5%B0%B1%E6%8A%A5%E9%94%99/","excerpt":"I have experience with several rapid application development frameworks in python, all of which uniformly bind element actions to Python functions to trigger updates or changes. So I should be fairly experienced in this area. However, Taipy genuinely puzzles me———it occasionally throws errors for no apparent reason…","text":"I have experience with several rapid application development frameworks in python, all of which uniformly bind element actions to Python functions to trigger updates or changes. So I should be fairly experienced in this area. However, Taipy genuinely puzzles me———it occasionally throws errors for no apparent reason… In Taipy, when we pass a function object directly to the on_action (or on_change) property, we may randomly encounter errors like “function not valid”. For example, the following code snippet might fail: 12345tgb.table( ... on_action=update_prod_link, # Passing the function name directly ...) But if we use an anonymous function instead, the aforementioned problem disappears and everything works fine: 12345tgb.table( ... on_action=lambda s, v, p: update_prod_link(s, v, p), ...) Since the official examples are relatively simple and always use anonymous functions, and I haven’t seen similar feedback in the issue tracker, for now I can only resort to this redundant workaround…","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"taipy","slug":"taipy","permalink":"https://silenwang.github.io/en/tags/taipy/"},{"name":"callback","slug":"callback","permalink":"https://silenwang.github.io/en/tags/callback/"}]},{"title":"Extending Taipy with Standalone HTML Components","slug":"taipy中利用独立的html扩展出新的组件","date":"2025-12-17T21:42:31.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2025/12/17/taipy中利用独立的html扩展出新的组件/","permalink":"https://silenwang.github.io/en/2025/12/17/taipy%E4%B8%AD%E5%88%A9%E7%94%A8%E7%8B%AC%E7%AB%8B%E7%9A%84html%E6%89%A9%E5%B1%95%E5%87%BA%E6%96%B0%E7%9A%84%E7%BB%84%E4%BB%B6/","excerpt":"I’ve been wondering: Taipy seems to be actively maintained, and it appears to have quite a few users (judging by the number of issues raised). Yet its component library isn’t particularly rich, and even some basic features come with minor bugs (like the light&#x2F;dark toggle button bug I discovered earlier). Then I accidentally came across the official tutorial on embedding third‑party content, and it suddenly clicked: even though its built‑in components are limited, you can always “stitch” in whatever you need!","text":"I’ve been wondering: Taipy seems to be actively maintained, and it appears to have quite a few users (judging by the number of issues raised). Yet its component library isn’t particularly rich, and even some basic features come with minor bugs (like the light&#x2F;dark toggle button bug I discovered earlier). Then I accidentally came across the official tutorial on embedding third‑party content, and it suddenly clicked: even though its built‑in components are limited, you can always “stitch” in whatever you need! Why Embed Third‑Party Components?Taipy, as a Python‑centric web‑application framework, provides a set of built‑in visual elements that already cover many common interaction scenarios. However, in real‑world projects, it’s all too easy to receive requirements from clients that fall just outside what the existing components can do. In those situations, we have to find a way to “stitch” those extra pieces into Taipy’s pages. Taipy does offer documentation on extending its Python components, but for a small, throw‑away project, it hardly makes sense to write HTML&#x2F;JS first, prepare the interface bindings for Taipy, then test, compile, and reinstall everything… If I really had to go through all that, I’d probably be fired before finishing the component! Fortunately, Taipy provides another “stitching” mechanism that allows us to render any page written in HTML + CSS + JS directly inside a Taipy page (essentially by wrapping it in an iframe). The core of this mechanism is the Gui.register_content_provider() function. Core Mechanism: register_content_providerGui.register_content_provider(type, provider_func) accepts two arguments: type: must be a Python type (e.g., folium.folium.Map) provider_func: a callback function that receives an object of that type and returns a bytes object, which should be the HTML content corresponding to that object. When Taipy encounters a variable of that type in a page, and that variable is placed in the content attribute of a part component, Taipy automatically calls the registered provider_func and embeds the returned HTML into the page via an iframe. This way, as long as you can ultimately obtain the content of an HTML file, Taipy can render it and display it on the page. The official example uses a Folium map object. Like Plotly, Folium wraps various JavaScript mapping libraries and provides a Python API. After drawing the map, it can directly output an HTML page, so the generated HTML can be seamlessly embedded into Taipy. AI Brings Greater ExtensibilityTwo years ago, Taipy’s approach might have felt somewhat limited, because extending functionality still required you to write the whole web stack—defeating the purpose of rapid Python‑based development. But with the rise of LLMs, the situation has changed dramatically. Nowadays, various LLMs can generate a usable web widget in a very short time. Then, with just a little learning of Jinja2 templates, you can quickly connect data to the web widget and extend Taipy’s capabilities. While this may not be suitable for extremely complex requirements, it’s absolutely sufficient for prototyping.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"taipy","slug":"taipy","permalink":"https://silenwang.github.io/en/tags/taipy/"},{"name":"html","slug":"html","permalink":"https://silenwang.github.io/en/tags/html/"}]},{"title":"Hands on Writing an Automated Deployment GitHub Action","slug":"实操写一个自动部署的github-action","date":"2025-12-17T01:44:13.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2025/12/17/实操写一个自动部署的github-action/","permalink":"https://silenwang.github.io/en/2025/12/17/%E5%AE%9E%E6%93%8D%E5%86%99%E4%B8%80%E4%B8%AA%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2%E7%9A%84github-action/","excerpt":"As the number of projects I need to maintain keeps growing (3 official websites, 2 systems, and 1 mini‑program, all with separate front‑end&#x2F;back‑end and independent databases), I’ve encountered many tasks that are trivial to do once but become chaotic when repeated many times across different contexts. I had already experimented with CI on GitHub, and this time I learned and practiced CD (Continuous Deployment).","text":"As the number of projects I need to maintain keeps growing (3 official websites, 2 systems, and 1 mini‑program, all with separate front‑end&#x2F;back‑end and independent databases), I’ve encountered many tasks that are trivial to do once but become chaotic when repeated many times across different contexts. I had already experimented with CI on GitHub, and this time I learned and practiced CD (Continuous Deployment). The Pain Points of Maintaining Multiple Projects AloneAs a solo developer who has to handle everything, I need to maintain several projects with different technology stacks. Each project has its own repository, build process, and deployment environment. My previous workflow was: modify code locally → run build scripts manually → upload artifacts to the server via SCP → log into the server to replace files and restart services. This process is highly repetitive and prone to mistakes due to oversight or human error, which could lead to production issues. Especially after fixing an urgent bug and needing to switch between multiple projects, the time and mental overhead of manual deployment increase significantly. Therefore, I wanted to hand over these repetitive operations to a machine, allowing me to focus more on the actual code logic. Why CD (Continuous Deployment) is NeededCI (Continuous Integration) handles automated building and testing of code, ensuring every commit is runnable. CD (Continuous Deployment) goes a step further by automatically deploying the tested code to a production or staging environment. For teams that require frequent updates and work on multiple projects in parallel, CD can greatly reduce repetitive manual work, lower the chance of human error, and let developers concentrate on feature development. Of course, I’m not such a team—I’m just one person. But as mentioned above, a usable CD workflow can still free me from mechanical repetition and let me devote my energy to fixing the actual code problems. Writing a GitHub Actions CD WorkflowBelow is the deployment workflow I designed for the official website projects I maintain. It triggers automatically when code is pushed to the main branch and changes occur under app/Official_Site/** or app/Official_Site_EN/**. It also supports manual triggering via the GitHub interface (workflow_dispatch). 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273name: DEPLOYon: push: branches: - main paths: # Only trigger when these directories change - &#x27;app/Official_Site/**&#x27; - &#x27;app/Official_Site_EN/**&#x27; workflow_dispatch:jobs: build: runs-on: ubuntu-latest strategy: matrix: environment: [website] steps: - name: Checkout repository uses: actions/checkout@v3 with: ref: main - name: Setup Pixi uses: prefix-dev/setup-pixi@v0.9.3 with: pixi-version: v0.59.0 environments: $&#123;&#123; matrix.environment &#125;&#125; - name: Build Website for ZH/EN run: | pixi run web_build mv app/Official_Site/dist front mv app/Official_Site_EN/dist front-en - name: Copy files ZH uses: appleboy/scp-action@v1 with: host: $&#123;&#123; vars.PROD_IP &#125;&#125; username: root password: $&#123;&#123; secrets.PROD_PASSWD &#125;&#125; port: 22 source: &quot;front&quot; target: /opt/project/uploadPath - name: Copy files EN uses: appleboy/scp-action@v1 with: host: $&#123;&#123; vars.PROD_IP &#125;&#125; username: root password: $&#123;&#123; secrets.PROD_PASSWD &#125;&#125; port: 22 source: &quot;front-en&quot; target: /opt/project/uploadPath - name: Deploy uses: appleboy/ssh-action@v1 with: host: $&#123;&#123; vars.PROD_IP &#125;&#125; username: root password: $&#123;&#123; secrets.PROD_PASSWD &#125;&#125; port: 22 script: | cd /opt/project rm -rf front.bak front-en.bak mv front front.bak mv front-en front-en.bak mv uploadPath/front front mv uploadPath/front-en front-en systemctl restart nginx Key Steps Explained Trigger Conditions (on) push to the main branch only when the specified directories change, avoiding builds triggered by unrelated commits. workflow_dispatch provides a manual trigger for immediate deployment when needed. Code Checkout Uses the official actions/checkout action to pull the code, specifying ref: main to ensure the build is based on the latest commit. Dependency Installation &amp; Build Artifacts Employs Pixi as a cross‑language package manager. Here it sets up the environment and builds the static websites with its built‑in command. File Transfer (SCP) Uses appleboy/scp-action to upload the built artifacts to a temporary directory /opt/project/uploadPath on the server. The server IP is passed via vars.PROD_IP (a project variable), and the password is stored in secrets.PROD_PASSWD (a repository secret) to avoid exposing sensitive information in the code. Deployment &amp; Rollback (SSH) Logs into the server via appleboy/ssh-action to perform the replacement. Key logic: first back up the currently running front and front‑en directories with a .bak suffix, then move the new versions from the upload directory into place, and finally restart Nginx. The backup operation provides a quick rollback capability—simply move the backup directories back to restore the previous version. ConclusionWith this simple GitHub Actions workflow, I’ve successfully reduced the deployment time for static official websites from about 10+ minutes of manual work per deployment to 2‑3 minutes of automated execution, and completely eliminated problems caused by operational oversights. CD isn’t just for large teams. Individual projects can also achieve significant efficiency gains through automation. If you’re also maintaining multiple projects that require frequent updates, consider spending an hour setting up your own continuous deployment pipeline. I believe it will bring a qualitative leap to your development experience.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"github action","slug":"github-action","permalink":"https://silenwang.github.io/en/tags/github-action/"},{"name":"CD","slug":"CD","permalink":"https://silenwang.github.io/en/tags/CD/"},{"name":"automated deployment","slug":"automated-deployment","permalink":"https://silenwang.github.io/en/tags/automated-deployment/"}]},{"title":"Rasterizing Single‑Cell Dot Plots in R","slug":"对R中绘制的单细胞点图进行格栅化","date":"2025-12-17T01:08:35.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2025/12/17/对R中绘制的单细胞点图进行格栅化/","permalink":"https://silenwang.github.io/en/2025/12/17/%E5%AF%B9R%E4%B8%AD%E7%BB%98%E5%88%B6%E7%9A%84%E5%8D%95%E7%BB%86%E8%83%9E%E7%82%B9%E5%9B%BE%E8%BF%9B%E8%A1%8C%E6%A0%BC%E6%A0%85%E5%8C%96/","excerpt":"In bioinformatics visualization, we often need to handle plots containing tens of thousands of data points, such as scatter plots from single‑cell RNA‑seq data. When saved in vector formats like PDF, such graphics can suffer from huge file sizes and slow rendering (most software other than AI will simply freeze), because a vector file records the coordinates, color, size, and other attributes of every single point, resulting in a PDF with an enormous number of objects that hampers viewing and editing efficiency.","text":"In bioinformatics visualization, we often need to handle plots containing tens of thousands of data points, such as scatter plots from single‑cell RNA‑seq data. When saved in vector formats like PDF, such graphics can suffer from huge file sizes and slow rendering (most software other than AI will simply freeze), because a vector file records the coordinates, color, size, and other attributes of every single point, resulting in a PDF with an enormous number of objects that hampers viewing and editing efficiency. In contrast, bitmap images (e.g., PNG, JPEG) store the picture as a pixel matrix, which yields relatively fixed file sizes and fast rendering, but they lose quality when zoomed in. To combine the strengths of both approaches, we can rasterize the data‑dense layers of a plot—i.e., convert only specific layers to a bitmap while keeping other layers (such as axes, text labels) as vectors. This significantly reduces file size while preserving clarity where it matters most. The following R code demonstrates how to use the rasterise function from the ggrastr package to rasterize layers of a ggplot2‑based graphic: 1234567891011121314151617library(ggrastr)library(scRNAtoolVis)# Create the plot; the same principle applies to any function that returns a ggplot2 object plot &lt;- jjVolcano( diffData = markerspbmc.markers, log2FC.cutoff = 0.5, col.type = &quot;adjustP&quot;, size = 3.5, fontface = &#x27;italic&#x27;)layers &lt;- plot$layers# After inspecting the layers, the third layer is the dot layer I want to rasterizelayers[[3]] &lt;- rasterise(layers[[3]], dpi = 300)# Replace the layers back into the plotplot$layers &lt;- layers This technique is especially useful for high‑data‑volume visualizations such as single‑cell dot plots, genome‑browser tracks, etc. It retains the crispness of important annotations in vector form while making the saved file editable on ordinary computers (though if you intend to edit the points themselves, that’s another story…).","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"},{"name":"ggplot2","slug":"ggplot2","permalink":"https://silenwang.github.io/en/tags/ggplot2/"},{"name":"rasterization","slug":"rasterization","permalink":"https://silenwang.github.io/en/tags/rasterization/"}]},{"title":"Subtree vs Submodule in Git","slug":"git中的subtree和submodule","date":"2025-12-06T01:49:33.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2025/12/06/git中的subtree和submodule/","permalink":"https://silenwang.github.io/en/2025/12/06/git%E4%B8%AD%E7%9A%84subtree%E5%92%8Csubmodule/","excerpt":"Emmmmm, the contract with our official website vendor expired this year, so… I ended up with one more project to manage… This project is still frontend&#x2F;backend separated, but the difference is that the official site has an English version, and the English version is actually a branch of the frontend project. I’ve used Submodule before to integrate a colleague’s independent module into the main project, but this time, after consulting with AI, I chose a different approach: Subtree.","text":"Emmmmm, the contract with our official website vendor expired this year, so… I ended up with one more project to manage… This project is still frontend&#x2F;backend separated, but the difference is that the official site has an English version, and the English version is actually a branch of the frontend project. I’ve used Submodule before to integrate a colleague’s independent module into the main project, but this time, after consulting with AI, I chose a different approach: Subtree. Quick Comparison: Submodule vs SubtreeIn short, Submodule creates a “link” in the main repository that points to a specific commit of a sub‑repository, while Subtree copies the entire code of the sub‑repository into a sub‑directory of the main repository. Both have their pros and cons; the table below helps you quickly grasp their core differences: Feature Submodule Subtree How code is stored Main repo only keeps a reference (commit hash) to the sub‑repo Sub‑repo’s code is fully copied into the main repo Extra initialization after cloning Requires git submodule init &amp;&amp; git submodule update No extra steps, all files are ready Directly modifying sub‑project code inside the main project Must enter the sub‑module directory and commit separately, which is cumbersome Can modify and commit directly within the main repo directory History Sub‑repo history is completely separate from the main repo Sub‑repo history can be optionally merged (--squash) or kept intact Suitable scenarios Sub‑project is relatively stable, updated infrequently; multiple main projects share the same sub‑project Need to frequently modify the sub‑project inside the main project; need to bring in multiple different branches of the same repository In a nutshell, Subtree essentially clones a full copy of the sub‑project code. When you make changes and commit, the modifications are synchronized back to the sub‑project, and when the sub‑project is updated elsewhere, those changes can be pulled back into the main project. On the other hand, submodule is more like a soft link—the main project only specifies which version of the sub‑project code to use, and pushing changes back to the sub‑project is relatively more cumbersome. For my case, on one hand I wanted to bring two different branches (master and en) of the same repository as separate sub‑directories; on the other hand, I actually need to modify the sub‑project code frequently, and the main project is essentially an integration of the frontend and backend. According to AI’s advice, Subtree is the simpler combination to operate. Practical Usage of SubtreeBelow are the specific commands I used to add each sub‑project to the main project. Note that I used the --squash parameter, which merges the sub‑project’s historical commits into a single commit, preventing the main project’s history from being flooded with numerous sub‑project commits. 1. Add remote repositories12git remote add Backend https://github.com/user/Backendgit remote add Frontend https://github.com/user/Frontend 2. Bring sub‑projects in as subtrees1234567# Chinese site (master branch)git subtree add --prefix=app/Backend Backend master --squash# English site independent branch (en branch)git subtree add --prefix=app/Backend_EN Backend en --squash# Backend admin projectgit subtree add --prefix=app/Backend Backend_Admin aiming_med --squash 3. Daily sync operations Pull updates from the sub‑project: git subtree pull --prefix=&lt;directory&gt; &lt;remote&gt; &lt;branch&gt; --squash Push changes made to the sub‑project inside the main project: git subtree push --prefix=&lt;directory&gt; &lt;remote&gt; &lt;branch&gt; Of course, to simplify operations, you can configure corresponding commands in Pixi. 4. Notes After setting up subtree as described above, the sub‑project code already has a full copy in this project. Therefore, when deploying again, after cloning the main project you do not need to run subtree add again. If you clone for development purposes, you still need to add the remote repositories (as in step 1 above) so that you can later push updates to the sub‑project. AfterwordI’ve been using Subtree for two weeks now. Apart from stumbling over the fact that the git in Devcontainer didn’t have the subtree module, everything else has been working fine… Hopefully no new issues will pop up…","categories":[{"name":"Other","slug":"Other","permalink":"https://silenwang.github.io/en/categories/Other/"}],"tags":[{"name":"git","slug":"git","permalink":"https://silenwang.github.io/en/tags/git/"},{"name":"version control","slug":"version-control","permalink":"https://silenwang.github.io/en/tags/version-control/"},{"name":"subtree","slug":"subtree","permalink":"https://silenwang.github.io/en/tags/subtree/"},{"name":"submodule","slug":"submodule","permalink":"https://silenwang.github.io/en/tags/submodule/"}]},{"title":"Pixi can now be used in github actions","slug":"pixi也可以在github-action中使用了","date":"2025-12-06T01:43:09.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2025/12/06/pixi也可以在github-action中使用了/","permalink":"https://silenwang.github.io/en/2025/12/06/pixi%E4%B9%9F%E5%8F%AF%E4%BB%A5%E5%9C%A8github-action%E4%B8%AD%E4%BD%BF%E7%94%A8%E4%BA%86/","excerpt":"In CI&#x2F;CD workflows, dependency management is often a key factor determining build efficiency and reliability. Recently, I tried the setup-pixi GitHub Action in a static website deployment pipeline.","text":"In CI&#x2F;CD workflows, dependency management is often a key factor determining build efficiency and reliability. Recently, I tried the setup-pixi GitHub Action in a static website deployment pipeline. Why Choose Pixi?When I first chose Pixi, it was mainly because it builds on conda‘s pre‑compiled resources, offers faster dependency resolution than mamba, and has a lock‑file mechanism similar to nodejs. These features make it ideal for rapid environment migration and deployment, which are essential in bioinformatics where software changes frequently. After using it for over a year, I not only manage bioinformatics environments with Pixi but also use it to set up and maintain development environments. Currently, except for system‑resident services like MySQL that it cannot handle, Pixi can install every toolchain I need for my work. Using a single tool to manage various toolchains, along with its built‑in simple workflow configuration, provides an excellent user experience. The only minor drawback was a slight inconvenience when I had to use GitHub Actions for automation at my supervisor’s request. Since pixi still involves virtual environments, installing and running it required some extra environment‑variable tweaks. However, the official team had already developed the setup-pixi action long ago—I just didn’t know about it… Using setup-pixi in GitHub ActionsUsing setup-pixi is straightforward. Here’s an example: 123456789101112131415161718192021222324252627282930name: DEPLOYon: push: branches: - main workflow_dispatch:jobs: build: runs-on: ubuntu-latest strategy: matrix: environment: [website] steps: - name: Checkout repository uses: actions/checkout@v3 with: ref: main - name: Setup Pixi uses: prefix-dev/setup-pixi@v0.9.3 with: pixi-version: v0.59.0 environments: $&#123;&#123; matrix.environment &#125;&#125; - name: Run Pixi Task run: | pixi run build Step‑by‑Step Explanation Checkout repository: Use actions/checkout to fetch the latest code. This must come before Setup Pixi because the pixi.toml and pixi.lock files reside in the project. Setup Pixi: The prefix-dev/setup-pixi Action downloads and installs the specified version of Pixi, then automatically activates the environment defined in pixi.toml that matches the one specified in the matrix (in this example, the website environment). Run Pixi Task: With the environment ready, you can freely invoke any task already defined in the project. The whole process is clean and clear. The Pixi environment ensures dependency consistency during the build phase—define it once, and no further manual setup is needed. SummaryBy leveraging setup-pixi in GitHub Actions, we can easily harness Pixi’s powerful dependency‑management capabilities, simplify configuration, reduce manual setup time, and guarantee high consistency from development to production environments.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"pixi","slug":"pixi","permalink":"https://silenwang.github.io/en/tags/pixi/"},{"name":"github action","slug":"github-action","permalink":"https://silenwang.github.io/en/tags/github-action/"}]},{"title":"Another Pitfall, Plotly's Scattermap Cannot Set Colors for Most Marker Icons","slug":"又踩坑了-plotly的scattermap不能对大部分marker图标设置颜色","date":"2025-12-02T15:20:39.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2025/12/02/又踩坑了-plotly的scattermap不能对大部分marker图标设置颜色/","permalink":"https://silenwang.github.io/en/2025/12/02/%E5%8F%88%E8%B8%A9%E5%9D%91%E4%BA%86-plotly%E7%9A%84scattermap%E4%B8%8D%E8%83%BD%E5%AF%B9%E5%A4%A7%E9%83%A8%E5%88%86marker%E5%9B%BE%E6%A0%87%E8%AE%BE%E7%BD%AE%E9%A2%9C%E8%89%B2/","excerpt":"Ah, I didn’t expect that after so many years, Plotly still hits its limits with just a little use. Previously it couldn’t draw timeline charts, and this time I found that the customization of map markers is insufficient…","text":"Ah, I didn’t expect that after so many years, Plotly still hits its limits with just a little use. Previously it couldn’t draw timeline charts, and this time I found that the customization of map markers is insufficient… I needed to use Plotly’s Scattermap object to draw a map with some markers on it. After seeing the first version, the client wanted to use the common inverted‑teardrop icon (like a map pin) instead of the default circle. Fortunately Scattermap supports that, so I wrote: 123456789101112131415161718map_fig = ( go.Figure() .add_trace( go.Scattermap( lat = Lat, lon = Lon, mode = &#x27;markers&#x27;, marker = dict( size = 12, color = AIMINGMED_COLOR[&#x27;AIMING_RED&#x27;], opacity = 0.5, symbol = &#x27;marker&#x27;, allowoverlap = True ), hoverinfo = &#x27;none&#x27; ) )) It turned out that although the icon could be customized, its color couldn’t be changed… After some searching, I found a last‑year’s issue that gave a rather hopeless answer: up to now, plotly simply does not support this change because it uses maki‑icons, where the icons are fixed black and there is no interface to change the color via requests. The issue is labeled P3, meaning… it’s on the schedule, but at the lowest priority… something that will be done who‑knows‑when… But I couldn’t wait. I tortured Copilot for another morning and got an even more desperate answer: plotly.py is just a wrapper for plotly.js; it only organizes the data and parameters as required, while the actual drawing is completely handled by plotly.js. In other words, if I wanted to fix this problem, I would have to look at the JS code, and not just modify it—I would also need to expose the interface on the JS side and then connect it with the Python part… That… isn’t impossible, but I didn’t have enough time… So I slapped my forehead and thought of a hack. From observation, plotly.js actually fetches the icon’s SVG file via an HTTPS request and uses that file for drawing. Well… is it possible to intercept that request with a userscript (like Tampermonkey) and feed it my prepared SVG file? This time ChatGPT saved my ass, Deepseek completely failed… ChatGPT indeed gave me a script that could intercept and replace the SVG file: 1234567891011121314151617181920212223242526272829303132333435363738394041// ==UserScript==// @name Intercept Mapbox Maki Icon via Image.src// @namespace http://tampermonkey.net/// @version 1.0// @description Intercept Mapbox icon requests and replace with custom SVG// @match *://*/*// @run-at document-start// @grant none// ==/UserScript==(function() &#123; &#x27;use strict&#x27;; const TARGET_PREFIX = &quot;https://unpkg.com/maki@2.1.0/icons/&quot;; const CUSTOM_SVG_CONTENT = `&lt;svg version=&quot;1.1&quot; id=&quot;svg4619&quot; inkscape:version=&quot;0.91 r13725&quot; sodipodi:docname=&quot;marker-15.svg&quot; xmlns:cc=&quot;http://creativecommons.org/ns#&quot; xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot; xmlns:inkscape=&quot;http://www.inkscape.org/namespaces/inkscape&quot; xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot; xmlns:sodipodi=&quot;http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd&quot; xmlns:svg=&quot;http://www.w3.org/2000/svg&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot; x=&quot;0px&quot; y=&quot;0px&quot; width=&quot;15px&quot; height=&quot;15px&quot; viewBox=&quot;0 0 15 15&quot; style=&quot;enable-background:new 0 0 15 15;&quot; xml:space=&quot;preserve&quot;&gt;&lt;path id=&quot;path4133&quot; inkscape:connector-curvature=&quot;0&quot; d=&quot;M7.5,0C5.0676,0,2.2297,1.4865,2.2297,5.2703 C2.2297,7.8378,6.2838,13.5135,7.5,15c1.0811-1.4865,5.2703-7.027,5.2703-9.7297C12.7703,1.4865,9.9324,0,7.5,0z&quot; fill=&quot;#E72410&quot;/&gt;&lt;/svg&gt;`; const CUSTOM_SVG_URL = &quot;data:image/svg+xml;base64,&quot; + btoa(CUSTOM_SVG_CONTENT); // Hijack Image.src Object.defineProperty(Image.prototype, &quot;src&quot;, &#123; set(url) &#123; if (typeof url === &quot;string&quot; &amp;&amp; url.startsWith(TARGET_PREFIX)) &#123; console.log(&quot;[Tampermonkey] Intercepted Mapbox icon: &quot;, url); url = CUSTOM_SVG_URL; &#125; this.setAttribute(&quot;src&quot;, url); &#125;, get() &#123; return this.getAttribute(&quot;src&quot;); &#125; &#125;);&#125;)(); Finally… I got it working for now. The remaining problems can be dealt with later… Sigh, back to overtime work…","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"taipy","slug":"taipy","permalink":"https://silenwang.github.io/en/tags/taipy/"},{"name":"scattermap","slug":"scattermap","permalink":"https://silenwang.github.io/en/tags/scattermap/"},{"name":"Tampermonkey","slug":"Tampermonkey","permalink":"https://silenwang.github.io/en/tags/Tampermonkey/"},{"name":"script","slug":"script","permalink":"https://silenwang.github.io/en/tags/script/"}]},{"title":"Everyone's Ideas Are Surprisingly Similar – Thoughts After Reading devpod Code","slug":"大家的想法还真是一样的","date":"2025-12-01T22:27:29.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2025/12/01/大家的想法还真是一样的/","permalink":"https://silenwang.github.io/en/2025/12/01/%E5%A4%A7%E5%AE%B6%E7%9A%84%E6%83%B3%E6%B3%95%E8%BF%98%E7%9C%9F%E6%98%AF%E4%B8%80%E6%A0%B7%E7%9A%84/","excerpt":"After using devpod for a short while, I realized that I still needed a tool that could automatically install VS Code on a remote host via SSH without relying on containers, and then forward the port back to my local machine. So I started using Copilot to help me understand devpod’s code…","text":"After using devpod for a short while, I realized that I still needed a tool that could automatically install VS Code on a remote host via SSH without relying on containers, and then forward the port back to my local machine. So I started using Copilot to help me understand devpod’s code… After interrogating Copilot for a while, I began to feel as frustrated as my former roommate – the explanations it gave were rarely correct, and the runnable examples it provided didn’t work at all… Eventually, I went back to the basics: giving it a file and having it explain bit by bit what objects were inside, what methods were available, and how I could use them (just without me copying the code into the chat box myself)… After spending several weekends reading through the code, I once again realized that indeed everyone has ideas; it just depends on who has the ability to execute them… Before I found devpod, when I wanted to write a program from scratch to quickly install VS Code, I outlined the features I needed to implement: The program should log in to a remote machine via SSH, check whether VS Code is already installed, and download it if not. Since executing commands via SSH to accomplish step 1 is cumbersome, I planned to write those functions into the main program and give the program the ability to upload itself to the remote machine, then verify the MD5 checksum. Because remote machines could be x86 or ARM, the project would need to compile executable files for all architectures at once, making it easy to transfer them from local to remote. The program should be able to forward remote machine ports to the local side. Using the host’s SSH program might cause compatibility issues, so I would need to use Golang’s SSH implementation. Then I discovered that devpod had already done everything I thought of… VS Code installation support: devpod not only supports open‑vscode but also all common web IDEs, including RStudio. Self‑upload to remote: devpod places a copy of its own executable on the remote host and inside container workspaces, receives sub‑commands from the client, and executes various tasks through the agent sub‑command. Multi‑architecture support for remote machines: It doesn’t keep different architecture executables locally, but instead detects the remote host’s architecture and downloads the corresponding devpod binary. Using Golang’s SSH implementation: The port‑forwarding part uses golang.org/x/crypto/ssh. Of course, even though devpod already provides so many ready‑made interfaces and examples, building on top of it for secondary development is not a simple task for me… The biggest obstacle is that Go, as a statically‑typed language, still feels quite unfamiliar to me. After all, the languages I use daily are all weakly‑typed, script‑oriented languages, and many Go concepts are really hard for me to grasp… Moreover, its syntax is quite different from the languages I’m used to… I even feel the gap is larger than between C# and Python. Much of it can’t be guessed, and with AI often being unhelpful, understanding the code is still quite slow… (and probably accompanied by many misunderstandings). I hope I can persist until I have a basic demo. Wish me luck…","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"devpod","slug":"devpod","permalink":"https://silenwang.github.io/en/tags/devpod/"},{"name":"golang","slug":"golang","permalink":"https://silenwang.github.io/en/tags/golang/"}]},{"title":"Hot-fixing R Functions When Discovering Bugs","slug":"对发现bug的R函数进行热修复","date":"2025-11-26T21:54:29.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2025/11/26/对发现bug的R函数进行热修复/","permalink":"https://silenwang.github.io/en/2025/11/26/%E5%AF%B9%E5%8F%91%E7%8E%B0bug%E7%9A%84R%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E7%83%AD%E4%BF%AE%E5%A4%8D/","excerpt":"In a short period of time, I’ve encountered two situations where I needed to fix bugs in R functions, and I’ve also learned how to perform hot replacements…","text":"In a short period of time, I’ve encountered two situations where I needed to fix bugs in R functions, and I’ve also learned how to perform hot replacements… Why Hot-fixing is Needed?In data analysis projects, time is often a critical factor. If there are bugs in the R package functions we depend on, it can block the entire analysis workflow. While reporting bugs to package maintainers and helping with fixes is the best way to give back to the open-source community… you can’t really tell your boss, “I’ve reported the bug to the author, and we’ll get the correct results sometime in the distant future!” So finding the problem yourself and performing a hot-fix is more practical. The Most Basic Fix - ReplacementBased on my years of experience, although R supports objects, not many packages actually use them this way. At least most bioinformatics packages are still functional. Therefore, the simplest approach is to replace the problematic function with a modified version. First, we can locate the function to be fixed, then use print(YOUR_FUNCTION) to get the code of the corresponding function in your current working environment. Of course, sometimes the function we need to fix is not exported by the package for user use - it might be an internal function of the package. In this case, we need to use package_name + ::: + function_name to get the code. After modifying the function, simply use &lt;- to overwrite the original function to complete the modification. For example, my solution to the Azimuth problem was to directly modify and replace the function this way. Setting the Namespace After Modifying FunctionsOf course, most functions can’t be fixed as simply as described above because most functions are not isolated. They might need to call other functions within the original package, or they might be non-exported internal functions. In such cases, simply replacing them in the current namespace won’t work - we also need to replace the original function in the corresponding package’s namespace. The steps are as follows: R provides the assignInNamespace function, which allows us to replace objects in a package’s namespace. Use assignInNamespace to set the function’s environment to the package’s namespace Assign the modified function back to the package’s namespace The actual operation is quite simple: 12345678910# Write the modified functioncreateOncoMatrix &lt;- function( print(&#x27;Modified!&#x27;))# Set the function environment to the package&#x27;s namespaceenvironment(createOncoMatrix) &lt;- asNamespace(&quot;maftools&quot;)# Replace the modified function back to the package&#x27;s namespaceassignInNamespace(&quot;createOncoMatrix&quot;, createOncoMatrix, ns = &quot;maftools&quot;) AfterwordAlthough I’ve documented the hot-fixing methods, I still hope I won’t encounter such problems again… A 30-minute analysis turning into an entire afternoon of debugging…","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"},{"name":"Debug","slug":"Debug","permalink":"https://silenwang.github.io/en/tags/Debug/"}]},{"title":"To Reproduce Results, I Had to Reproduce Bugs Too","slug":"为了复现结果一并复现bug","date":"2025-11-26T21:11:53.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2025/11/26/为了复现结果一并复现bug/","permalink":"https://silenwang.github.io/en/2025/11/26/%E4%B8%BA%E4%BA%86%E5%A4%8D%E7%8E%B0%E7%BB%93%E6%9E%9C%E4%B8%80%E5%B9%B6%E5%A4%8D%E7%8E%B0bug/","excerpt":"If you live long enough, you’ll encounter plenty of awkward situations.","text":"If you live long enough, you’ll encounter plenty of awkward situations. An article we’re collaborating on with a client needed minor revisions, and I had to add some plots showing WES status according to the reviewers’ comments. Since the colleague previously responsible had left the company, this task naturally fell to me… But like most work taken over midway, when I got the data and ran it, unsurprisingly, the results didn’t match the figures in the article. We needed to generate oncoplot using maftools, selecting about 20+ genes to show mutation consistency in paired samples. However, with the same input files, the mutation types in my plots were fundamentally different from those provided by my former colleague. Compared to her version, Splice_Site mutations disappeared in my plots, and while mutations were detected, my version showed lots of MultiHit, whereas her version mostly showed single mutation types per gene. I checked the original files and found that Splice_Site mutations were indeed present, so I concluded the issue was with my version. I started digging through GitHub issues, and after some investigation, I discovered it involved two bugs - one in my version, and another in my former colleague’s… Background InformationFirst, let me explain the software version differences. My former colleague used her personal computer for analysis, with self-installed R and RStudio - the versions are unknown. I work on a company-purchased server, so I’ve been using server-based analysis. Following Pixi’s original design logic, I use per-project configurations and don’t preserve pixi.lock across projects. In pixi.toml, unless there are compatibility issues, I don’t specify versions, so the packages I use are the latest available on bioconda within dependency constraints. Currently, maftools is updated to 2.22 there. Regarding input files, our current workplace uses ANNOVAR (I’ve come full circle back to ANNOVAR after job hopping). The Splice_Site Parsing IssueTracing the disappearance of Splice_Site in my version revealed the problem occurred when mapping mutation types from ANNOVAR results to MAF-compatible types, as shown below: 1234567891011121314151617181920212223242526272829ann[, `:=`(Hugo_Symbol, unlist(data.table::tstrsplit(Gene.refGene, split = &quot;;&quot;, keep = 1)))]annovar_values = c(exonic = &quot;RNA&quot;, splicing = &quot;Splice_Site&quot;, UTR5 = &quot;5&#x27;UTR&quot;, UTR3 = &quot;3&#x27;UTR&quot;, intronic = &quot;Intron&quot;, upstream = &quot;5&#x27;Flank&quot;, downstream = &quot;3&#x27;Flank&quot;, intergenic = &quot;IGR&quot;, `frameshift insertion` = &quot;Frame_Shift_Ins&quot;, `frameshift deletion` = &quot;Frame_Shift_Del&quot;, `frameshift block substitution` = &quot;Frameshift_INDEL&quot;, `frameshift substitution` = &quot;Frameshift_INDEL&quot;, stopgain = &quot;Nonsense_Mutation&quot;, stoploss = &quot;Nonstop_Mutation&quot;, startloss = &quot;Translation_Start_Site&quot;, startgain = &quot;Unknown&quot;, `nonframeshift insertion` = &quot;In_Frame_Ins&quot;, `nonframeshift deletion` = &quot;In_Frame_Del&quot;, `nonframeshift block substitution` = &quot;Inframe_INDEL&quot;, `nonframeshift substitution` = &quot;Inframe_INDEL&quot;, `nonsynonymous SNV` = &quot;`Missense`_Mutation&quot;, `synonymous SNV` = &quot;Silent&quot;, unknown = &quot;Unknown&quot;, ncRNA_exonic = &quot;RNA&quot;, ncRNA_intronic = &quot;RNA&quot;, ncRNA_UTR3 = &quot;RNA&quot;, ncRNA_UTR5 = &quot;RNA&quot;, ncRNA = &quot;RNA&quot;, ncRNA_splicing = &quot;RNA&quot;)ann[, `:=`(Func.refGene, unlist(data.table::tstrsplit(x = as.character(Func.refGene), split = &quot;;&quot;, keep = 1)))]ann[, `:=`(ExonicFunc.refGene, unlist(data.table::tstrsplit(x = as.character(ExonicFunc.refGene), split = &quot;;&quot;, keep = 1)))]ann$Variant_Classification = ifelse( # The problem is here. The logic is: # When ExonicFunc.refGene has no value, use Func.refGene value for mapping via named vector, # Otherwise use ExonicFunc.refGene for mapping. However, according to ANNOVAR documentation, # annotations typically use `-nasting .`, meaning dots represent missing values, # so ExonicFunc.refGene would never actually be missing is.na(ann$ExonicFunc.refGene), annovar_values[ann$Func.refGene], annovar_values[ann$ExonicFunc.refGene] ) Due to the described issue, since ExonicFunc.refGene would never actually be missing, all mutations where ExonicFunc.refGene appeared missing (actually mutations outside exon regions) would be filtered out because Variant_Classification would map to missing values. The effect was that these mutations completely disappeared after parsing. The fix was simple - either check for . explicitly, or specify during fread that . represents missing values. 123456ann$Variant_Classification = ifelse( # I chose to modify the condition directly is.na(ann$ExonicFunc.refGene) | ann$ExonicFunc.refGene == &quot;.&quot;, annovar_values[ann$Func.refGene], annovar_values[ann$ExonicFunc.refGene] ) Tracing this issue, it might not necessarily be maftools’ fault. I checked several previous versions and didn’t see changes in this part - maybe fread changed the default nastring parameter… Anyway, my problem was solved, and I didn’t investigate further. The MultiHit Parsing IssueThis issue originates from a reported GitHub issue. Simply put, the logic for MultiHit in older versions of maftools was flawed. MultiHit was intended to show when a gene had two different amino acid-changing mutations (Missense + Missense &#x3D; MultiHit). However, in older versions, only different types of mutations in the same gene would be classified as MultiHit. If both were Missense mutations, it would simply display as Missense. In the latest version I was using, this issue had been fixed, which is why my results showed lots of MultiHit… So… what could I do? I had to reproduce the bug according to the issue’s description… AfterwordI had just complained about Azimuth‘s bugs a couple of days ago, and I never expected that maftools, which I’ve used many times, would also have issues. I guess I’m not just an unlucky star, but also a freaking jinx…","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"},{"name":"maftools","slug":"maftools","permalink":"https://silenwang.github.io/en/tags/maftools/"}]},{"title":"Even Well-Established Projects Can Be Buggy - Azimuth is Full of Bugs","slug":"出身豪门也还是会坑","date":"2025-11-22T23:00:37.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2025/11/22/出身豪门也还是会坑/","permalink":"https://silenwang.github.io/en/2025/11/22/%E5%87%BA%E8%BA%AB%E8%B1%AA%E9%97%A8%E4%B9%9F%E8%BF%98%E6%98%AF%E4%BC%9A%E5%9D%91/","excerpt":"In bioinformatics, the more cutting-edge your research direction, the more problems you’ll face from the informatics side. Even when papers are published with excellent results, and the original authors share their code or even provide ready-to-use software tools, it doesn’t mean we can easily use these existing resources for reproduction or further research. Chaotic environment setup is just one aspect - more often than not, since software authors aren’t professional software engineers, we should be grateful if the tool just works. We can’t expect these software to be bug-free, nor can we expect them to have decent performance (unless performance was a development goal). Even tools from well-established labs aren’t free to these issues, such as… Azimuth.","text":"In bioinformatics, the more cutting-edge your research direction, the more problems you’ll face from the informatics side. Even when papers are published with excellent results, and the original authors share their code or even provide ready-to-use software tools, it doesn’t mean we can easily use these existing resources for reproduction or further research. Chaotic environment setup is just one aspect - more often than not, since software authors aren’t professional software engineers, we should be grateful if the tool just works. We can’t expect these software to be bug-free, nor can we expect them to have decent performance (unless performance was a development goal). Even tools from well-established labs aren’t free to these issues, such as… Azimuth. Introduction to AzimuthAzimuth is a single-cell data annotation tool developed by the Satija Lab, designed to simplify the Label Transfer process in Seurat and quickly perform Label Transfer on cells to be classified. Problem DescriptionIn the current latest 0.5.0 version, running AzimuthReference in Jupyter Notebook trigger an error: Error in ValidateAzimuthReference(object = object): Reference must contain an AzimuthData object in the tools slot. A quick Google search reveals that this issue was first reported in April 2024 (issue #219). The user who reported the problem already provided a solution, but the issue remains open and unresolved (last week another user reported encountering the same problem). Solution ApproachAs mentioned earlier, issue reporter zacharyrs has already identified the problem. In commit b1b6895, the code author uses sys.calls() to determine the name of the currently called function and makes subsequent processing decisions based on this name. 1234567891011121314tool.name &lt;- as.character(x = sys.calls()) # &lt;-- sys.calls() here returns a list; if not calling AzimuthReference directly, it deletes some information from the objecttool.name &lt;- lapply( X = strsplit(x = tool.name, split = &quot;(&quot;, fixed = TRUE), FUN = &quot;[&quot;, 1)[[1]]if (tool.name != &quot;AzimuthReference&quot;) &#123; slot(object, name = &quot;tools&quot;)[&quot;AzimuthReference&quot;] &lt;- slot(object, name = &quot;tools&quot;)[tool.name] slot(object, name = &quot;tools&quot;)[tool.name] &lt;- NULL&#125;object &lt;- DietSeurat(object = object, counts = FALSE, assays = c(&quot;refAssay&quot;, assays), dimreducs = c(&quot;refDR&quot;, &quot;refUMAP&quot;)) However, in practice, we often need to wrap AzimuthReference within functions. In such cases, the first element in the list returned by sys.calls() becomes the outermost function name, which causes the data required for Azimuth to run to be deleted, leading to errors in subsequent checks. In my case, I learned for the first time that all code in notebooks is wrapped within IRKernel functions (thanks to AI for helping me troubleshoot). So even when directly running AzimuthReference in my notebook, it triggers the same error that would occur when running AzimuthReference within a function. The solution is simple: modify the judgment logic - after sys.calls() returns the list, take the last element and then extract the function name using regular expressions: 12call_list &lt;- sys.calls()tool.name &lt;- as.character(x = call_list[[length(call_list)]]) In our actual code, we can load Azimuth and then override the original AzimuthReference function with our modified version, allowing subsequent code to run properly. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091library(Azimuth)AzimuthReference &lt;- function (object, refUMAP = &quot;umap&quot;, refDR = &quot;spca&quot;, refAssay = &quot;SCT&quot;, dims = 1:50, k.param = 31, plotref = &quot;umap&quot;, plot.metadata = NULL, ori.index = NULL, colormap = NULL, assays = NULL, metadata = NULL, reference.version = &quot;0.0.0&quot;, verbose = FALSE) &#123; if (!refUMAP %in% Reductions(object = object)) &#123; stop(&quot;refUMAP (&quot;, refUMAP, &quot;) not found in Seurat object provided&quot;) &#125; if (is.null(x = Misc(object = object[[refUMAP]], slot = &quot;model&quot;))) &#123; stop(&quot;refUMAP (&quot;, refUMAP, &quot;) does not have the umap model info stored. &quot;, &quot;Please rerun RunUMAP with return.model = TRUE.&quot;) &#125; if (!refDR %in% Reductions(object = object)) &#123; stop(&quot;refDR (&quot;, refDR, &quot;) not found in Seurat object provided&quot;) &#125; if (is.null(x = metadata)) &#123; stop(&quot;Please specify at least one metadata field (for transfer and plotting).&quot;) &#125; for (i in metadata) &#123; if (!i %in% colnames(x = object[[]])) &#123; warning(i, &quot; not found in Seurat object metadata&quot;) next &#125; if (!is.factor(x = object[[i, drop = TRUE]])) &#123; warning(i, &quot; is not a factor. Converting to factor with alphabetical &quot;, &quot;levels.&quot;, call. = FALSE) object[[i, drop = TRUE]] &lt;- factor(x = object[[i, drop = TRUE]], levels = sort(x = unique(object[[i, drop = TRUE]]))) &#125; &#125; if (!refAssay %in% Assays(object = object)) &#123; stop(&quot;Seurat object provided must have the SCT Assay stored.&quot;) &#125; if (!inherits(x = object[[refAssay]], what = &quot;SCTAssay&quot;)) &#123; stop(&quot;refAssay (&quot;, refAssay, &quot;) is not an SCTAssay.&quot;) &#125; if (length(x = levels(x = object[[refAssay]])) != 1) &#123; stop(&quot;refAssay (&quot;, refAssay, &quot;) should contain a single SCT model.&quot;) &#125; suppressWarnings(expr = object[[&quot;refUMAP&quot;]] &lt;- object[[refUMAP]]) suppressWarnings(expr = object[[&quot;refDR&quot;]] &lt;- object[[refDR]]) object &lt;- FindNeighbors(object = object, reduction = &quot;refDR&quot;, dims = dims, graph.name = &quot;refdr.annoy.neighbors&quot;, k.param = k.param, nn.method = &quot;annoy&quot;, annoy.metric = &quot;cosine&quot;, cache.index = TRUE, return.neighbor = TRUE, l2.norm = FALSE, verbose = verbose) if (verbose) &#123; message(&quot;Computing pseudobulk averages&quot;) &#125; features &lt;- rownames(x = Loadings(object = object[[&quot;refDR&quot;]])) plot.metadata &lt;- plot.metadata %||% object[[metadata]] if (inherits(x = plotref, what = &quot;DimReduc&quot;)) &#123; plot.metadata &lt;- plot.metadata[Cells(x = plotref), ] &#125; ad &lt;- CreateAzimuthData(object = object, plotref = plotref, plot.metadata = plot.metadata, colormap = colormap, reference.version = reference.version) ori.index &lt;- ori.index %||% match(Cells(x = object), Cells(x = object[[&quot;refUMAP&quot;]])) object$ori.index &lt;- ori.index DefaultAssay(object = object) &lt;- refAssay object[[refAssay]] &lt;- subset(x = object[[refAssay]], features = features) DefaultAssay(object = object[[&quot;refDR&quot;]]) &lt;- refAssay object &lt;- DietSeurat(object = object, counts = FALSE, assays = c(refAssay, assays), dimreducs = c(&quot;refDR&quot;, &quot;refUMAP&quot;)) metadata &lt;- c(metadata, &quot;ori.index&quot;) for (i in colnames(x = object[[]])) &#123; if (!i %in% metadata) &#123; object[[i]] &lt;- NULL &#125; &#125; sct.model &lt;- slot(object = object[[refAssay]], name = &quot;SCTModel.list&quot;)[[1]] object[[&quot;refAssay&quot;]] &lt;- as(object = suppressWarnings(Seurat:::CreateDummyAssay(assay = object[[refAssay]])), Class = &quot;SCTAssay&quot;) slot(object = object[[&quot;refAssay&quot;]], name = &quot;SCTModel.list&quot;) &lt;- list(refmodel = sct.model) DefaultAssay(object = object) &lt;- &quot;refAssay&quot; DefaultAssay(object = object[[&quot;refDR&quot;]]) &lt;- &quot;refAssay&quot; Tool(object = object) &lt;- ad call_list &lt;- sys.calls() # use the last element of sys.call() to get the right function name tool.name &lt;- as.character(x = call_list[[length(call_list)]]) # use the last element of sys.call() to get the right function name tool.name &lt;- lapply(X = strsplit(x = tool.name, split = &quot;(&quot;, fixed = TRUE), FUN = &quot;[&quot;, 1)[[1]] if (tool.name != &quot;AzimuthReference&quot;) &#123; slot(object, name = &quot;tools&quot;)[&quot;AzimuthReference&quot;] &lt;- slot(object, name = &quot;tools&quot;)[tool.name] slot(object, name = &quot;tools&quot;)[tool.name] &lt;- NULL &#125; object &lt;- DietSeurat(object = object, counts = FALSE, assays = c(&quot;refAssay&quot;, assays), dimreducs = c(&quot;refDR&quot;, &quot;refUMAP&quot;)) ValidateAzimuthReference(object = object) return(object)&#125; Azimuth Seems AbandonedAlthough it comes from the Satija Lab, it feels like this project has been abandoned. There are nearly a hundred open issues on GitHub, and PRs submitted since last year still have 5 that are neither merged nor rejected. Recently, they’ve started a new Python-based deep learning-based universal cell type Label Transfer project, suggesting that as research moves forward, older projects are no longer being maintained. Moreover, for Label Transfer, one can always follow tutorials step by step - it’s not absolutely necessary to use Azimuth. Perhaps soon, this project will be archived…","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/tags/Bioinformatics/"},{"name":"Single-cell","slug":"Single-cell","permalink":"https://silenwang.github.io/en/tags/Single-cell/"},{"name":"Open-source Software","slug":"Open-source-Software","permalink":"https://silenwang.github.io/en/tags/Open-source-Software/"},{"name":"Software Maintenance","slug":"Software-Maintenance","permalink":"https://silenwang.github.io/en/tags/Software-Maintenance/"}]},{"title":"Thoughts Inspired by a Video About RISC-V","slug":"从一个讲riscv的视频想到","date":"2025-11-22T22:22:47.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2025/11/22/从一个讲riscv的视频想到/","permalink":"https://silenwang.github.io/en/2025/11/22/%E4%BB%8E%E4%B8%80%E4%B8%AA%E8%AE%B2riscv%E7%9A%84%E8%A7%86%E9%A2%91%E6%83%B3%E5%88%B0/","excerpt":"I’ve developed a habit of listening to videos during my bike ride home after work. This week, I came across a video by XPIN about RISC-V. The video presents a viewpoint: RISC-V, as an instruction set architecture, suffers from severe fragmentation in its chip products, which poses significant challenges to the development of its application ecosystem. Therefore, it’s argued that fragmentation must be addressed through unified standards to achieve rapid development. This perspective reminded me of my years of experience using Linux, Android, and the R language, leading me to believe that the video’s argument might not be entirely accurate.","text":"I’ve developed a habit of listening to videos during my bike ride home after work. This week, I came across a video by XPIN about RISC-V. The video presents a viewpoint: RISC-V, as an instruction set architecture, suffers from severe fragmentation in its chip products, which poses significant challenges to the development of its application ecosystem. Therefore, it’s argued that fragmentation must be addressed through unified standards to achieve rapid development. This perspective reminded me of my years of experience using Linux, Android, and the R language, leading me to believe that the video’s argument might not be entirely accurate. If fragmentation truly posed such a serious obstacle to technology adoption as suggested in the video, it would be difficult to explain why Linux has managed to capture a much larger market share than Windows in the server domain, why Android has secured its position in the global mobile device market, and why R language has developed so successfully in statistical computing that it even presents strong competition to commercial software like SAS. Personally, I believe the term “fragmentation” is often used to emphasize the negative aspects of open-source technologies. Many discussions on this topic tend to magnify the negative impacts of “fragmentation” while selectively overlooking the core value of open-source technology—“freedom”, even if it’s the “limited freedom” for some time. It is precisely this freedom that allows individuals without financial resources or existing networks to take part, thereby maximizing the collective wisdom of contributors and driving technological progress. I firmly believe that RISC-V will develop at a pace exceeding expectations, just like other open-source technologies, especially in an era where commercial technologies might become unavailable at any moment…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"},{"name":"RISC-V","slug":"RISC-V","permalink":"https://silenwang.github.io/en/tags/RISC-V/"},{"name":"SAS","slug":"SAS","permalink":"https://silenwang.github.io/en/tags/SAS/"},{"name":"Linux","slug":"Linux","permalink":"https://silenwang.github.io/en/tags/Linux/"}]},{"title":"AI is Still Changing Lives - Enabling Me to Do Things That Were Once Hard to Start","slug":"AI还是改变生活-让我能做以前很难起步的事情","date":"2025-11-22T20:25:43.000Z","updated":"2026-02-10T14:03:13.683Z","comments":true,"path":"2025/11/22/AI还是改变生活-让我能做以前很难起步的事情/","permalink":"https://silenwang.github.io/en/2025/11/22/AI%E8%BF%98%E6%98%AF%E6%94%B9%E5%8F%98%E7%94%9F%E6%B4%BB-%E8%AE%A9%E6%88%91%E8%83%BD%E5%81%9A%E4%BB%A5%E5%89%8D%E5%BE%88%E9%9A%BE%E8%B5%B7%E6%AD%A5%E7%9A%84%E4%BA%8B%E6%83%85/","excerpt":"I’m still quite fond of my current Hexo theme. From using its predecessor material-x in 2021 to upgrading to Volantis 5.x in 2023, the theme’s functionality has completely met my personal needs, to the point where I haven’t upgraded to the author’s subsequent versions 6 and 7. However, after using AI to batch-translate my blog content into English earlier this year, there have been persistent issues with some interface elements remaining in Chinese. This year, instead of undertaking major renovations, I decided to fix these minor bugs myself under AI guidance.","text":"I’m still quite fond of my current Hexo theme. From using its predecessor material-x in 2021 to upgrading to Volantis 5.x in 2023, the theme’s functionality has completely met my personal needs, to the point where I haven’t upgraded to the author’s subsequent versions 6 and 7. However, after using AI to batch-translate my blog content into English earlier this year, there have been persistent issues with some interface elements remaining in Chinese. This year, instead of undertaking major renovations, I decided to fix these minor bugs myself under AI guidance. Problem BackgroundSince using AI tools to batch-translate my blog content into English, my blog has had both Chinese and English versions. But during the modification process, two issues emerged: Some component switching effects were invalid in the English site, such as tabs on the “About Me” page that couldn’t switch when clicked Many elements on the theme interface remained in Chinese, and the visitor statistics at the bottom of the page always displayed fixed Chinese content that was clearly forgotten during the multilingual support implementation. After examining the theme’s code, I felt the former was a configuration issue, while the latter was more due to the Hexo framework itself. For example, many titles and category headings on the page were directly encoded in the configuration file. Deepseek suggested that a simpler approach in Hexo would be to prepare two configuration files directly. My current setup already uses separate folders for the Chinese and English sites, and after practical testing, I found that both issues could be resolved by modifying the configuration files in the Volantis code. Therefore, I ultimately chose to write two patch files and place them in my blog project. Solutions1. The Switching IssueFrom browser error messages, when I set the English site’s root to /en/, the path for the app.js file on the page was incorrectly set to /en/en/js/app.js. This file should be responsible for implementing the switching effects. When the file couldn’t be found, the switching effects were lost. According to Deepseek’s explanation, modifying the CDN settings in _config.yml could solve the problem. The specific diff is as follows: 123456789101112--- a/_config.yml+++ b/_config.yml@@ -50,7 +50,7 @@ cdn: # 以下配置可以覆盖 cdn.prefix,配置项的值可以为空，但是要使用CDN必须依据路径填写配置项的键 set: js:- #app: /js/app.js+ app: /js/app.js css: #style: /css/style.css # (异步加载样式) # 静态资源版本控制 2. Incomplete English LocalizationThese issues were simpler to fix - just replace the title of the display sections with English and remove the unused parts at the bottom of the page. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990--- a/_config.yml+++ b/_config.yml@@ -266,12 +266,12 @@ article: # 文章创建日期 date: icon: fa-solid fa-calendar-alt- title: &#x27;发布于：&#x27;+ title: &#x27;Published on: &#x27; format: &#x27;ll&#x27; # 日期格式 http://momentjs.com/docs/ # 文章更新日期 updated: icon: fa-solid fa-edit- title: &#x27;更新于：&#x27;+ title: &#x27;Updated on: &#x27; format: &#x27;ll&#x27; # 日期格式 http://momentjs.com/docs/ # 文章分类 category:@@ -279,15 +279,15 @@ article: # 文章浏览计数 counter: icon: fa-solid fa-eye- unit: &#x27;次浏览&#x27;+ unit: &#x27;views&#x27; # waline 文章评论数量 walinecount: icon: fa-solid fa-comment-dots- desc: &#x27;条评论&#x27; # 条评论+ desc: &#x27;comments&#x27; # 条评论 # artalk 文章评论数量 artalkcount: icon: fa-solid fa-comment-dots- desc: &#x27;条评论&#x27; # 条评论+ desc: &#x27;comments&#x27; # 条评论 # 文章字数和阅读时长 wordcount: icon_wordcount: fa-solid fa-keyboard@@ -523,7 +523,7 @@ sidebar: sticky: true header: icon: fa-solid fa-list- title: 本文目录+ title: TOC list_number: false min_depth: 2 max_depth: 5@@ -540,7 +540,7 @@ sidebar: display: [desktop] # [desktop, mobile] header: icon: fa-solid fa-folder-open- title: 文章分类+ title: Categories url: /blog/categories/ # --------------------------------------- # tagcloud widget@@ -549,7 +549,7 @@ sidebar: display: [desktop, mobile] # [desktop, mobile] header: icon: fa-solid fa-tags- title: 热门标签+ title: Tags url: /blog/tags/ min_font: 14 max_font: 24@@ -572,7 +572,7 @@ sidebar: display: [desktop] header: icon: fa-solid fa-award- title: 站点信息+ title: Website ino type: article: enable: true@@ -608,7 +608,7 @@ sidebar: display: [desktop, mobile] header: icon: fa-solid fa-clock WISTERIA- title: 最近更新+ title: Laste Update ############################### Sidebar ############################### &gt; end @@ -647,8 +647,6 @@ site_footer: source: https://github.com/volantis-x/volantis-docs/ # analytics using leancloud analytics: &gt;- &lt;span id=&quot;lc-sv&quot;&gt;本站总访问量为 &lt;span id=&#x27;number&#x27;&gt;&lt;i class=&quot;fa-solid fa-loader fa-spin fa-fw&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/span&gt; 次&lt;/span&gt;- &lt;span id=&quot;lc-uv&quot;&gt;访客数为 &lt;span id=&#x27;number&#x27;&gt;&lt;i class=&quot;fa-solid fa-loader fa-spin fa-fw&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/span&gt; 人&lt;/span&gt; # site copyright copyright: &#x27;[Copyright © since 2017 XXX](/)&#x27; # You can add your own property here. (Support markdown, for example: br: &#x27;&lt;br&gt;&#x27;) 3. Patch GenerationAfter saving the above modifications, git commit, then you can use git diff COMMIT1 COMMIT2 &gt; PATCH_FILE to generate a patch. Since I cloned the original volantis project code and modified it to generate the patch, the file paths in the patch file are incorrect. You need to modify the following path in the patch file to the actual file path: 12--- a/_config.yml+++ b/_config.yml Additionally, the index 91bd9709..9304175e 100644 above needs to be removed, otherwise it will fail due to commit verification. ConclusionSince the second quarter of this year, as my department gradually dwindled to just me, all the websites and systems that the company needs to maintain have fallen into my hands… If AI hadn’t saved me, I absolutely wouldn’t have been able to handle analysis work while maintaining all these front-end and back-end systems simultaneously. Nor would I have been able to fix these minor blog bugs with just a bit of common knowledge now… But just like my experience with AI last year, my upper limit determines AI’s upper limit. When we can’t judge whether the solutions we get are right or wrong, we can only spin in circles like a little dog chasing its own tail…","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://silenwang.github.io/en/tags/hexo/"},{"name":"Volantis","slug":"Volantis","permalink":"https://silenwang.github.io/en/tags/Volantis/"},{"name":"blog","slug":"blog","permalink":"https://silenwang.github.io/en/tags/blog/"},{"name":"internationalization","slug":"internationalization","permalink":"https://silenwang.github.io/en/tags/internationalization/"}]},{"title":"Half Success - Compiling and Running Podman on FydeOS Using Pixi","slug":"只成功了一半-利用pixi在FydeOS编译运行podman","date":"2025-11-11T01:30:59.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2025/11/11/只成功了一半-利用pixi在FydeOS编译运行podman/","permalink":"https://silenwang.github.io/en/2025/11/11/%E5%8F%AA%E6%88%90%E5%8A%9F%E4%BA%86%E4%B8%80%E5%8D%8A-%E5%88%A9%E7%94%A8pixi%E5%9C%A8FydeOS%E7%BC%96%E8%AF%91%E8%BF%90%E8%A1%8Cpodman/","excerpt":"Recently, I attempted to use pixi to compile and run podman on FydeOS. While I made some progress, it ultimately only achieved half success. In this blog post, I’ll share the entire process and the issues encountered.","text":"Recently, I attempted to use pixi to compile and run podman on FydeOS. While I made some progress, it ultimately only achieved half success. In this blog post, I’ll share the entire process and the issues encountered. Background and MotivationI use Fydetab Duo for local development and wanted to improve development efficiency through devpod. However, the Linux environment that comes with FydeOS has stability issues: after the device wakes from sleep, it often cannot access the Linux environment via the network, which severely impacts the development workflow. As an alternative, I considered running podman outside the virtual machine to manage the development environment. This way, even if the Linux environment becomes unavailable, the development environment could still be maintained through podman. However, the podman package provided by conda-forge doesn’t have an ARM64 version, making it impossible to use directly on Fydetab Duo (which is based on ARM architecture). Therefore, I decided to clone the podman package build code from conda-forge and use the pixi environment management tool to compile the ARM64 version of podman myself. Pixi is a cross-platform package and environment manager that can help manage complex compilation dependencies. Environment Configuration and Compilation ProcessFirst, I cloned the podman package build code from conda-forge and created a pixi project to manage the compilation environment. The pixi.toml file was configured as follows: 12345678910111213141516171819[workspace]authors = [&quot;Sylens &lt;qiumin14@163.com&gt;&quot;]channels = [&quot;conda-forge&quot;, &quot;file:///usr/local/share/project/podman/.pixi/envs/default/conda-bld&quot;]name = &quot;podman&quot;platforms = [&quot;linux-aarch64&quot;]version = &quot;0.1.0&quot;[tasks][system-requirements]libc = &#123; family = &quot;glibc&quot;, version = &quot;2.39&quot; &#125;[dependencies]boa = &quot;&gt;=0.17.0,&lt;0.18&quot;mamba = &quot;&gt;=1.5.12,&lt;2&quot;conda-build = &quot;&gt;=24.5.1,&lt;25&quot;diffutils = &quot;&gt;=3.12,&lt;4&quot;podman = &quot;&gt;=5.6.2,&lt;6&quot;libcap = &quot;&gt;=2.76,&lt;3&quot; This configuration specifies project dependencies and system requirements, particularly for ARM64 architecture (linux-aarch64) and glibc version 2.39. The version specification in the [system-requirements] section is actually Pixi’s virtual package specification mechanism, used to inform the dependency calculator about the current system’s library versions during dependency calculation (everything depends on libc). The channels section specifies a local path because podman has another necessary dependency netavark that also doesn’t have an Arm64 version, which needs to be compiled first to form a local Conda package before compiling podman. Compilation Challenges and SolutionsThe conda package build code for the two software packages are located at https://github.com/conda-forge/netavark-feedstock and https://github.com/conda-forge/podman-feedstock respectively. After cloning them, compile them in sequence. When compiling both netavark and podman, there were dependency issues. We need to modify their recipe/conda_build_config.yaml files. The specific differences are shown below, essentially specifying to use sysroot as the C standard library. 123456789index d402d1d..7dbfd74 100644--- a/recipe/conda_build_config.yaml+++ b/recipe/conda_build_config.yaml@@ -1,2 +1,5 @@+c_stdlib:+ - sysroot+ c_stdlib_version: # [linux64] - 2.17 # [linux64] After successful compilation, you can use pixi add to add podman to the environment, and then you’ll have the podman command line available. Core Issue Leading to Runtime FailureEverything went quite smoothly up to this point. However, when running podman pull alpine, it would prompt that newgidmap and newuidmap were missing. These are used for user and group ID mapping. This didn’t stump me, as I’m used to finding workarounds - I just copied them from the Linux virtual machine. However, even with these programs present, I still encountered the following error: 12newuidmap: Could not set capscannot set up namespace using &quot;/usr/bin/newuidmap&quot;: should have setuid or have filecaps setuid At this point, I encountered an insurmountable obstacle: FydeOS&#x2F;ChromeOS is designed from the ground up to not allow UID&#x2F;GID mapping in user namespaces. I tried setting the setuid bit for newuidmap and newgidmap using sudo chmod u+s as suggested by AI, but the system still refused to execute them. The kernel log showed: 1SafeSetID: Operation requires CAP_SETUID, which is not available to UID 1000 for operations besides approved set*uid transitions Here’s what AI told me about this: SafeSetID is a ChromeOS-specific security mechanism (LSM&#x2F;kernel patch) used to restrict setuid &#x2F; UID mapping CAP_SETUID is unavailable → Even if newuidmap has setuid or setcap, regular users (UID 1000) cannot modify UID mapping in rootless Podman Other logs are normal remount or USB device information, unrelated to rootless Podman Since I don’t have enough knowledge to verify the accuracy of this information, I could only test by compiling Podman in the same way within the Linux virtual machine. The test confirmed that Podman in the virtual machine runs normally, which at least proves that there are indeed special restrictions on the host system. ConclusionThis attempt proved that using pixi to manage podman dependencies on FydeOS is feasible, and the compilation process can be completed successfully. However, due to deep kernel security restrictions in FydeOS (particularly the SafeSetID module), the runtime environment cannot meet podman’s requirements for user namespaces, so this can only be considered “half success.” Therefore, to achieve my goal, I’ll need to compile the Openfyde image myself, directly modifying parts of the kernel during compilation and including podman directly.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"FydeOS","slug":"FydeOS","permalink":"https://silenwang.github.io/en/tags/FydeOS/"},{"name":"pixi","slug":"pixi","permalink":"https://silenwang.github.io/en/tags/pixi/"},{"name":"podman","slug":"podman","permalink":"https://silenwang.github.io/en/tags/podman/"}]},{"title":"Fixing Missing Dependency Issues When Deploying Some Bioconductor R Packages with pixi","slug":"修复pixi部署部分bioconda-r包后出现的缺依赖问题","date":"2025-10-29T03:37:56.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2025/10/29/修复pixi部署部分bioconda-r包后出现的缺依赖问题/","permalink":"https://silenwang.github.io/en/2025/10/29/%E4%BF%AE%E5%A4%8Dpixi%E9%83%A8%E7%BD%B2%E9%83%A8%E5%88%86bioconda-r%E5%8C%85%E5%90%8E%E5%87%BA%E7%8E%B0%E7%9A%84%E7%BC%BA%E4%BE%9D%E8%B5%96%E9%97%AE%E9%A2%98/","excerpt":"When using pixi to manage bioinformatics analysis environments, we often encounter issues where some Bioconductor R packages show missing dependencies after installation. The exact cause of this problem is currently unclear. After using pixi for a year, this issue still hasn’t been fixed (as of October 2025). This article introduces how to use pixi’s tasks feature to resolve such problems.","text":"When using pixi to manage bioinformatics analysis environments, we often encounter issues where some Bioconductor R packages show missing dependencies after installation. The exact cause of this problem is currently unclear. After using pixi for a year, this issue still hasn’t been fixed (as of October 2025). This article introduces how to use pixi’s tasks feature to resolve such problems. Problem DescriptionDuring the past year of using pixi, I’ve found that when deploying bioinformatics analysis environments, dependencies like GenomeInfoDbData and BSgenome.Hsapiens.UCSC.hg38 (R packages that fetch genomic data) for packages like Seurat and maftools always have issues. Even when these packages are explicitly specified and installed in the pixi environment, they still appear to be missing when actually loading them. Therefore, additional configuration is needed to supplement these dependencies. SolutionI currently use pixi’s tasks feature to turn dependency installation commands into tasks, allowing for quick dependency fixes after environment deployment. Here’s an example configuration: 1234567891011121314151617181920212223242526[workspace]name = &quot;azimuth_demo&quot;version = &quot;1.1&quot;description = &quot;workspace with azimuth and jupyter&quot;authors = [&quot;Sylens Wong &lt;qiumin14@163.com&gt;&quot;]channels = [&quot;conda-forge&quot;, &quot;bioconda&quot;]platforms = [&quot;linux-64&quot;][environments]label = [&#x27;kernel&#x27;, &#x27;label&#x27;][feature.kernel.dependencies]r-irkernel = &#x27;*&#x27;jupyterlab = &#x27;*&#x27;[feature.label.dependencies]r-base = &#x27;*&#x27;r-azimuth = &#x27;*&#x27;r-BiocManager = &#x27;*&#x27;[feature.label.tasks]GenomeInfoDbData = &#123;cmd = &#x27;Rscript -e &quot;BiocManager::install(\\&quot;GenomeInfoDbData\\&quot;)&quot;&#x27;&#125;BSgenome = &#123;cmd = &#x27;Rscript -e &quot;BiocManager::install(\\&quot;BSgenome.Hsapiens.UCSC.hg38\\&quot;)&quot;&#x27;&#125;EnsDb = &#123;cmd = &#x27;Rscript -e &quot;BiocManager::install(\\&quot;EnsDb.Hsapiens.v86\\&quot;)&quot;&#x27;&#125;JASPAR2020 = &#123;cmd = &#x27;Rscript -e &quot;BiocManager::install(\\&quot;JASPAR2020\\&quot;)&quot;&#x27;&#125;r_dep = &#123;cmd = &#x27;echo &quot;bio dep for R done&quot;&#x27;, depends-on=[&#x27;GenomeInfoDbData&#x27;, &#x27;BSgenome&#x27;, &#x27;EnsDb&#x27;, &#x27;JASPAR2020&#x27;]&#125; After activating the pixi environment, execute pixi run r_dep to complete the dependency fixes and start working.","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"},{"name":"pixi","slug":"pixi","permalink":"https://silenwang.github.io/en/tags/pixi/"},{"name":"Bioconductor","slug":"Bioconductor","permalink":"https://silenwang.github.io/en/tags/Bioconductor/"},{"name":"dependency management","slug":"dependency-management","permalink":"https://silenwang.github.io/en/tags/dependency-management/"}]},{"title":"Fixing the Issue of Lost obs and var Index When Reading/Writing Loom Files with scanpy","slug":"解决scanpy读写loom文件格式丢失obs和var的index的问题","date":"2025-10-28T17:22:12.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2025/10/28/解决scanpy读写loom文件格式丢失obs和var的index的问题/","permalink":"https://silenwang.github.io/en/2025/10/28/%E8%A7%A3%E5%86%B3scanpy%E8%AF%BB%E5%86%99loom%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%E4%B8%A2%E5%A4%B1obs%E5%92%8Cvar%E7%9A%84index%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"In our current single-cell analysis pipeline based on scanpy, there’s a step that requires saving AnnData objects in loom format. However, unlike saving to h5ad format, when we write an AnnData object to a loom file without any special handling and then read it back, we find that the index information of obs and var (typically cell barcodes and gene names) is lost, and these indices become ordinary numeric identifiers.","text":"In our current single-cell analysis pipeline based on scanpy, there’s a step that requires saving AnnData objects in loom format. However, unlike saving to h5ad format, when we write an AnnData object to a loom file without any special handling and then read it back, we find that the index information of obs and var (typically cell barcodes and gene names) is lost, and these indices become ordinary numeric identifiers. Problem DescriptionBased on actual testing and the documentation of the write_loom method, it can be inferred that during actual read&#x2F;write operations, write_loom does not save the index portions of the obs and var tables by default. Even when specifying the parameter write_obsm_varm=True, the writing process only creates two new columns with fixed names (obs_names and var_names). According to the read_loom method documentation, when reading, read_loom looks for a column named CellID in obs and a column named Gene in var to use as indices for the respective tables. This creates a mismatch: by default, indices aren’t saved, and even if they are saved, what write_loom writes and what read_loom reads don’t align - it’s like two departments failing to coordinate properly, resulting in the loss of critical information… SolutionMy goal is to make minimal changes to the original pipeline code, so directly generating loom files that the original pipeline can read is the best approach: 12345678910# Save index information to columns in obs and varadata.obs[&#x27;CellID&#x27;] = adata.obs.index.tolist()adata.var[&#x27;Gene&#x27;] = adata.var.index.tolist()# Write to loom file (including obsm and varm information)merge_adata.write_loom(&#x27;/path/to/raw.loom&#x27;)# Read the loom file - since CellID and Gene exist,# the read adata successfully preserves the index information of both tablesadata = sc.read_loom(&#x27;/path/to/raw.loom&#x27;)","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"scanpy","slug":"scanpy","permalink":"https://silenwang.github.io/en/tags/scanpy/"},{"name":"loom","slug":"loom","permalink":"https://silenwang.github.io/en/tags/loom/"},{"name":"adata","slug":"adata","permalink":"https://silenwang.github.io/en/tags/adata/"}]},{"title":"Comparing Milo Implementations in R and Python","slug":"在R和Python下运行milo的对比","date":"2025-10-28T15:57:51.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2025/10/28/在R和Python下运行milo的对比/","permalink":"https://silenwang.github.io/en/2025/10/28/%E5%9C%A8R%E5%92%8CPython%E4%B8%8B%E8%BF%90%E8%A1%8Cmilo%E7%9A%84%E5%AF%B9%E6%AF%94/","excerpt":"Milo is a differential abundance analysis method for single-cell RNA sequencing data that can detect compositional changes in cell neighborhoods across different conditions.","text":"Milo is a differential abundance analysis method for single-cell RNA sequencing data that can detect compositional changes in cell neighborhoods across different conditions. Introduction to the Milo AlgorithmMilo is a differential abundance analysis method specifically designed for single-cell RNA sequencing data. Its core idea is to detect changes in cell population composition across different experimental conditions by constructing cell neighborhoods. My understanding is that the purpose of this algorithm is to identify cells with population proportion differences between two conditions without pre-defining them. Once detection results are obtained, these differentially abundant cells can be extracted for further analysis. Milo Implementation in R EnvironmentIn R language, the miloR package is used for analysis. Here’s the corresponding implementation: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# Load required librarieslibrary(Seurat)library(miloR)library(SingleCellExperiment)library(scater)library(dplyr)library(tidyr)# Read Seurat object and convert to SingleCellExperimentseurat_obj &lt;- readRDS(&quot;seurat.rds&quot;)sce &lt;- as.SingleCellExperiment(seurat_obj)# Set random seed for reproducibility - mainly affects the kNN componentset.seed(4466)# Create Milo objectmilo_obj &lt;- Milo(sce)# Build graph structure - note: k is for kNN, d is for dimensions/componentsmilo_obj &lt;- buildGraph( milo_obj, k = 30, d = 20, transposed = TRUE, reduced.dim = &quot;UMAP&quot;)# Define neighborhoods - note: k, d, and reduced_dims should match previous step# refinement_scheme enables a faster algorithm# Provides significant computational acceleration for large datasetsmilo_obj &lt;- makeNhoods( x = milo_obj, prop = 0.1, k = 30, d = 20, refined = TRUE, reduced_dims = &quot;UMAP&quot;, refinement_scheme = &quot;graph&quot;)# Count cells in neighborhoodsmilo_obj &lt;- countCells( milo_obj, meta.data = data.frame(colData(milo_obj)), sample = &quot;sample_id&quot;)# Prepare experimental design - create a data frame with sample IDs and group tagstraj_design &lt;- data.frame(colData(milo_obj))[,c(&quot;sample_id&quot;, &quot;group&quot;)] %&gt;% distinct() %&gt;% mutate( sample_id = as.character(sample_id), group = as.character(group) )rownames(traj_design) &lt;- traj_design$sample_idtraj_design &lt;- traj_design[colnames(nhoodCounts(milo_obj)), , drop=FALSE]# Differential abundance testing - fdr.weighting = &quot;graph-overlap&quot;# Works with parameters from makeNhoods to improve computation speed# Note: Comparison groups can be specified using formula-like syntax# design = ~ 0 + group means using group variable for grouping, ~ 0 + is fixed syntax# model.contrasts = c(&#x27;groupCase - groupControl&#x27;) means:# Use rows where group equals &#x27;Case&#x27; as target, and &#x27;Control&#x27; as referenceda_results &lt;- testNhoods( milo_obj, design = ~ 0 + group, model.contrasts = c(&#x27;groupCase - groupControl&#x27;), design.df = traj_design, fdr.weighting = &quot;graph-overlap&quot;)# View significant resultsda_results %&gt;% arrange(SpatialFDR) %&gt;% filter(SpatialFDR &lt; 0.1) %&gt;% tail(3)# Build neighborhood graphmilo_obj &lt;- buildNhoodGraph(milo_obj)# All computation steps completed# Subsequent visualization can use plotUMAP or other functions Milo Implementation in Python EnvironmentPython has multiple Milo implementations. Besides Mipopy mentioned in the official miloR project, the pertpy library also implements the Milo algorithm. Here’s an example using pertpy: 12345678910111213141516171819202122232425262728293031323334353637import numpy as npimport pertpy as ptimport scanpy as scimport pandas as pd# Read dataadata = sc.read_h5ad(&#x27;adata.h5ad&#x27;)# Initialize Milo analysis objectmilo = pt.tl.Milo()mdata = milo.load(adata)# Build neighbor graphsc.pp.neighbors( mdata[&quot;rna&quot;], use_rep=&quot;X_umap&quot;, # Use existing UMAP dimensionality reduction n_pcs=20, # Number of principal components, corresponds to d in miloR n_neighbors=30, # Number of neighbors, corresponds to k in miloR)# Create neighborhoodsmilo.make_nhoods(mdata[&quot;rna&quot;], prop=0.1)mdata = milo.count_nhoods(mdata, sample_col=&quot;sample_id&quot;)# Group specification is more intuitive than in miloR, but order differs - Control comes firstmdata[&quot;rna&quot;].obs[&quot;group&quot;] = mdata[&quot;rna&quot;].obs[&quot;group&quot;].cat.reorder_categories([&quot;Control&quot;, &quot;Case&quot;])# Differential abundance analysis (using pydeseq2 solver)milo.da_nhoods(mdata, design=&quot;~group&quot;, solver=&quot;pydeseq2&quot;)# Build neighborhood graph for visualizationmilo.build_nhood_graph(mdata)# Filter significant resultssignificant_results = mdata[&#x27;milo&#x27;].var.query(&#x27;SpatialFDR &lt; 0.1&#x27;)# All computations completed PostscriptIn terms of actual performance, there isn’t a huge difference between the two implementations. However, there are differences in computation results since the Python version is an algorithmic implementation rather than a step-by-step reproduction. Regarding computational consistency, I’ll need to find a dataset from cellxgene to perform actual calculations in the future.","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/tags/Bioinformatics/"},{"name":"Single-cell analysis","slug":"Single-cell-analysis","permalink":"https://silenwang.github.io/en/tags/Single-cell-analysis/"},{"name":"Differential abundance analysis","slug":"Differential-abundance-analysis","permalink":"https://silenwang.github.io/en/tags/Differential-abundance-analysis/"},{"name":"Milo","slug":"Milo","permalink":"https://silenwang.github.io/en/tags/Milo/"}]},{"title":"Setting Up Self-Hosted Codespaces with DevPod","slug":"使用devpod设置自部署的codespace","date":"2025-10-28T05:49:54.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2025/10/28/使用devpod设置自部署的codespace/","permalink":"https://silenwang.github.io/en/2025/10/28/%E4%BD%BF%E7%94%A8devpod%E8%AE%BE%E7%BD%AE%E8%87%AA%E9%83%A8%E7%BD%B2%E7%9A%84codespace/","excerpt":"DevPod is an open-source development environment management tool that allows you to create development environments similar to GitHub Codespaces on any Kubernetes cluster or Docker host. This article will introduce how to use the DevPod CLI to create workspaces and provide a detailed explanation of writing DevContainer configuration files.","text":"DevPod is an open-source development environment management tool that allows you to create development environments similar to GitHub Codespaces on any Kubernetes cluster or Docker host. This article will introduce how to use the DevPod CLI to create workspaces and provide a detailed explanation of writing DevContainer configuration files. Introduction to DevPodDevPod is an open-source tool developed by Loft Labs that enables developers to create reproducible, disposable development environments on any infrastructure. Similar to GitHub Codespaces, but DevPod is self-hosted and can run on local machines, supported cloud providers, or Kubernetes clusters. Key features include: Support for multiple backends (Docker, Kubernetes, AWS EC2, etc.) Based on the DevContainer standard Development environments as code Fast environment startup and teardown DevPod Provider and Workspace CreationInstalling DevPodDevPod offers a desktop version, but since I’m running it on FydeOS, I’ll install the DevPod CLI. The DevPod CLI is quite friendly as it has almost no external dependencies and even includes built-in SSH functionality, making it easy to install on FydeOS’s native command line. 1curl -fsSL https://raw.githubusercontent.com/loft-sh/devpod/master/scripts/install.sh | sh Configuring ProviderProvider is the backend driver for DevPod, defining where the development environment runs. Using SSH as an example: 1devpod provider add ssh --name amd -o HOST=AMD The above addition method is based on existing SSH configuration, corresponding to the content in ~/.ssh/config: 12345Host AMD HostName 192.168.0.2 Port 22 User user IdentityFile /home/user/.ssh/AMD_Key Note: DevPod is a tool based on container technology, so the target machine for SSH needs to have Docker or Podman already installed. Additionally, when using Ubuntu, pay special attention to the source of the Docker software. If Docker is installed via Snap on Ubuntu, due to Snap restrictions, it cannot access hidden folders (those starting with .) in the user’s Home directory, causing DevPod to fail when executing any build commands with Docker. Creating WorkspaceSpecify the provider and project path to start the workspace. DevPod will create the necessary development environment based on the configuration files. More importantly, DevPod automatically installs the web version of OpenVSCode in the workspace, starts the service, automatically forwards the service port to the local machine, allowing you to code happily. This truly achieves the goal of development environments as code. At the same time, these configurations only require the server to have SSH port open, which is really quite convenient. 1devpod up --provider amd --source git https://github.com/your-username/your-repo This will create a development environment based on the configuration files in the .devcontainer folder of the repository. Introduction to DevContainer FormatDevContainer is a standard format for development environment configuration. This example includes two main files: devcontainer.json and Dockerfile. Dockerfile ConfigurationThe Dockerfile is essentially the configuration file used during docker build. DevPod uses this file to create the workspace container. Note: It’s best to build configuration containers based on containers from mcr.microsoft.com&#x2F;devcontainers, as these containers are built according to specific requirements. Some of DevPod’s features depend on files and configurations within the container. If using other containers, you may need to handle compatibility issues yourself when starting the workspace. 123456789101112131415# Use MS official base image, which contains vscode user to avoid issues when DevPod startsFROM mcr.microsoft.com/devcontainers/base:bookworm# Install pixi for vscodeUSER vscode# Install pixi, as my projects all use it for dependency managementRUN curl -fsSL https://pixi.sh/install.sh | sh# Configure git user information and inject needed alias content into the environmentRUN git config --global user.name &quot;Sylens&quot; &amp;&amp; \\ git config --global user.email &quot;qiumin14@163.com&quot; &amp;&amp; \\ echo &#x27;alias aider=&quot;aider --no-check-update --no-show-model-warnings --yes --no-auto-commits --model deepseek/deepseek-reasoner&quot;&#x27; &gt;&gt; ~/.bashrcENV PATH=&quot;/home/vscode/.pixi/bin:$&#123;PATH&#125;&quot; Another important point to note is that DevPod’s operation logic is to first build the container, then mount the project code and perform further configuration according to devcontainer.json. Therefore, any settings that depend on project code cannot be performed during the container build phase. devcontainer.json Configurationdevcontainer.json defines the metadata and IDE configuration for the development container. Here’s a basic configuration that only sets some environment variables and the minimum required plugins. 12345678910111213141516171819202122&#123; &quot;name&quot;: &quot;silen_blog&quot;, &quot;build&quot;: &#123; &quot;dockerfile&quot;: &quot;Dockerfile&quot; &#125;, &quot;postCreateCommand&quot;: &quot;pixi run deploy&quot;, &quot;containerEnv&quot;: &#123; &quot;AIDER_DARK_MODE&quot;: &quot;$&#123;localEnv:AIDER_DARK_MODE&#125;&quot;, &quot;AIDER_CODE_THEME&quot;: &quot;$&#123;localEnv:AIDER_CODE_THEME&#125;&quot;, &quot;DEEPSEEK_API_KEY&quot;: &quot;$&#123;localEnv:DEEPSEEK_API_KEY&#125;&quot; &#125;, &quot;customizations&quot;: &#123; &quot;vscode&quot;: &#123; &quot;settings&quot;: &#123; &quot;workbench.colorTheme&quot;: &quot;Solarized Dark&quot; &#125;, &quot;extensions&quot;: [ &quot;naumovs.color-highlight&quot;, &quot;ms-ceintl.vscode-language-pack-zh-hans&quot;, &quot;tamasfe.even-better-toml&quot; ] &#125; &#125;&#125; Note: postCreateCommand can only contain one item. If you need to execute multiple commands, it’s recommended to write them into a shell script, place it in the .devcontainer directory, and then call it with postCreateCommand. AfterwordBack when I was writing ReviewGPT last year, I realized that everyone has ideas, but what matters is actually building them. I was originally modifying the terminal app in ChromeOS, hoping to add port forwarding and automatically install VSCode web on the target machine, then automatically forward the port to the local machine. As I worked on it, I discovered that Google had long abandoned pNaCl (which means native apps can’t access local ports), so I thought about developing a Go application that meets my requirements. Continuing to work on it, I discovered DevPod - except that it’s container-based… it’s already very close to my needs… Well, what else should I write then… Let me first learn how to use DevPod…","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"DevPod","slug":"DevPod","permalink":"https://silenwang.github.io/en/tags/DevPod/"},{"name":"DevOps","slug":"DevOps","permalink":"https://silenwang.github.io/en/tags/DevOps/"},{"name":"Development Environment","slug":"Development-Environment","permalink":"https://silenwang.github.io/en/tags/Development-Environment/"},{"name":"Docker","slug":"Docker","permalink":"https://silenwang.github.io/en/tags/Docker/"},{"name":"Podman","slug":"Podman","permalink":"https://silenwang.github.io/en/tags/Podman/"}]},{"title":"Debugging Techniques in TypeScript and JavaScript","slug":"TS-JS中进行调试","date":"2025-10-13T23:08:14.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2025/10/13/TS-JS中进行调试/","permalink":"https://silenwang.github.io/en/2025/10/13/TS-JS%E4%B8%AD%E8%BF%9B%E8%A1%8C%E8%B0%83%E8%AF%95/","excerpt":"As my classmate said, I’m currently experiencing a situation of “being forced to code”… The several websites I maintain each have their own frontend, backend databases, and some even have test code and migration code, involving languages ranging from Python commonly used in bioinformatics to JS, TS, HTML, C#, which I rarely used before. In actual work, there isn’t much time to systematically learn each language from scratch, so mastering the most basic debugging methods is crucial. Among them, I find the debugging methods for JavaScript and TypeScript particularly worth recording, as they do have some unique aspects compared to other languages.","text":"As my classmate said, I’m currently experiencing a situation of “being forced to code”… The several websites I maintain each have their own frontend, backend databases, and some even have test code and migration code, involving languages ranging from Python commonly used in bioinformatics to JS, TS, HTML, C#, which I rarely used before. In actual work, there isn’t much time to systematically learn each language from scratch, so mastering the most basic debugging methods is crucial. Among them, I find the debugging methods for JavaScript and TypeScript particularly worth recording, as they do have some unique aspects compared to other languages. Debugging Environment in Web Frontend FrameworksIt’s more accurate to say it’s debugging in a web frontend framework environment rather than pure JS&#x2F;TS debugging. Modern frontend frameworks (such as Vue, React, Angular) have both similarities and obvious differences compared to traditional HTML file structures. Essentially, these frameworks use various compilation and build tools to convert template, script, and style code into final webpage code, which is then rendered and displayed by the browser. This mixed programming model is similar to Snakemake workflow files, where a single file may contain multiple language elements. In actual development, what we mainly need to debug are the template part (HTML-like) and the script part (JavaScript&#x2F;TypeScript). Debugging Techniques for Template Parts1. Directly Display Variable ValuesIn Vue templates, you can use double curly brace syntax to directly display variables: 1&lt;p&gt;&#123;&#123; value &#125;&#125;&lt;/p&gt; In React, use single curly braces: 1&lt;p&gt;&#123;value&#125;&lt;/p&gt; 2. Conditional Debug DisplaySometimes we only want to display debug information during the development phase: 123456&lt;template&gt; &lt;div&gt; &lt;p v-if=&quot;isDevelopment&quot;&gt;Debug Info: &#123;&#123; debugValue &#125;&#125;&lt;/p&gt; &lt;!-- Normal content --&gt; &lt;/div&gt;&lt;/template&gt; Debugging Methods for Script Parts1. Basic Console Debugging123console.log(&#x27;Variable value:&#x27;, variable);console.table(arrayData); // Display arrays or objects in table formatconsole.dir(object); // Display object properties 2. Grouped Logs1234console.group(&#x27;User Information&#x27;);console.log(&#x27;Name:&#x27;, user.name);console.log(&#x27;Email:&#x27;, user.email);console.groupEnd(); 3. Conditional Breakpoints and Debuggerif (someCondition) &#123; debugger; // Browser will automatically pause at this line &#125;","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Debugging","slug":"Debugging","permalink":"https://silenwang.github.io/en/tags/Debugging/"},{"name":"Typescript","slug":"Typescript","permalink":"https://silenwang.github.io/en/tags/Typescript/"},{"name":"Javascript","slug":"Javascript","permalink":"https://silenwang.github.io/en/tags/Javascript/"},{"name":"Frontend Development","slug":"Frontend-Development","permalink":"https://silenwang.github.io/en/tags/Frontend-Development/"}]},{"title":"Monitoring My Websites with Uptime Kuma","slug":"使用uptime-kuma监控我维护的网站","date":"2025-10-09T15:30:29.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2025/10/09/使用uptime-kuma监控我维护的网站/","permalink":"https://silenwang.github.io/en/2025/10/09/%E4%BD%BF%E7%94%A8uptime-kuma%E7%9B%91%E6%8E%A7%E6%88%91%E7%BB%B4%E6%8A%A4%E7%9A%84%E7%BD%91%E7%AB%99/","excerpt":"Uptime Kuma is an extremely user-friendly open-source monitoring tool that makes monitoring various network services simple and efficient. It supports multiple protocol monitoring including HTTP(s), TCP, Ping, DNS queries, and even simulates real user access through Chrome engine mode to more accurately monitor service stability. Additionally, it supports dozens of notification methods for alerts and can integrate with various applications we use daily. It also offers a unique Push monitoring method, allowing users to extend monitoring functionality through custom code (such as monitoring SSH service availability).","text":"Uptime Kuma is an extremely user-friendly open-source monitoring tool that makes monitoring various network services simple and efficient. It supports multiple protocol monitoring including HTTP(s), TCP, Ping, DNS queries, and even simulates real user access through Chrome engine mode to more accurately monitor service stability. Additionally, it supports dozens of notification methods for alerts and can integrate with various applications we use daily. It also offers a unique Push monitoring method, allowing users to extend monitoring functionality through custom code (such as monitoring SSH service availability). Introduction to Uptime KumaUptime Kuma is a service monitoring application developed by Louis Lam. The design philosophy of this project is simplicity and ease of use, making it accessible even for users without technical backgrounds. Installing Uptime KumaThe simplest method is to use Docker for installation. In iStoreOS’s app store, there are pre-configured solutions available - just install and start it. The project runs on port 3001 by default. If you’re also using Gogs, there might be a port conflict, so you’ll need to change the port. Basic Usage1. Initial SetupWhen accessing for the first time, you need to set up an administrator account and password. The entire process takes only a few minutes to complete. 2. Adding Monitoring ItemsClick the “Add Monitor” button and fill in the following information: Monitor Type: Select the monitoring type (such as HTTP, TCP, Ping, etc.) Friendly Name: Give the monitor an easy-to-remember name URL&#x2F;Hostname: The website address or host to monitor Heartbeat Interval: Check interval (default 60 seconds) Particularly worth mentioning is the Chrome Engine Monitoring Mode: In the advanced settings of HTTP monitoring. This mode uses the actual Chrome browser kernel to access websites, can execute JavaScript, and detect whether page loading is truly complete. This is especially useful for Single Page Applications (SPA) and websites that require user interaction. 3. Setting Up NotificationsIn the “Setup Notification” section, you can add various notification methods. Uptime Kuma supports dozens of notification channels, including: Instant Messaging: Telegram, Discord, Slack, Line, Mattermost, etc. Email: SMTP email notifications Mobile Push: Pushover, Gotify, NTFY, etc. Webhook: Can be configured to directly integrate with domestic office software like WeCom, DingTalk, Feishu, etc. Taking WeCom as an example: Create a notification bot in a group chat Note down the bot’s key Select WeCom notification method in Uptime Kuma Fill in the key This way, when services become unavailable or recover, you’ll receive notifications in WeCom. 4. Using Push Monitoring ModePush monitoring is a unique feature of Uptime Kuma that allows services to actively report their status to Uptime Kuma. This means even for unsupported services (like SSH), we can write simple scripts to push monitoring results to Uptime Kuma for monitoring. I plan to write a Go service to monitor SSH connectivity when I have time. SummaryUptime Kuma is an extremely feature-rich and very user-friendly monitoring tool, particularly suitable for individual developers and small teams. If you’re also looking for a powerful yet easy-to-use monitoring solution, I highly recommend trying Uptime Kuma - let’s become cyber monitoring room masters together!","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Monitoring","slug":"Monitoring","permalink":"https://silenwang.github.io/en/tags/Monitoring/"},{"name":"Uptime-Kuma","slug":"Uptime-Kuma","permalink":"https://silenwang.github.io/en/tags/Uptime-Kuma/"},{"name":"Website Maintenance","slug":"Website-Maintenance","permalink":"https://silenwang.github.io/en/tags/Website-Maintenance/"},{"name":"Service Monitoring","slug":"Service-Monitoring","permalink":"https://silenwang.github.io/en/tags/Service-Monitoring/"}]},{"title":"Drawing a Circos Plot for Paired Samples","slug":"绘制一个配对样本的Circos图","date":"2025-10-06T22:09:54.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2025/10/06/绘制一个配对样本的Circos图/","permalink":"https://silenwang.github.io/en/2025/10/06/%E7%BB%98%E5%88%B6%E4%B8%80%E4%B8%AA%E9%85%8D%E5%AF%B9%E6%A0%B7%E6%9C%AC%E7%9A%84Circos%E5%9B%BE/","excerpt":"In cancer research, comparing genomic features between tumor samples and organoid models is crucial for validating model reliability. Circos plots provide an intuitive way to visualize detected mutations across the genome, making them commonly used for representing overall detection results of representative samples. While reading the circlize documentation, I came across an example demonstrating paired samples, which I found suitable for showcasing paired primary samples and organoids. I’ve adapted it to create a plot for displaying paired samples. The code is primarily based on the official documentation’s 9.5 Concatenating two genomes","text":"In cancer research, comparing genomic features between tumor samples and organoid models is crucial for validating model reliability. Circos plots provide an intuitive way to visualize detected mutations across the genome, making them commonly used for representing overall detection results of representative samples. While reading the circlize documentation, I came across an example demonstrating paired samples, which I found suitable for showcasing paired primary samples and organoids. I’ve adapted it to create a plot for displaying paired samples. The code is primarily based on the official documentation’s 9.5 Concatenating two genomes PreparationLoading Necessary R PackagesFirst, we need to load some essential R packages: 12345library(circlize)library(dplyr)library(stringr)library(MutationalPatterns)library(BSgenome.Hsapiens.UCSC.hg38) Among these, MutationalPatterns is mainly used for loading VCF files to avoid manual processing, while BSgenome.Hsapiens.UCSC.hg38 provides the necessary information for drawing basic chromosomal regions. Data PreparationThis visualization showcases three types of information: Somatic mutation detection results (VCF files) CNV detection results (from CNVKit) Sequencing coverage data (directly using results from CNV software to avoid duplication) Building Genomic Framework DataWe first construct a combined framework containing both tumor and organoid genomes: 1234567# Remove XY chromosomes and build combined genomic datatumor_cytoband &lt;- read.cytoband(species = &quot;hg38&quot;)$df %&gt;% filter(!(V1 %in% c(&#x27;chrX&#x27;, &#x27;chrY&#x27;)))organoid_cytoband &lt;- read.cytoband(species = &quot;hg38&quot;)$df %&gt;% filter(!(V1 %in% c(&#x27;chrX&#x27;, &#x27;chrY&#x27;)))tumor_cytoband[ ,1] &lt;- paste0(&quot;tumor_&quot;, tumor_cytoband[, 1])organoid_cytoband[ ,1] &lt;- paste0(&quot;organoid_&quot;, organoid_cytoband[, 1])cytoband &lt;- rbind(tumor_cytoband, organoid_cytoband) This creates a data frame named cytoband that contains basic information about the two genomes to be visualized. Processing Mutation DataUsing the MutationalPatterns package to read VCF files and convert them to a suitable format for plotting: 12345678910111213141516171819vcfs &lt;- read_vcfs_as_granges( c( &#x27;tumor.vcf&#x27;, &#x27;pdo.vcf&#x27; ), c(&quot;tumor&quot;, &quot;pdo&quot;), &quot;BSgenome.Hsapiens.UCSC.hg38&quot;)mut_data &lt;- rbind( data.frame(vcfs[[t_sample]]) %&gt;% mutate(value = 1) %&gt;% mutate(seqnames = str_c(&quot;tumor_&quot;, seqnames)) %&gt;% select(seqnames, start, end, value), data.frame(vcfs[[o_sample]]) %&gt;% mutate(value = 1) %&gt;% mutate(seqnames = str_c(&quot;organoid_&quot;, seqnames)) %&gt;% select(seqnames, start, end, value)) Processing CNV and Coverage DataBoth CNV and coverage data are obtained from CNVkit: 12345678910111213141516171819202122232425262728293031# CNV datacnv_data &lt;- rbind( read.table( &quot;tumor.cnvkit.call.cns&quot;, sep=&quot;\\t&quot;, header = F, col.names = c(&quot;chromosome&quot;, &quot;start&quot;, &quot;end&quot;, &quot;cn&quot;) ) %&gt;% mutate(value = cn) %&gt;% mutate(chromosome = str_c(&quot;tumor_&quot;, chromosome)) %&gt;% select(chromosome, start, end, value), read.table( &quot;pdo.cnvkit.call.cns&quot;, sep=&quot;\\t&quot;, header = F, col.names = c(&quot;chromosome&quot;, &quot;start&quot;, &quot;end&quot;, &quot;cn&quot;) ) %&gt;% mutate(value = cn) %&gt;% mutate(chromosome = str_c(&quot;organoid_&quot;, chromosome)) %&gt;% select(chromosome, start, end, value)) %&gt;% mutate(value = pmin(value, 4)) # Limit CNV values to maximum of 4# Coverage datacov_data &lt;- rbind( read.table(&quot;tumor.cnvkit.cov.cnn&quot;, sep=&quot;\\t&quot;, header = T) %&gt;% mutate(value = log2) %&gt;% mutate(chromosome = str_c(&quot;tumor_&quot;, chromosome)) %&gt;% select(chromosome, start, end, value), read.table(&quot;pdo.cnvkit.cov.cnn&quot;, sep=&quot;\\t&quot;, header = T) %&gt;% mutate(value = log2) %&gt;% mutate(chromosome = str_c(&quot;organoid_&quot;, chromosome)) %&gt;% select(chromosome, start, end, value)) Drawing the Circos PlotSetting Colors and Initialization1234567891011121314151617181920212223# Color settingsred &lt;- &quot;#FFC6D6&quot;blue &lt;- &quot;#bebcff&quot;green &lt;- &quot;#9CCF83&quot;orange &lt;- &quot;#fbd988ff&quot;purpule &lt;- &quot;#ff726dff&quot;black &lt;- &quot;#000000&quot;# Set image sizepng(filename = &quot;demo.circos.png&quot;, width = 1200, height = 1200, res = 200)# Build basic circos frameworkchromosome.index = c( paste0(&quot;tumor_chr&quot;, c(1:22)), rev(paste0(&quot;organoid_chr&quot;, c(1:22))))circos.par(gap.after = c(rep(1, 22), rep(1, 22)))circos.initializeWithIdeogram( cytoband, plotType = NULL, chromosome.index = chromosome.index) Adding Chromosome Tracks1234567891011121314# Set chromosome trackcircos.track( ylim = c(0, 1), panel.fun = function(x, y) &#123; circos.text(CELL_META$xcenter, CELL_META$ylim[2] + mm_y(2), gsub(&quot;.*chr&quot;, &quot;&quot;, CELL_META$sector.index), cex = 0.6, niceFacing = TRUE) &#125;, track.height = mm_h(1), cell.padding = c(0, 0, 0, 0), bg.border = NA)highlight.chromosome(paste0(&quot;tumor_chr&quot;, c(1:22)), col = red, track.index = 1)highlight.chromosome(paste0(&quot;organoid_chr&quot;, c(1:22)), col = blue, track.index = 1)circos.genomicIdeogram(cytoband) Adding Coverage, CNV, and Mutation Density Tracks1234567891011121314151617181920212223# Sequencing coveragecircos.genomicDensity(cov_data, col=orange, track.height = 0.1, window.size = 1e7)# CNV trackcircos.genomicTrackPlotRegion( cnv_data, ylim = c(0, 4), panel.fun = function(region, value, ...) &#123; cell.xlim = get.cell.meta.data(&quot;cell.xlim&quot;) for(h in c(0, 1, 2, 3, 4)) &#123; circos.lines(cell.xlim, c(h, h), col = black) &#125; col = ifelse(value[[1]] &gt; 2, &quot;red&quot;, ifelse(value[[1]] == 2, &quot;green&quot;, &quot;blue&quot;) ) i = getI(...) circos.genomicRect(region, value, col = col, ytop = value + 0.3, ybottom = value - 0.3 , border = NA) &#125;, track.height = 0.1)# Mutation densitycircos.genomicDensity(mut_data, col=purpule, track.height = 0.1, window.size = 1e7) Adding Text and Legend123456789101112131415161718192021# Set center texttext(0, 0.2, &quot;Demo&quot;, cex = 2, font = 2)# Set corner texttext(-0.9, -0.8, &quot;Tumor\\nGenome&quot;)text(0.9, 0.8, &quot;Organoid\\nGenome&quot;)# Set legend below center textlegend( x = 0, y = 0, legend = c(&quot;Coverage&quot;, &quot;CNV gain&quot;, &quot;CNV neutral&quot;, &quot;CNV loss&quot;, &quot;Mutation density&quot;), col = c(orange, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;, purpule), pch = 15, pt.cex = 1, cex = 0.8, bty = &quot;n&quot;, xjust = 0.5,)circos.clear()dev.off() Complete Code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144library(circlize)library(dplyr)library(stringr)library(MutationalPatterns)library(BSgenome.Hsapiens.UCSC.hg38)tumor_cytoband &lt;- read.cytoband(species = &quot;hg38&quot;)$df %&gt;% filter(!(V1 %in% c(&#x27;chrX&#x27;, &#x27;chrY&#x27;)))organoid_cytoband &lt;- read.cytoband(species = &quot;hg38&quot;)$df %&gt;% filter(!(V1 %in% c(&#x27;chrX&#x27;, &#x27;chrY&#x27;)))tumor_cytoband[ ,1] &lt;- paste0(&quot;tumor_&quot;, tumor_cytoband[, 1])organoid_cytoband[ ,1] &lt;- paste0(&quot;organoid_&quot;, organoid_cytoband[, 1])cytoband &lt;- rbind(tumor_cytoband, organoid_cytoband)t_sample &lt;- &quot;tumor&quot;o_sample &lt;- &quot;pdo&quot;vcfs &lt;- read_vcfs_as_granges( c( &#x27;tumor.vcf&#x27;, &#x27;pdo.vcf&#x27; ), c(t_sample, o_sample), &quot;BSgenome.Hsapiens.UCSC.hg38&quot;)mut_data &lt;- rbind( data.frame(vcfs[[t_sample]]) %&gt;% mutate(value = 1) %&gt;% mutate(seqnames = str_c(&quot;tumor_&quot;, seqnames)) %&gt;% select(seqnames, start, end, value), data.frame(vcfs[[o_sample]]) %&gt;% mutate(value = 1) %&gt;% mutate(seqnames = str_c(&quot;organoid_&quot;, seqnames)) %&gt;% select(seqnames, start, end, value))cnv_data &lt;- rbind( read.table( &quot;tumor.cnvkit.call.cns&quot;, sep=&quot;\\t&quot;, header = T ) %&gt;% mutate(value = cn) %&gt;% mutate(chromosome = str_c(&quot;tumor_&quot;, chromosome)) %&gt;% select(chromosome, start, end, value), read.table( &quot;pdo.cnvkit.call.cns&quot;, sep=&quot;\\t&quot;, header = T ) %&gt;% mutate(value = cn) %&gt;% mutate(chromosome = str_c(&quot;organoid_&quot;, chromosome)) %&gt;% select(chromosome, start, end, value)) %&gt;% mutate(value = pmin(value, 4))cov_data &lt;- rbind( read.table(&quot;tumor.cnvkit.cov.cnn&quot;, sep=&quot;\\t&quot;, header = T) %&gt;% mutate(value = log2) %&gt;% mutate(chromosome = str_c(&quot;tumor_&quot;, chromosome)) %&gt;% select(chromosome, start, end, value), read.table(&quot;pdo.cnvkit.cov.cnn&quot;, sep=&quot;\\t&quot;, header = T) %&gt;% mutate(value = log2) %&gt;% mutate(chromosome = str_c(&quot;organoid_&quot;, chromosome)) %&gt;% select(chromosome, start, end, value))########################################### Plotting ###########################################red &lt;- &quot;#FFC6D6&quot;blue &lt;- &quot;#bebcff&quot;green &lt;- &quot;#9CCF83&quot;orange &lt;- &quot;#fbd988ff&quot;purpule &lt;- &quot;#ff726dff&quot;black &lt;- &quot;#000000&quot;png(filename = &quot;demo.circos.png&quot;, width = 1200, height = 1200, res = 200)chromosome.index = c( paste0(&quot;tumor_chr&quot;, c(1:22)), rev(paste0(&quot;organoid_chr&quot;, c(1:22))))circos.par(gap.after = c(rep(1, 22), rep(1, 22))) # Set gaps?circos.initializeWithIdeogram( cytoband, plotType = NULL, chromosome.index = chromosome.index)circos.track( ylim = c(0, 1), panel.fun = function(x, y) &#123; circos.text(CELL_META$xcenter, CELL_META$ylim[2] + mm_y(2), gsub(&quot;.*chr&quot;, &quot;&quot;, CELL_META$sector.index), cex = 0.6, niceFacing = TRUE) &#125;, track.height = mm_h(1), cell.padding = c(0, 0, 0, 0), bg.border = NA)highlight.chromosome(paste0(&quot;tumor_chr&quot;, c(1:22)), col = red, track.index = 1)highlight.chromosome(paste0(&quot;organoid_chr&quot;, c(1:22)), col = blue, track.index = 1)circos.genomicIdeogram(cytoband)circos.genomicDensity(cov_data, col=orange, track.height = 0.1, window.size = 1e7)circos.genomicTrackPlotRegion( cnv_data, ylim = c(0, 4), panel.fun = function(region, value, ...) &#123; cell.xlim = get.cell.meta.data(&quot;cell.xlim&quot;) for(h in c(0, 1, 2, 3, 4)) &#123; circos.lines(cell.xlim, c(h, h), col = black) &#125; col = ifelse(value[[1]] &gt; 2, &quot;red&quot;, ifelse(value[[1]] == 2, &quot;green&quot;, &quot;blue&quot;) ) i = getI(...) circos.genomicRect(region, value, col = col, ytop = value + 0.3, ybottom = value - 0.3 , border = NA) &#125;, track.height = 0.1)circos.genomicDensity(mut_data, col=purpule, track.height = 0.1, window.size = 1e7)text(0, 0.2, &quot;Demo&quot;, cex = 2, font = 2)text(-0.9, -0.8, &quot;Tumor\\nGenome&quot;)text(0.9, 0.8, &quot;Organoid\\nGenome&quot;)legend( x = 0, y = 0, legend = c(&quot;Coverage&quot;, &quot;CNV gain&quot;, &quot;CNV neutral&quot;, &quot;CNV loss&quot;, &quot;Mutation density&quot;), col = c(orange, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;, purpule), pch = 15, pt.cex = 1, cex = 0.8, bty = &quot;n&quot;, xjust = 0.5, )circos.clear()dev.off() Final ResultThis uses data generated from one sample to create both the upper and lower circles, so the detection results appear identical on both sides. In reality, if paired results were this consistent, it would be quite remarkable.","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"},{"name":"circlize","slug":"circlize","permalink":"https://silenwang.github.io/en/tags/circlize/"},{"name":"Genome","slug":"Genome","permalink":"https://silenwang.github.io/en/tags/Genome/"},{"name":"Mutation","slug":"Mutation","permalink":"https://silenwang.github.io/en/tags/Mutation/"},{"name":"CNV","slug":"CNV","permalink":"https://silenwang.github.io/en/tags/CNV/"}]},{"title":"Compiling ffmpeg-rockchip on FydeOS","slug":"FydeOS下编译ffmpeg-rockchip","date":"2025-10-02T16:50:39.000Z","updated":"2026-02-10T14:03:13.683Z","comments":true,"path":"2025/10/02/FydeOS下编译ffmpeg-rockchip/","permalink":"https://silenwang.github.io/en/2025/10/02/FydeOS%E4%B8%8B%E7%BC%96%E8%AF%91ffmpeg-rockchip/","excerpt":"Last year, I actually used Fydetab Duo to compress videos from my travels. At that time, I found a pre-built library on GitHub, but now I can’t find that repository anymore… So this time, I tried to compile it myself…","text":"Last year, I actually used Fydetab Duo to compress videos from my travels. At that time, I found a pre-built library on GitHub, but now I can’t find that repository anymore… So this time, I tried to compile it myself… The compilation referred to the documentation by library author dnyanmisaka, and the entire process was relatively smooth~ Preparing the compilation environment using pixiCompared to the git, meson, cmake, pkg-config, gcc, libdrm-dev mentioned in the original documentation, many more things are needed here. After all, FydeOS&#x2F;ChromeOS is not a traditional Linux distribution and lacks many common libraries, so we need to supplement the missing dependencies. 12pixi initpixi add make gxx meson cmake pkg-config gcc libdrm libdrm-devel-conda-aarch64 pthread-stubs binutils diffutils awk After creating the pixi environment, you need to enter the virtual environment with pixi shell. Once inside, all subsequent compilation and installation destinations will point to this virtual environment, ensuring that all dependencies installed can be found during the final ffmpeg compilation. Compiling rkmpp123456789101112git clone -b jellyfin-mpp --depth=1 https://github.com/nyanmisaka/mpp.git rkmppmkdir -p rkmpp/rkmpp_buildcd rkmpp/rkmpp_buildcmake \\ -DCMAKE_INSTALL_PREFIX=/usr/local/share/project/ffmpeg/.pixi/envs/default/ \\ -DCMAKE_BUILD_TYPE=Release \\ -DBUILD_SHARED_LIBS=ON \\ -DBUILD_TEST=OFF \\ ..make -j $(nproc)make install Compiling RGA1234567891011git clone -b jellyfin-rga --depth=1 https://github.com/nyanmisaka/rk-mirrors.git rkrgameson setup rkrga rkrga_build \\ --prefix=/usr/local/share/project/ffmpeg/.pixi/envs/default/ \\ --libdir=lib \\ --buildtype=release \\ --default-library=shared \\ -Dcpp_args=-fpermissive \\ -Dlibdrm=false \\ -Dlibrga_demo=falsemeson configure rkrga_buildninja -C rkrga_build install Compiling and installing ffmpeg1234567# ffmpeg partgit clone --depth=1 https://github.com/nyanmisaka/ffmpeg-rockchip.git ffmpegcd ffmpeg./configure --prefix=/usr/local/share/project/ffmpeg/.pixi/envs/default \\ --enable-gpl --enable-version3 --enable-libdrm --enable-rkmpp --enable-rkrgamake -j $(nproc)make install AfterwordAfter executing all the steps above, the virtual environment will have the ffmpeg specifically optimized for Rockchip chips. The video processing speed is more than 10 times faster than the CPU version~ I’m considering turning the above content into a Pixi environment with tasks, and also learning how to package and upload packages using pixi.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"FydeOS","slug":"FydeOS","permalink":"https://silenwang.github.io/en/tags/FydeOS/"},{"name":"ffmpeg","slug":"ffmpeg","permalink":"https://silenwang.github.io/en/tags/ffmpeg/"}]},{"title":"Developing a Data Dashboard with Taipy and the Pitfalls Encountered","slug":"使用taipy开发一个数据面板以及遇到的坑","date":"2025-10-01T22:49:37.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2025/10/01/使用taipy开发一个数据面板以及遇到的坑/","permalink":"https://silenwang.github.io/en/2025/10/01/%E4%BD%BF%E7%94%A8taipy%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9D%A2%E6%9D%BF%E4%BB%A5%E5%8F%8A%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/","excerpt":"There are really many modules for rapidly developing data or AI-related applications in Python. I’ve already used Dash, Streamlit, Gradio, NiceGUI, and recently I discovered two more. Just when I needed to develop a simple data dashboard to display company data, I once again recklessly decided to use a new framework - Taipy.","text":"There are really many modules for rapidly developing data or AI-related applications in Python. I’ve already used Dash, Streamlit, Gradio, NiceGUI, and recently I discovered two more. Just when I needed to develop a simple data dashboard to display company data, I once again recklessly decided to use a new framework - Taipy. Using TaipyPage LayoutTaipy designs multiple ways to write layouts. Depending on the language you’re familiar with, you can choose from Markdown, HTML, or Python syntax for page layout settings. However, it seems the official recommendation is Markdown. Yes, it’s quite surprising - a library written in Python primarily promotes using Markdown for layouts… I naturally chose Python syntax. Its usage is basically consistent with Gradio, using with statements and objects to control element containment relationships. Specifically, through the taipy.gui.builder module, we can use Python syntax to define page structure. Main layout components include: tgb.layout(): Creates a layout container, where you can set the number of columns and gap tgb.part(): Creates content blocks for organizing related components In my data dashboard, I adopted a three-level layout structure: Top: Company logo, title, and theme toggle control Middle: Data visualization components like maps, pie charts, line charts, bar charts Bottom: Key metric card displays Layout example: 123456with tgb.layout(columns=&quot;1 2 1&quot;, gap=&quot;10px&quot;): # Three columns with width ratio 1:2:1 # Left block with tgb.part(class_name=&quot;card&quot;): tgb.text(&quot;Milestones&quot;, class_name=&quot;h5&quot;) tgb.chart(figure=&quot;&#123;timeline&#125;&quot;, plot_config=&#123;&quot;staticPlot&quot;: True&#125;) # Middle and right blocks are similar Style ConfigurationTaipy supports custom styling through CSS files and Stylekit. CSS Styles: Using CSS is the same as writing regular web pages. I created a style.css file to define global styles and component styles:1234567.card &#123; --element-padding: 0.4rem;&#125;.scrollable-part &#123; overflow-y: auto; /* Vertical scrolling */&#125; Then when creating the project, specify the additional CSS to load: 12gui = Gui(page, css_file=&quot;style.css&quot;)gui.run() Stylekit Themes: Define theme colors through Python dictionaries, then pass them when starting the GUI:12345678STYLEKIT = &#123; &quot;color-primary&quot;: &quot;#E72410&quot;, &quot;color-secondary&quot;: &quot;#F28E2A&quot;, &quot;color-background-light&quot;: &quot;#EDEDED&quot;, &quot;color-paper-light&quot;: &quot;#FFFFFF&quot;,&#125;gui.run(stylekit=STYLEKIT, dark_mode=False) Note that Stylekit controls far fewer elements than the classic approach of setting classes and adjusting them in CSS files. For what can be adjusted, refer to Taipy’s documentation. Besides these two methods, different Taipy controls can also use the class_name parameter to set some module-preset styles. What’s available varies by control, so you need to refer to each control’s documentation. Value BindingTaipy provides a method called Value Binding to make updating interface content simpler. Specifically, it uses curly brace &#123;&#125; syntax to bind Python variables to interface components. Taking the simplest text example, I can set text display as follows: 12text = &#x27;My Text&#x27;tgb.text(&quot;&#123;text&#125;&quot;) In the above step, first assigning the variable text, then using &quot;&#123;text&#125;&quot; in the text control’s content section to bind the variable to the content might seem redundant, but this is very important in Taipy. In Taipy, all objects&#x2F;variables defined in the namespace are placed in a special object called state. In the code above, state.text is actually created, and any changes to state.text will be reflected in the tgb.text(&quot;&#123;text&#125;&quot;) text control. So if I want to update the displayed text, I just need to modify its content where I can access state, and the text will be modified and displayed on the page, like this: 1state.assign(text, &quot;Changed Text&quot;) Compared to commonly used Dash&#x2F;Streamlit&#x2F;Gradio, this saves writing code to update control content, making development faster. Especially for data display apps, often the chart and table drawing code is fixed, and we’re just reorganizing data frames or calculating metrics. Taipy’s design allows developers to focus on updating data rather than spending too much time on display aspects. Callback FunctionsLike other frameworks, various controls in Taipy can also set callback functions. For example, the most basic button has an on_action callback, which is triggered when the button is pressed. Different controls have different available callbacks, so you need to refer to the official documentation. Unlike other frameworks, Taipy can also set global default callback functions. We can directly define a function named on_action and place it in the namespace. Taipy’s logic is that if a control can trigger an on_action callback and there’s a function named on_action in the current namespace, the callback will be directly triggered. Therefore, in Taipy, using this global callback requires good logic judgment to avoid accidental callback triggers. Background TasksAlthough Taipy has built-in scheduling methods, they’re placed in the paid version. However, fortunately, in the community version, we can use invoke_long_callback for high-load task callbacks to achieve timed data refresh. I referenced an official example: 12345678910111213141516171819202122232425262728from random import randintfrom time import sleepdef countdown(): while True: sleep(10)def update_value(value): value = value + randint(1, 4) return valuedef update_display_value(state): state.total_client = update_value(state.total_client) state.total_visits = update_value(state.total_visits) state.total_reads = update_value(state.total_reads)def on_init(state): set_icon(state) invoke_long_callback( state, user_function = countdown, user_status_function = update_display_value, period = 3000 ) In the code above, invoke_long_callback is a function provided by Taipy. We use it to set up a high-load task, which is user_function. This function will be executed in a separate thread, then a thread is set up to check the high-load task results, calling the function passed in user_status_function to update the data in state. This way, once the data is updated, it triggers Taipy’s value binding logic, updating the content displayed on the page. Pitfall RecordsOriginally, I thought that since Taipy has a clear commercialization plan, the basic functions of this module should be quite good. However, during actual use, I encountered pitfalls that stumped me for over a day… Toggle Control in Version 4.1.0 Doesn’t Trigger in Theme ModeIn version 4.1.0 available through pixi&#x2F;conda, I found that the Toggle control with theme=True (set as theme toggle button) had ineffective value binding and function callbacks… This problem troubled me for a day and a half… I tried having AI give me many solutions, but Taipy doesn’t seem to be that popular a module, so I couldn’t directly inject JavaScript to solve it like with other frameworks. So I eventually went to look at the source code… I found that in version 4.1.0, the theme toggle control defined in ThemeToggle.tsx simply didn’t have value binding and callback logic… Only the latest main branch had it… Finally, I solved this problem by installing Taipy from source code… Stylekit Can’t Set All Colors at OnceAlthough the official documentation says that using Stylekit design allows developers to flexibly and concisely control theme colors, in practice I found that it can’t control the page’s overall background color, which needs to be solved with additional CSS: 1234/* Setting theme color */body &#123; background-color: var(--color-background);&#125; Plotly Doesn’t Have a Function for Drawing Timeline ChartsThis… is purely a complaint about Plotly. Although Python is widely used in data science, whether it’s Plotly, matplotlib, seaborn, or altair, the richness of their graphics still can’t match the ecosystem under R… Of course, someone has written it in R, but package management is too chaotic, and I can’t use others’ code, which is another matter… But fortunately, milestone timeline things are just points, lines, and some text, so I finally managed to create it manually with AI’s help… Insufficient Documentation on Plotly Configuration MethodsThis problem also stumped me for a long time. Taipy’s documentation explains how to not use Taipy’s interface but directly pass Plotly objects to draw graphics on the page. But how to configure Plotly objects, the documentation just says to refer to Plotly documentation. Then I experienced the problem with Plotly’s documentation - it seems very detailed but is actually quite poor in content. In their official documentation, many configuration items can only be passed when using fig.show()… rather than directly configuring the Plotly object… Then after going in circles, I found that Taipy’s chart control has a plot_config parameter… This can pass parameters equivalent to those in fig.show()…","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"Taipy","slug":"Taipy","permalink":"https://silenwang.github.io/en/tags/Taipy/"}]},{"title":"Attempting to Modify the Terminal App in ChromeOS (Part 2)","slug":"尝试修改chromeos中的终端app-中","date":"2025-09-05T01:53:10.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2025/09/05/尝试修改chromeos中的终端app-中/","permalink":"https://silenwang.github.io/en/2025/09/05/%E5%B0%9D%E8%AF%95%E4%BF%AE%E6%94%B9chromeos%E4%B8%AD%E7%9A%84%E7%BB%88%E7%AB%AFapp-%E4%B8%AD/","excerpt":"Picking up from where we left off, after successfully compiling the code in the libapp project, the next steps are: modifying the libapp code and applying the changes to the image.","text":"Picking up from where we left off, after successfully compiling the code in the libapp project, the next steps are: modifying the libapp code and applying the changes to the image. How the Terminal App WorksI’ve successfully seen the terminal app in the browser. To modify the code, I first need to understand how this application works. After some experimentation, my understanding of the terminal application is as follows: The terminal app consists of several pages. The outermost interface is actually the terminal.html page, which calls and loads a series of JS scripts within the project to render the page content When establishing an SSH connection, the application actually calls special chrome native APIs (similar to Electron?) available only in ChromeOS, opens a new window, and displays the terminal_ssh.html page. This page calls JS scripts in the nassh directory to perform the actual terminal rendering Starting an SSH connection actually passes SSH parameters to the terminal_ssh.html page, where scripts within the page construct the SSH command and hand it off to downstream APIs to establish the SSH connection Settings are also on a separate page, corresponding to terminal_settings.html Modifying the libapp CodeAfter understanding how the application works, the next step is to try modifying the code. Although I haven’t specifically studied HTML&#x2F;JS&#x2F;CSS web technology stacks and can roughly understand the code logic, I don’t plan to design and write code from scratch. I mainly refer to existing code and combine it to implement the functionality I need. From my own usage experience, the existing SFTP mount to local functionality is somewhat similar to the quick port forwarding feature I want: SFTP mounting requires setting up SSH details like port, user information, etc. Based on existing login entries, SFTP mounting is performed. Clicking mount opens a new page similar to SSH login, but it’s not interactive—it only displays mount information, and the mount automatically ends when the window is closed The functionality I want follows this logic: Port forwarding requires setting up SSH details like port, user information, etc. Based on existing login entries, port forwarding is performed. Clicking forward opens a window to set which port to forward Similar to SSH login, a new page opens but is not interactive—it only displays the forwarded port, and forwarding automatically ends when the window is closed The actual modified files are mainly JavaScript files in the terminal and nassh directories, primarily adding a forward button, and then referencing the SFTP functionality implementation to create a function that can perform port forwarding Applying Modifications to the ImageIn the ChromeOS source code, libapp is not the package name. After searching the project, I found that the compilation of these contents is actually in crosh-extention.ebuild. I found the directory for this ebuild, used git to generate a patch, and placed it in this directory 1git diff COMMIT_HASH_1 COMMIT_HASH_2 &gt; patchfile.patch I also needed to further modify the ebuild file by adding the following content. During the build, the patch will be automatically applied to modify the code 123PATCHES=( &quot;$&#123;FILESDIR&#125;&quot;/0001-forward.patch) Recompiling Packages and ImagesHere I encountered a pitfall: if you only modify files and compile directly using the previous method, the changes won’t actually be applied. According to this content I found, you need to use the cros_workon command in cros_sdk to mark specific packages for modification. This ensures that the compilation uses my modified ebuild file instead of fetching source code from a specific version. The compilation process can [refer to previous steps]. When executing build-packages, the tool automatically identifies changed packages, rebuilds them, and places the updated packages into the chroot environment. Flashing and Booting[Flashing hasn&#39;t changed much either]. However, after randomly modifying the kernel last time, my test device (the Duo) was bricked. So I referred to the [Fydetab Duo wiki instructions](https://wiki.fydetabduo.com/unbrick_the_fydetab_duo) to enter maskrom mode for flashing. Debugging?This actually leaves a question: no matter what development you do, you should theoretically complete testing in a test or virtual environment before conducting real device testing and debugging. Otherwise, you’ll waste a lot of time compiling and flashing back and forth. I think this is also the significance of various SDK and Studio suites. However, I currently don’t know… How should I debug such modifications before flashing? According to a blog I found, it seems possible to use the virtual machine functionality of cros_sdk, calling QEMU’s KVM virtual machine, which seems to allow debugging of compiled images. Next time, I’ll debug the functionality properly and show the results! For now, here’s a preview of the interface changes after flashing!","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"chromeos","slug":"chromeos","permalink":"https://silenwang.github.io/en/tags/chromeos/"},{"name":"terminal","slug":"terminal","permalink":"https://silenwang.github.io/en/tags/terminal/"},{"name":"libapp","slug":"libapp","permalink":"https://silenwang.github.io/en/tags/libapp/"},{"name":"terminal app","slug":"terminal-app","permalink":"https://silenwang.github.io/en/tags/terminal-app/"}]},{"title":"SteamOS is Indeed Maturing","slug":"SteamOS确实日趋成熟","date":"2025-08-25T00:53:31.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2025/08/25/SteamOS确实日趋成熟/","permalink":"https://silenwang.github.io/en/2025/08/25/SteamOS%E7%A1%AE%E5%AE%9E%E6%97%A5%E8%B6%8B%E6%88%90%E7%86%9F/","excerpt":"From last year to this year, I played two PC games: Black Myth and WuChang, both downloaded and played on their release days. This would have been hard to imagine five years ago. Back then, I had just finished Monster Hunter: World’s main story using a Windows KVM virtual machine with GPU passthrough. When the Iceborne expansion was released, Proton could already support quite a few older games, but the latest games were difficult to support at launch. I remember it took two to three months after Iceborne’s official PC release before the game could be launched, and there were still some bugs. Moreover, whenever Proton was updated, the game might become unplayable again…","text":"From last year to this year, I played two PC games: Black Myth and WuChang, both downloaded and played on their release days. This would have been hard to imagine five years ago. Back then, I had just finished Monster Hunter: World’s main story using a Windows KVM virtual machine with GPU passthrough. When the Iceborne expansion was released, Proton could already support quite a few older games, but the latest games were difficult to support at launch. I remember it took two to three months after Iceborne’s official PC release before the game could be launched, and there were still some bugs. Moreover, whenever Proton was updated, the game might become unplayable again… Later, I recall when Cyberpunk 2077 was released, it still couldn’t run on launch day. It took a few weeks before 2077 could run on the Proton compatibility layer. By then, I had upgraded my graphics card and tested it out. There were no major issues getting in, but Chinese fonts and some in-game videos had problems. This was quite acceptable… I just wasn’t that interested in Cyberpunk 2077’s theme, so I didn’t actually play it. For several years after that, I didn’t play PC games anymore. My computer was mainly used for analytical testing, and there wasn’t enough hard drive space for games. I mostly played on my Switch. Then came the release of Black Myth in 2024. Coincidentally, I had changed jobs and didn’t need to use my computer for work anymore, so I reinstalled the system and tried Bazzite, as it was the only SteamOS-like distribution that supported Nvidia cards at the time. Honestly, the performance exceeded my expectations—the game could be launched on release day. Although the smoothness wasn’t perfect, as someone who’s played as a “pixel hunter”… 540p at less than 30 FPS wasn’t unfamiliar to me. 1080p at 40-60 FPS was completely acceptable, and the game itself wasn’t too difficult, so I followed along with everyone and finished it. This year’s WuChang surprised me even more. I didn’t keep the Bazzite system—I just installed Nvidia’s proprietary drivers on Ubuntu 22.04, and the game could be launched directly. The performance was similar to last year’s Black Myth, and I could even stream it to my iPad (to use my NS controller) without lag… This means I should be able to try out domestically produced games as soon as they’re released from now on… Thank you, Gabe. I wish you the best in eventually overtaking Windows…","categories":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/categories/Gaming/"}],"tags":[{"name":"SteamOS","slug":"SteamOS","permalink":"https://silenwang.github.io/en/tags/SteamOS/"},{"name":"Bazzite","slug":"Bazzite","permalink":"https://silenwang.github.io/en/tags/Bazzite/"},{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/tags/Gaming/"},{"name":"Steam","slug":"Steam","permalink":"https://silenwang.github.io/en/tags/Steam/"}]},{"title":"Trying to Modify the Terminal App in ChromeOS (Part 1)","slug":"尝试修改chromeos中的终端app-上","date":"2025-08-21T00:13:10.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2025/08/21/尝试修改chromeos中的终端app-上/","permalink":"https://silenwang.github.io/en/2025/08/21/%E5%B0%9D%E8%AF%95%E4%BF%AE%E6%94%B9chromeos%E4%B8%AD%E7%9A%84%E7%BB%88%E7%AB%AFapp-%E4%B8%8A/","excerpt":"I’ve been using FydeOS&#x2F;ChromeOS for about two years now. While the system provides a usable terminal app, it’s honestly not that great. For example, during development, I often need to forward multiple ports. Although I can achieve this by entering SSH commands for port forwarding, this requires manually inputting quite a few parameters. Additionally, during port forwarding, I need to keep the SSH login window open. For someone like me who’s particularly obsessive about minimizing the number of open windows, keeping three or four windows open that won’t be used in the foreground is really uncomfortable… So I thought, can I do it myself, with the help of AI, modify the system’s default terminal client, and add quick forwarding functionality like VSCode has?","text":"I’ve been using FydeOS&#x2F;ChromeOS for about two years now. While the system provides a usable terminal app, it’s honestly not that great. For example, during development, I often need to forward multiple ports. Although I can achieve this by entering SSH commands for port forwarding, this requires manually inputting quite a few parameters. Additionally, during port forwarding, I need to keep the SSH login window open. For someone like me who’s particularly obsessive about minimizing the number of open windows, keeping three or four windows open that won’t be used in the foreground is really uncomfortable… So I thought, can I do it myself, with the help of AI, modify the system’s default terminal client, and add quick forwarding functionality like VSCode has? Cloning the CodeThe first step is to find out if the project code can be obtained. DeepSeek, Kimi, and ChatGPT can all give me the correct answer: https://chromium.googlesource.com/apps/libapps. Just clone this project locally. Project CompilationAlmost every directory in the project comes with documentation, some detailed and some brief. After reading through it and combining it with AI’s answers, the key folders are roughly as follows: hterm: A terminal emulator written in JS. All the content you see in the terminal after logging in is rendered by it nassh: The terminal emulator extension in the Chrome Web Store, combining content from hterm and ssh_client ssh_client: The part that communicates with OpenSSH terminal: The frontend of the terminal app we see Trying to Run Outside ChromeOSTo develop&#x2F;modify an application, you first need to set up the corresponding development and debugging environment. This was the most challenging part for me. Although the project documentation is quite rich, it’s not rich enough for a beginner like me to easily learn how to build a test version of the app that can be debugged. So with the help of AI, I found at least a solution that can show the interface: Use pixi to create a virtual environment (On my machine, this is necessary, otherwise the next step will report file copy permission errors)123456789[workspace]authors = [&quot;Sylens Wong &lt;qiumin14@163.com&gt;&quot;]channels = [&quot;conda-forge&quot;]name = &quot;fydeos_dev&quot;platforms = [&quot;linux-64&quot;]version = &quot;0.1.0&quot;[activation.env]PATH = &quot;$PIXI_PROJECT_ROOT/libapps/libdot/bin:$PATH&quot; Enter the virtual environment with pixi shell, then run the project’s built-in kokoro/build script to complete the build and testing of all sub-projects (Testing must be run completely, as the test part is directly written in) Enter the terminal directory, then copy some files needed by this frontend that aren’t in this project123cp ../node_modules/xterm/css/xterm.css css/ # Missing CSScp ../nassh/js/* js/ # Missing JS scriptscp -r ../nassh/_locales ../ # Missing international language files Run npm run start in the project root directory to start the HTTP service, then enter the following addresses in the browser to see the terminal’s frontend page: http://localhost:8080/terminal/html/terminal.html: Main page http://localhost:8080/terminal/html/terminal_settings.html: Settings page Although some content on the page is usable, such as adding SSH configurations, the core functionality like actually making SSH connections doesn’t work because this is just the frontend of the app. The actual connection&#x2F;terminal rendering and other content aren’t here. However, this is at least a small step forward~","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"chromeos","slug":"chromeos","permalink":"https://silenwang.github.io/en/tags/chromeos/"},{"name":"terminal","slug":"terminal","permalink":"https://silenwang.github.io/en/tags/terminal/"},{"name":"libapp","slug":"libapp","permalink":"https://silenwang.github.io/en/tags/libapp/"}]},{"title":"Am I Some Kind of Jinx?","slug":"我是什么天煞孤星体质么","date":"2025-07-27T19:57:09.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2025/07/27/我是什么天煞孤星体质么/","permalink":"https://silenwang.github.io/en/2025/07/27/%E6%88%91%E6%98%AF%E4%BB%80%E4%B9%88%E5%A4%A9%E7%85%9E%E5%AD%A4%E6%98%9F%E4%BD%93%E8%B4%A8%E4%B9%88/","excerpt":"When people are speechless, they really do laugh - my direct supervisor has resigned again and again and again. Why so many “agains”? Let me draw a timeline to see how many bosses I’ve “jinxed”.","text":"When people are speechless, they really do laugh - my direct supervisor has resigned again and again and again. Why so many “agains”? Let me draw a timeline to see how many bosses I’ve “jinxed”. Some time points may not be entirely accurate, possibly off by a month or two, but the jinxing is absolutely real… 2017.07-2018.01 Company: A bioinformatics service company in TianjinStory: My first job. Two months after probation, I was assigned to a major project team focused on personalized analysis. Then my team leader resigned and went to Beijing after receiving the year-end bonus. After that, I reported directly to the general manager. I also left in August 2018. 2018.08-2018.10 Company: A gene technology company in HangzhouStory: My second job, moving to Hangzhou. I came all the way from Tianjin for the interview. The team leader and department head were very nice, so I decided to join. But before I even passed probation, my entire department was dissolved… 2018.10-2019.04 Company: Same gene technology company in HangzhouStory: After department dissolution, I was transferred to an algorithm development team in another department. The team leader was a math PhD returnee, very nice. I thought this was a good outcome. Later I learned the department head wasn’t satisfied with the team leader (though the leader never complained). The leader resigned in April to pursue deep learning (in hindsight… such a wise decision). After the leader left, our team wasn’t merged. A colleague and I happily did nothing serious… while getting paid for months. Eventually feeling this wasn’t sustainable, I resigned again… 2019.07-2019.12 Company: A biotech company in HangzhouStory: Then came the company where I stayed the longest. When hired, they said they planned to recruit two people, but after I joined, they never found the second. The team leader said candidates kept getting rejected by management (I wasn’t interviewed by leadership due to special circumstances). At first it was fine - Monster Hunter World had just released on PC and I built a new computer, playing for 100+ hours. Then suddenly the team leader resigned… and I was appointed by the boss to replace him. 2020.04-2021.04 Company: Same biotech company in HangzhouStory: The exact timeline is fuzzy but the events are clear. There were two bioinformatics teams - mine was R&amp;D, the neighboring one was production. After becoming R&amp;D lead with no new hires, as R&amp;D tasks were limited, I ended up covering production too when that team collapsed. First a backup member resigned to get married, then the boss was dissatisfied with progress, leading to resignations and eventually the production lead being fired… The boss even discussed this with me… Of course I suggested finding replacements first, but no… So work for 5 people fell to me, and I got pulled into clinical trial applications… 2022.01-2023.12 Company: Still the same biotech companyStory: The longer I stayed, the more I jinxed. With clinical applications, workload became unbearable (9:30 AM to midnight). In 2022 I was allowed to hire - at peak we had 5 people. But as applications struggled and funding tightened, layoffs came… down to 3… The boss consulted me on who to keep - I suggested keeping the more stable members, but the boss prioritized ability… Predictably by January 2023 only 2 remained… Thankfully applications eventually succeeded, securing investment so the company survived (or I’d have jinxed the whole company). I resigned after the success. 2024.04-2025.01 Company: A medical technology company in HangzhouStory: Current company, same old story… Peaceful first half year, then the department imploded - first forced resignations, then my department head quit… Slightly different this time - we had both administrative and technical leadership, and it was the administrative head who left. 2025.01-2025.07 Company: Same medical technology companyStory: After another half year, this week the technical leader told me he’s resigning too… I could only laugh speechlessly… Looking back at my career, I’ve jinxed bosses, colleagues, and nearly the whole company… What kind of curse is this…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"Hangzhou","slug":"Hangzhou","permalink":"https://silenwang.github.io/en/tags/Hangzhou/"},{"name":"career","slug":"career","permalink":"https://silenwang.github.io/en/tags/career/"},{"name":"job-hopping","slug":"job-hopping","permalink":"https://silenwang.github.io/en/tags/job-hopping/"},{"name":"bioinformatics","slug":"bioinformatics","permalink":"https://silenwang.github.io/en/tags/bioinformatics/"}]},{"title":"What AI Can and Cannot Do - Two Failed Attempts","slug":"AI能做和不能做的-两次失败的尝试","date":"2025-07-15T00:44:45.000Z","updated":"2026-02-10T14:03:13.683Z","comments":true,"path":"2025/07/15/AI能做和不能做的-两次失败的尝试/","permalink":"https://silenwang.github.io/en/2025/07/15/AI%E8%83%BD%E5%81%9A%E5%92%8C%E4%B8%8D%E8%83%BD%E5%81%9A%E7%9A%84-%E4%B8%A4%E6%AC%A1%E5%A4%B1%E8%B4%A5%E7%9A%84%E5%B0%9D%E8%AF%95/","excerpt":"Compared to two years ago at my previous company, I now use AI much more frequently. If the limitation back then was the convenience of the tools themselves, the current limitation seems to be my own knowledge and technical skills rather than AI.","text":"Compared to two years ago at my previous company, I now use AI much more frequently. If the limitation back then was the convenience of the tools themselves, the current limitation seems to be my own knowledge and technical skills rather than AI. Last weekend I made two attempts. The first was trying to use Aider and Openhands to help me modify Theia’s remote module. According to AI’s explanation, Theia’s design is completely frontend-backend separated - the frontend only displays while the backend handles all requests. If this is correct, developing a remote version that works in the browser version of Theia should be feasible. With this, I could permanently solve my pain point when using VSCode on Fydetab Duo - insufficient graphics performance causing VSCode to lag. However, although both agents could provide some guidance, without basic knowledge of Node.js&#x2F;Electron, I couldn’t determine whether the code written by the agents was reliable just based on their responses. Since I also didn’t know how to debug a large Node.js project like Theia, I couldn’t identify the issues from error messages. In the end, with my current knowledge and skills, I still couldn’t complete the development under AI’s guidance. The second attempt was to solve the same problem but use a different method. Since I couldn’t build my desired IDE, could I improve the graphics performance of Fydetab Duo? Specifically, by updating the kernel to enable Vulkan support on FydeOS running on Duo. With Vulkan, VSCode’s performance might improve significantly. This attempt also failed. While AI guided me to find the new kernel source code that might work with RK3588, I couldn’t fully understand the descriptions about modifying&#x2F;replacing content during compilation. So I tried the most brute-force approach - directly replacing the kernel project repo in overlay, forcibly using 6.12 as 6.1. I thought at worst some features wouldn’t work? I underestimated it - it bricked my engineering version Duo… Again, the same problem - with my knowledge and skills, I didn’t know whether what I was doing was correct, and it was hard to get effective improvement information from the results. I only knew it didn’t work, but how to make it work? Based on two years of experience, blindly trying AI’s suggestions one by one isn’t reliable, as there might be many solutions that professionals would immediately recognize as wrong. Without any foundation, after endless trials, I couldn’t get any positive feedback. Therefore, I think the AI I can currently use is truly just a Copilot. I shouldn’t expect it to implement anything from scratch that I have no concept of. Only after establishing basic knowledge and being able to get effective feedback (at least being able to test and understand error messages) can I maximize its value as a Copilot. That is, going from 0 to 1 is still difficult, but after getting started, going from 1 to 10 should be easier than before. As for 10 to 100… I’ve never reached 100, so I really don’t know… But I think… these might be limited by context length.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://silenwang.github.io/en/tags/AI/"},{"name":"aider","slug":"aider","permalink":"https://silenwang.github.io/en/tags/aider/"},{"name":"openfyde","slug":"openfyde","permalink":"https://silenwang.github.io/en/tags/openfyde/"},{"name":"theia","slug":"theia","permalink":"https://silenwang.github.io/en/tags/theia/"}]},{"title":"Trying to compile openfyde again (Part 2)","slug":"再次尝试编译openfyde-下","date":"2025-06-19T23:09:23.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2025/06/19/再次尝试编译openfyde-下/","permalink":"https://silenwang.github.io/en/2025/06/19/%E5%86%8D%E6%AC%A1%E5%B0%9D%E8%AF%95%E7%BC%96%E8%AF%91openfyde-%E4%B8%8B/","excerpt":"After changing the motherboard and CPU, everything went smoothly… I’m not sure whether it was the motherboard or CPU that was causing the issue before - compilation would fail after tens of minutes, and subsequent attempts would fail at different points. This was actually the first time in my life I encountered real hardware incompatibility issues… Another “first time in my life” experience. So here comes the second part of compiling openfyde. Unfortunately, during the time I was struggling with the faulty hardware, the prebuilt image for r132-dev version was already updated… Missed the hot topic…","text":"After changing the motherboard and CPU, everything went smoothly… I’m not sure whether it was the motherboard or CPU that was causing the issue before - compilation would fail after tens of minutes, and subsequent attempts would fail at different points. This was actually the first time in my life I encountered real hardware incompatibility issues… Another “first time in my life” experience. So here comes the second part of compiling openfyde. Unfortunately, during the time I was struggling with the faulty hardware, the prebuilt image for r132-dev version was already updated… Missed the hot topic… Actually, I had already reached the final steps last time, but got stuck when compiling the massive Chrome project due to my hardware issues. After completing the compilation, we can generate the image that can be directly flashed to a USB drive. This time I actually solved a problem from last time - how to get a flashable image using Rockchip tools after obtaining chromiumos.bin. The tutorial gives us a bin image, while official prebuilt versions are in img format, clearly different. Also, unpacking official openfyde prebuilt images with Rockchip tools seems to yield some content not included in the bin. Actually, the official tools are available - it’s openFyde&#x2F;rk3588-image-maker. But last time as a complete beginner, I couldn’t understand what was happening during compilation, let alone figure out what all the projects under openfyde were for. The steps to generate the required image are simple: first clone this project, then go to the root directory and run the following commands as instructed: 1234# Mount required content from bin./map_chromiumos_image.sh /PATH/TO/CHROMIUMOS/IMAGE.bin --board fydetab_duo# Generate update.img./rk3588-mkupdate.sh Note: For the r132 version I compiled this time, the Image/parameter.txt file in the project needs modification - change 0xa0006d@0x00b02040(STATE) to 0xa0006d@0x00b02040(STATE). Then copy the generated image and flash it using the official Rockchip tool. The compiled image this time has working touchscreen, Bluetooth, and Linux containers - should be problem-free. Next step is to try adding stuff to it!","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"openfyde","slug":"openfyde","permalink":"https://silenwang.github.io/en/tags/openfyde/"},{"name":"fydeos","slug":"fydeos","permalink":"https://silenwang.github.io/en/tags/fydeos/"}]},{"title":"Compiling ARM version of Theia-IDE using GitHub Actions","slug":"使用github-action编译arm版的theia-ide","date":"2025-06-16T23:20:58.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2025/06/16/使用github-action编译arm版的theia-ide/","permalink":"https://silenwang.github.io/en/2025/06/16/%E4%BD%BF%E7%94%A8github-action%E7%BC%96%E8%AF%91arm%E7%89%88%E7%9A%84theia-ide/","excerpt":"Since my first encounter with ChromeOS&#x2F;FydeOS, I’ve been trying various VSCode-like editors. Recently, after learning about Huawei’s CodeArt and its upstream project Theia, I started tinkering again.Unfortunately, my current device is an 8GB FydeOS, not the 16GB Manjaro or PixelBook 2017 I used before. The available memory in the Linux container is very limited - I can’t even compile an ARM version of Theia-IDE browser edition… So… I had to “borrow” GitHub’s resources again.","text":"Since my first encounter with ChromeOS&#x2F;FydeOS, I’ve been trying various VSCode-like editors. Recently, after learning about Huawei’s CodeArt and its upstream project Theia, I started tinkering again.Unfortunately, my current device is an 8GB FydeOS, not the 16GB Manjaro or PixelBook 2017 I used before. The available memory in the Linux container is very limited - I can’t even compile an ARM version of Theia-IDE browser edition… So… I had to “borrow” GitHub’s resources again. This combines what I mentioned before about [Using Codespace to write blogs] and [Using GitHub Actions]. First, create a new project on GitHub, then click Code in the upper right corner to create a Codespace for this project, which allows you to use VSCode directly in the browser for coding and saving.I directly referenced my previous certimate_win7 project to create a GitHub workflow, making sure to use GitHub’s Arm Runner. The original Theia-IDE project doesn’t have official releases yet, so we can’t download pre-built releases by tags. Instead, we need to use checkout to get specific code versions. Then follow the instructions in the project’s browser.Dockerfile to set up the compilation steps. After that, package the entire code directory and upload it to my project’s Releases. After downloading the compiled files, install system-level dependencies as per the official instructions, then run yarn browser start to launch it. Since I’m using FydeOS, I compiled the browser version directly. If you need the desktop version, just follow the official instructions to generate the ARM version.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"theia","slug":"theia","permalink":"https://silenwang.github.io/en/tags/theia/"},{"name":"github","slug":"github","permalink":"https://silenwang.github.io/en/tags/github/"}]},{"title":"Trying to Compile openfyde Again (Part 1)","slug":"再次尝试编译openfyde-上","date":"2025-06-15T10:50:34.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2025/06/15/再次尝试编译openfyde-上/","permalink":"https://silenwang.github.io/en/2025/06/15/%E5%86%8D%E6%AC%A1%E5%B0%9D%E8%AF%95%E7%BC%96%E8%AF%91openfyde-%E4%B8%8A/","excerpt":"At the beginning of this year, I tried to compile openfyde, hoping to learn about compiling and modifying the openfyde system, and also to make use of my extra engineering machine. However, half a year has passed… and the issues from the last compilation still haven’t been resolved…","text":"At the beginning of this year, I tried to compile openfyde, hoping to learn about compiling and modifying the openfyde system, and also to make use of my extra engineering machine. However, half a year has passed… and the issues from the last compilation still haven’t been resolved… Seeing that the latest r132 code is being gradually updated recently, I decided to… try compiling it once again… This time I basically followed the INVALID POST SLUG PROVIDED 编译openfyde.md: installing system dependencies, downloading depot_tools, cloning the code, entering the chroot environment and starting compilation. The difference is that the r132 version code hasn’t been fully uploaded yet, so there are still some issues during compilation: The clone address for chromeos-kernel-6_1-6.1.115-r1676 hasn’t been modified yet, making it impossible to get the source code and compile There are still bugs in the libv4lplugins-0.0.1 code that require an additional patch to fix:1234567891011Index: libv4l-rkmpp/src/libv4l-rkmpp.c===================================================================--- libv4l-rkmpp.orig/src/libv4l-rkmpp.c+++ libv4l-rkmpp/src/libv4l-rkmpp.c@@ -1206,5 +1206,4 @@ .init = &amp;plugin_init, .close = &amp;plugin_close, .ioctl = &amp;plugin_ioctl,- .mmap = &amp;plugin_mmap, &#125;; Currently my old computer keeps crashing during compilation… I’ll update this post when it’s done…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"openfyde","slug":"openfyde","permalink":"https://silenwang.github.io/en/tags/openfyde/"}]},{"title":"Using Playwright for Browser Testing","slug":"使用playwright控制浏览器进行测试","date":"2025-06-09T00:44:33.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2025/06/09/使用playwright控制浏览器进行测试/","permalink":"https://silenwang.github.io/en/2025/06/09/%E4%BD%BF%E7%94%A8playwright%E6%8E%A7%E5%88%B6%E6%B5%8F%E8%A7%88%E5%99%A8%E8%BF%9B%E8%A1%8C%E6%B5%8B%E8%AF%95/","excerpt":"For websites requiring interactive operations, using pytest for API testing alone is insufficient as it doesn’t cover frontend interactions. Previously I used Selenium for browser control in web scraping tasks, but now I’ve found that Microsoft’s Playwright can also control browsers for automated testing, and it’s even simpler to use than Selenium since it supports automatic test case recording.","text":"For websites requiring interactive operations, using pytest for API testing alone is insufficient as it doesn’t cover frontend interactions. Previously I used Selenium for browser control in web scraping tasks, but now I’ve found that Microsoft’s Playwright can also control browsers for automated testing, and it’s even simpler to use than Selenium since it supports automatic test case recording. InstallationUsing pixi makes Playwright installation quick, but after installation you still need to call Playwright to install browser dependencies. This differs from Selenium which directly uses browsers supported by the system. 12# Install firefox and chromiumplaywright install --with-deps firefox chromium Using with PytestPlaywright supports multiple languages and also works with Python’s Pytest framework. Using the command playwright codegen -b firefox http://127.0.0.1:8000 to open the browser, then selecting Pytest in the recording window allows you to record Pytest code. A recorded test would look like this: 1234567891011121314151617BASE_URL = &#x27;http://127.0.0.1:8000&#x27;def test_add_sample(lab_mem_page): try: lab_mem_page.goto(f&quot;&#123;BASE_URL&#125;/page&quot;, wait_until=&quot;load&quot;) except TimeoutError: # Retry once if failed lab_mem_page.reload(wait_until=&quot;load&quot;) lab_mem_page.get_by_role(&quot;button&quot;, name=&quot;添加/修改样本&quot;).click() lab_mem_page.get_by_role(&quot;menuitem&quot;, name=&quot;添加样本&quot;).click() lab_mem_page.get_by_role(&quot;textbox&quot;, name=&quot;*内部编号&quot;).fill(&quot;CODE&quot;) lab_mem_page.get_by_role(&quot;textbox&quot;, name=&quot;*住院号&quot;).fill(&quot;6666&quot;) lab_mem_page.get_by_role(&quot;textbox&quot;, name=&quot;*姓名&quot;).fill(&quot;2222&quot;) lab_mem_page.get_by_role(&quot;textbox&quot;, name=&quot;*性别&quot;).click() lab_mem_page.get_by_role(&quot;listitem&quot;).filter(has_text=&quot;男&quot;).click() lab_mem_page.get_by_role(&quot;button&quot;, name=&quot;提交&quot;).click() expect(lab_mem_page.get_by_text(&quot;添加成功&quot;)).to_be_visible() After creating test cases, you can run tests and see if any content needs trimming or adjustment. Leveraging Pytest FeaturesWhen using Pytest for testing, Pytest’s fixture system and special conftest.py files are all available, allowing information sharing between tests just like regular Pytest testing to make tests more efficient and stable. Issues with Recording SystemWhile the recording feature significantly speeds up test writing, automatic recording doesn’t always perfectly select page elements, especially when interactive elements are dynamically generated. Therefore, to write stable test cases, some frontend knowledge is still needed to uniquely identify target elements in the resource tree.","categories":[{"name":"Script","slug":"Script","permalink":"https://silenwang.github.io/en/categories/Script/"}],"tags":[{"name":"python","slug":"python","permalink":"https://silenwang.github.io/en/tags/python/"},{"name":"pytest","slug":"pytest","permalink":"https://silenwang.github.io/en/tags/pytest/"},{"name":"playwright","slug":"playwright","permalink":"https://silenwang.github.io/en/tags/playwright/"}]},{"title":"Testing Network APIs with pytest","slug":"使用pytest进行测试网络API测试","date":"2025-06-09T00:25:03.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2025/06/09/使用pytest进行测试网络API测试/","permalink":"https://silenwang.github.io/en/2025/06/09/%E4%BD%BF%E7%94%A8pytest%E8%BF%9B%E8%A1%8C%E6%B5%8B%E8%AF%95%E7%BD%91%E7%BB%9CAPI%E6%B5%8B%E8%AF%95/","excerpt":"For the past six months, a significant part of my work has been maintaining the company’s website. Since I took over the code midway, there’s always a risk of introducing new bugs during maintenance. Therefore, as required by my supervisor, I learned how to implement automated testing.","text":"For the past six months, a significant part of my work has been maintaining the company’s website. Since I took over the code midway, there’s always a risk of introducing new bugs during maintenance. Therefore, as required by my supervisor, I learned how to implement automated testing. The testing framework used is pytest, the most popular testing framework in Python. The framework is very simple to use. Create files starting with test_, then write functions starting with test_ inside them. Within these functions, write test code and add assert statements to complete the most basic tests: 123456789BASE_URL = &#x27;http://127.0.0.1:4567&#x27;def test_login_success(): login_data = &#123;&quot;username&quot;: &quot;USER&quot;, &quot;password&quot;: &quot;123456&quot;&#125; response = requests.post(f&quot;&#123;BASE_URL&#125;/login&quot;, json=login_data) # Assert key results assert response.status_code == 200 assert &quot;accessToken&quot; in response.json().get(&quot;data&quot;) In actual testing, we often need to set up some shared information between different tests. For example, all the APIs I need to test require authentication, so it’s perfect to use pytest’s fixture system. We can first obtain a login token before testing begins, then use the same token to access APIs in each test function: 1234567@pytest.fixture(scope=&#x27;session&#x27;)def prod_admin_auth_header(): return auth_header(&quot;admin&quot;, &quot;123456&quot;)def test_get_dictionary_id(prod_admin_auth_header): response = requests.get(f&quot;&#123;BASE_URL&#125;/GetDictionaryid&quot;, headers=lab_mem_auth_header) assert response.status_code == 200 Here, functions marked as fixture can be directly passed into test functions as parameters by their function names, without requiring additional operations. Additionally, you can create a special file conftest.py in the test folder to store all the fixtures that need to be shared across the entire test suite. This way, in other test files, you can directly use the configured fixtures without manual imports.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"python","slug":"python","permalink":"https://silenwang.github.io/en/tags/python/"},{"name":"pytest","slug":"pytest","permalink":"https://silenwang.github.io/en/tags/pytest/"}]},{"title":"OMG...Our company server got hacked by malware","slug":"活久见公司服务器被恶意程序攻击了","date":"2025-06-07T00:32:44.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2025/06/07/活久见公司服务器被恶意程序攻击了/","permalink":"https://silenwang.github.io/en/2025/06/07/%E6%B4%BB%E4%B9%85%E8%A7%81%E5%85%AC%E5%8F%B8%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A2%AB%E6%81%B6%E6%84%8F%E7%A8%8B%E5%BA%8F%E6%94%BB%E5%87%BB%E4%BA%86/","excerpt":"Since I started playing with the ancient version of Linux Deepin in my junior year (2011), it’s been 14 years now. During this long period, I’ve used Debian&#x2F;RedHat&#x2F;Arch-based distros, but except for accidentally deleting my graduation thesis in my final year of grad school, I’ve never encountered any malware or viruses on Linux distributions. However last week, our company server… actually got infected…","text":"Since I started playing with the ancient version of Linux Deepin in my junior year (2011), it’s been 14 years now. During this long period, I’ve used Debian&#x2F;RedHat&#x2F;Arch-based distros, but except for accidentally deleting my graduation thesis in my final year of grad school, I’ve never encountered any malware or viruses on Linux distributions. However last week, our company server… actually got infected… At first when the maintenance vendor told me our server might be infected, I was skeptical, because the technical support staff seemed quite unreliable. I watched the whole remote operation process… their command familiarity… was even worse than mine… They just opened TOP and went through process names one by one, didn’t even know how to filter by user or sort by CPU usage. They even asked me about some obviously normal processes… After rebooting, they looked at TOP and said “seems fine now” (while CPU was still at 100%)… After messing around for a while, they just said “probably a mining virus, you need to reinstall the system” and stopped responding… Although our data and system are separated, reinstalling would still cost us extra time reconfiguring everything. Moreover, prescribing medicine without diagnosing the problem, doesn’t satisfy my curiosity might not actually solve the problem… So I decided to Google first… It really was a mining virusInitial Google searches didn’t yield much. Besides CPU being at 100% with no visible process responsible (which matched), netstat didn’t show any connections to mining pools, and systemctl mysteriously stopped working, preventing me from installing some tools mentioned in this blog. At this point I still wasn’t thinking virus, until I found a blog suggesting using lsattr and chattr to check&#x2F;modify systemctl permissions to fix software installation issues. I found that lsattr was missing from the server, and chattr gave a very strange error. I initially thought maybe these tools weren’t installed? Then I remembered - we have another server! After logging into that one… wow, the other server did have lsattr and chattr… I went back to check what chattr was on the infected server… cat revealed it had clearly been replaced! Then it hit me - top&#x2F;htop&#x2F;netstat etc. must have all been modified! Discovering malware modifications and countermeasures lsattr: Completely deleted, couldn’t check file attributes. Replaced with file from the other server. chattr: Replaced and set with ‘i’ and ‘a’ attributes so even root couldn’t delete it. Couldn’t reinstall chattr to fix. Replaced with file from the other server. top&#x2F;htop: Must have been modified to hide malicious processes. Replaced top from the other server, reinstalled htop. systemctl&#x2F;service management: Added a service to launch malware. Modified systemctl couldn’t show the malicious service. Removed immutable attributes and reinstalled systemd. ls: Replaced to hide malicious executables and service files. Replaced with file from the other server. reboot: Modified (SHA didn’t match the other server). Replaced with file from the other server, but still couldn’t reboot properly after fixing. netstat: Replaced to hide malicious network connections. Reinstalled net-tools. AfterwordI guess you really do learn from experience. I never imagined malware would evolve to… just modify all the tools you’d use to detect it…Also, I’ve… really become a sysadmin now…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://silenwang.github.io/en/tags/linux/"}]},{"title":"Using aider for programming assistance","slug":"使用aider辅助编程","date":"2025-04-25T00:42:46.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2025/04/25/使用aider辅助编程/","permalink":"https://silenwang.github.io/en/2025/04/25/%E4%BD%BF%E7%94%A8aider%E8%BE%85%E5%8A%A9%E7%BC%96%E7%A8%8B/","excerpt":"This year, DeepSeek has made LLMs popular again. Although after ChatGPT, Copilot and Cursor have been advertising everywhere, I never actually tried them… So this year, I… tried Cline, which is indeed good, but also matches my preconception - not that convenient. Especially since I need something that can be used directly in the command line and quickly integrated into scripts to batch complete some simple coding initialization tasks. That’s how I found aider…","text":"This year, DeepSeek has made LLMs popular again. Although after ChatGPT, Copilot and Cursor have been advertising everywhere, I never actually tried them… So this year, I… tried Cline, which is indeed good, but also matches my preconception - not that convenient. Especially since I need something that can be used directly in the command line and quickly integrated into scripts to batch complete some simple coding initialization tasks. That’s how I found aider… InstallationThe installation is quite simple since it’s a Python project, just use pip: 12python -m pip install aider-installaider-install Usageaider is a CUI application, just enter aider to start. Aider supports a series of chat commands starting with / that help you more efficiently manage code editing tasks, switch modes, and interact with the codebase. Here are some commonly used commands and their functions: File management commands /add: Add files to the chat session so Aider can edit or review them in detail. /drop: Remove files from the chat session to free up context space. /ls: List all known files and indicate which ones are included in the chat session. Editing and code operation commands /code: Request code changes, or switch to code mode if no prompt is provided. /diff: Show the diff of changes since the last message. /commit: Commit changes made to the codebase outside the chat (optional commit message). /undo: Undo Aider’s last Git commit. Mode switching commands /chat-mode: Switch to new chat mode. /architect: Enter architect&#x2F;edit mode, using two different models for code design or editing. /ask: Ask questions about the codebase without editing any files. Other utility commands /clear: Clear chat history. /copy: Copy the assistant’s last message to clipboard. /model: Switch to a new language model. /help: Get help about using Aider. /exit or /quit: Exit the Aider application. Embedding in shell scriptsAs a CUI program, aider can also be run directly from the command line by specifying parameters and writing the prompt directly in the command line (this is quite similar to ssh). Use Case 1 - Batch blog translationSince it can be run from the command line, aider can be conveniently embedded in shell scripts to batch complete some not-too-heavy tasks, like translating all past blogs… Although when ChatGPT first became popular, I thought about using it for translation, but after actual operation I found there was still significant manual work. Because I already have hundreds of blog files… Each time entering the prompt, then copying content in and out and proofreading was still tedious and boring work. Using aider, I can at least be a bit lazier. 1234567891011121314151617181920set -e DIR_A=blog_cn/source/_postsDIR_B=blog_en/source/_postsfor file in `ls $DIR_A/*`; do # Get filename (without path) filename=$(basename &quot;$file&quot;) # Check if file exists in directory B if [[ -e &quot;$&#123;DIR_B&#125;/$&#123;filename&#125;&quot; ]]; then echo &quot;File $&#123;filename&#125; exists in $&#123;DIR_B&#125;, skipping&quot; else echo &quot;File $&#123;filename&#125; doesn&#x27;t exist in $&#123;DIR_B&#125;, starting aider translation...&quot; echo &quot;Starting document proofreading&quot; aider \\ --no-show-model-warnings --yes --no-auto-commits \\ --message &quot;I have a blog post here, please translate it to English and save it to $&#123;DIR_B&#125;/$&#123;filename&#125;&quot; \\ $file fi Use Case 2 - Batch API test generationSimilar to blog generation, theoretically test code for interfaces can be quickly written based on OpenAPI or Swagger documentation, but if there are many interfaces, writing them manually is still troublesome. In such cases, having aider read the code and pre-generate a test code framework can save a lot of effort. 123456789for group in group1 group2 group3;do # Get filename (without path) pytest=test/$&#123;group&#125;.py touch $pytest aider \\ --no-show-model-warnings --yes --no-auto-commits \\ --message &quot;Here is an API&#x27;s Swagger description file $&#123;group&#125;.swagger.json, please write pytest test scripts based on its content and save the test scripts to $&#123;pytest&#125;.&quot; \\ data/$&#123;group&#125;.swagger.json $pytestdone","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://silenwang.github.io/en/tags/AI/"},{"name":"aider","slug":"aider","permalink":"https://silenwang.github.io/en/tags/aider/"}]},{"title":"Basic Usage of GitHub Actions","slug":"github-actions使用","date":"2025-04-13T16:40:47.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2025/04/13/github-actions使用/","permalink":"https://silenwang.github.io/en/2025/04/13/github-actions%E4%BD%BF%E7%94%A8/","excerpt":"In modern software development, Continuous Integration (CI) and Continuous Deployment (CD) are key practices for improving development efficiency and quality. GitHub Actions is a powerful automation tool provided by GitHub that helps developers easily implement CI&#x2F;CD and broader automation workflows.","text":"In modern software development, Continuous Integration (CI) and Continuous Deployment (CD) are key practices for improving development efficiency and quality. GitHub Actions is a powerful automation tool provided by GitHub that helps developers easily implement CI&#x2F;CD and broader automation workflows. The Role of GitHub ActionsGitHub Actions is a powerful automation platform that allows developers to define and run workflows in GitHub repositories. It can automatically execute a series of tasks (such as code building, testing, deployment, etc.) based on specific events (like code pushes, pull requests, tag releases, etc.). Through GitHub Actions, developers can achieve full automation from code submission to deployment, saving time and effort while reducing human errors. GitHub Actions Workflow StructureGitHub Actions workflows consist of several components, with the most important being jobs and steps. JobsA workflow can contain multiple jobs, each being an independent execution unit that can run in different environments. 123jobs: build: # Job name runs-on: [ubuntu-latest] # Runtime environment, here specifying GitHub-hosted runners runs-on specifies the environment for running the job - here we’re using GitHub’s latest Ubuntu environment. StepsEach job can contain multiple steps, where each step is an independent task that can execute a series of commands or call predefined actions. Predefined actions can be found in the GitHub Marketplace. 123- name: Get current tag # Step name id: get_tag # Step ID uses: zingimmick/github-action-get-current-tag@v1 # Using a specific pre-written step Variable PassingJust like in bioinformatics analysis pipelines I’ve written before, different steps often need to pass information to complete a task. As a simple configuration file, YAML can’t implement variable passing directly, so we need to rely on some environment variables set in the GitHub Actions framework. Passing Information Between StepsWhen executing code in steps, you can pass information between steps by appending content to the special environment variable $GITHUB_OUTPUT: 12345678- name: Save info id: save_info run: | echo &quot;tag=3&quot; &gt;&gt; $GITHUB_OUTPUT- name: Use info id: use_info run: | echo $&#123;&#123; steps.save_info.outputs.tag &#125;&#125; Passing Information Between JobsIn jobs, you can specify to pass outputs from specific steps to job inputs as follows: 1234567891011121314151617jobs: get_info: steps: - name: Save info id: save_info run: | echo &quot;tag=3&quot; &gt;&gt; $GITHUB_OUTPUT outputs: tag: $&#123;&#123; steps.save_info.outputs.tag &#125;&#125; use_info: needs: [get_info] # Set dependencies to get information from other jobs steps: - name: Save info id: save_info run: | echo $&#123;&#123; needs.get_info.outputs.tag &#125;&#125; Example: Building a Project and Publishing123456789101112131415161718192021222324252627282930313233343536373839404142name: BUILDon: push: tags: - &quot;v*&quot; # Triggered by version tagsjobs: build: runs-on: [ubuntu-latest] permissions: # Need write permission for releases contents: write steps: - name: Get current tag id: get_tag uses: zingimmick/github-action-get-current-tag@v1 - name: Checkout code uses: actions/checkout@v4 with: fetch-depth: 0 ref: $&#123;&#123; steps.get_tag.outputs.tag &#125;&#125; - name: Build frontend run: pixi run front_prod - name: Build backend run: pixi run backend - name: Pack to zip run: pixi run release - name: Release uses: ncipollo/release-action@v1.15.0 with: draft: false generateReleaseNotes: true # Automatically generate release notes artifacts: &#x27;$&#123;&#123; github.workspace &#125;&#125;/release/*.zip&#x27; tag: $&#123;&#123; steps.get_tag.outputs.tag &#125;&#125; name: Release $&#123;&#123; steps.get_tag.outputs.tag &#125;&#125;","categories":[{"name":"Script","slug":"Script","permalink":"https://silenwang.github.io/en/categories/Script/"}],"tags":[{"name":"github","slug":"github","permalink":"https://silenwang.github.io/en/tags/github/"},{"name":"github actions","slug":"github-actions","permalink":"https://silenwang.github.io/en/tags/github-actions/"}]},{"title":"Writing Objects in R","slug":"编写R中的对象","date":"2025-04-13T15:57:55.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2025/04/13/编写R中的对象/","permalink":"https://silenwang.github.io/en/2025/04/13/%E7%BC%96%E5%86%99R%E4%B8%AD%E7%9A%84%E5%AF%B9%E8%B1%A1/","excerpt":"Object-oriented programming is a common paradigm in programming. In my actual work, using object-oriented programming mainly serves two purposes: reducing duplicate code through inheritance features, and encapsulating frequently used data into objects to avoid excessive, repetitive, and nested parameter passing.","text":"Object-oriented programming is a common paradigm in programming. In my actual work, using object-oriented programming mainly serves two purposes: reducing duplicate code through inheritance features, and encapsulating frequently used data into objects to avoid excessive, repetitive, and nested parameter passing. Different Object Systems in RYes… R has different object systems. Besides the previously encountered S3 (like ggplot) and S4 (adopted by many single-cell packages), there’s R6 which we’ll focus on this time. S3 is the most basic and widely used object-oriented system in R, implementing method dispatch through generic functions. S3 objects essentially feel like extensions of lists, with relatively simple inheritance implemented via generic functions. This differs significantly from the class systems I’m more familiar with in Python. S4 provides a more formal object-oriented programming framework suitable for complex applications requiring strict class checking. It’s more complex than S3 with more comprehensive inheritance mechanisms, but its methods are still implemented through generic functions. While still different from Python’s approach, with inheritance and methods available, and the ability to conveniently use the %&gt;% pipe operator, S4 is somewhat usable. Originally I wanted to base my development on S4, but unexpectedly, S4 and box have compatibility issues. The author of box stated in 2022 that it wouldn’t be compatible with S4… So finally, I used the R6 class, which is a third-party package (yet another third-party one…). The objects it provides are very similar to Python objects - with inheritance, methods, and support for box::use. The only drawback is I haven’t found a way to integrate it with %&gt;%. Using R6 ObjectsHere’s a concrete example of using R6, which indeed closely resembles Python’s approach, though R6 doesn’t require defining an initialization function. 12345678910111213141516171819202122box::use( R6[R6Class])Person &lt;- R6Class( &quot;Person&quot;, public = list( name = NULL, age = NULL, initialize = function(name, age) &#123; self$name &lt;- name self$age &lt;- age &#125;, greet = function() &#123; cat(&quot;Hello, my name is&quot;, self$name, &quot;and I am&quot;, self$age, &quot;years old.\\n&quot;) &#125; ))# Using the objectperson &lt;- Person$new(&quot;John&quot;, 30) # Instantiationperson$greet() # Calling the method","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"rlang","slug":"rlang","permalink":"https://silenwang.github.io/en/tags/rlang/"},{"name":"object","slug":"object","permalink":"https://silenwang.github.io/en/tags/object/"}]},{"title":"Managing R Script Dependencies with the box Package","slug":"使用box包管理R脚本的依赖","date":"2025-04-13T15:07:54.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2025/04/13/使用box包管理R脚本的依赖/","permalink":"https://silenwang.github.io/en/2025/04/13/%E4%BD%BF%E7%94%A8box%E5%8C%85%E7%AE%A1%E7%90%86R%E8%84%9A%E6%9C%AC%E7%9A%84%E4%BE%9D%E8%B5%96/","excerpt":"After getting used to Python, developing scripts in R can feel quite painful. Putting aside the often-discussed issues with unclear error tracing, when scripts become slightly more complex and need to be split into multiple files, I realized R’s import mechanism is also quite frustrating… Fortunately, there’s the box package that allows module imports similar to Python’s logic.","text":"After getting used to Python, developing scripts in R can feel quite painful. Putting aside the often-discussed issues with unclear error tracing, when scripts become slightly more complex and need to be split into multiple files, I realized R’s import mechanism is also quite frustrating… Fortunately, there’s the box package that allows module imports similar to Python’s logic. Default Import Methods in RR has two built-in import methods: source and library. The former essentially runs all contents of the specified R file, while the latter requires packaging the code into an R package that needs to be installed before use. The former works well when writing relatively simple things, but problems arise when the functionality becomes more complex and needs to be split across multiple files: Namespace pollution: source() loads all objects from the script directly into the current environment, requiring additional configuration to prevent conflicts. Unclear dependencies: If there are dependencies between scripts, incorrect import order can cause errors. Difficult maintenance: From the source() part, it’s hard to see what exactly is being imported. When trying to find source code for modifications, you need to backtrack level by level, which is very troublesome. Using library is also problematic - besides the hassle of packaging each time, namespace pollution and difficulty tracing function&#x2F;object origins remain issues. In summary, while R’s built-in methods work, they’re not very user-friendly… Using BoxPython-style modularity: Supports box::use() syntax for module imports similar to Python’s mechanism, used as follows: 12345box::use( plots = ./modules/plot_functions, # Import ./modules/plot_functions.R based on path shiny_utils = shiny/utils, # Import file with object renaming ggplot2[ggplot, aes] # Import ggplot and aes from ggplot2) What makes this similar to Python is that it can import specific contents from an R file without importing everything. It can also import all contents of a file into an object, then call functions within the object using mod$func(), similar to how methods are used within objects in Python. Beyond these advantages, using box also enables relative path resolution, automatically identifying project root directories, avoiding the need to constantly switch directories with setwd(). This approach also considers cross-platform compatibility, automatically handling path format differences between Windows&#x2F;Linux&#x2F;macOS. Finally, using box solves the tracing problem during imports. Whether using mod$func() or partial imports like ggplot2[ggplot, aes], you can immediately see the source of functions&#x2F;objects being used, avoiding the need to search through piles of files.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"box","slug":"box","permalink":"https://silenwang.github.io/en/tags/box/"},{"name":"rlang","slug":"rlang","permalink":"https://silenwang.github.io/en/tags/rlang/"}]},{"title":"Compile openfyde","slug":"编译openfyde","date":"2025-01-04T14:58:33.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2025/01/04/编译openfyde/","permalink":"https://silenwang.github.io/en/2025/01/04/%E7%BC%96%E8%AF%91openfyde/","excerpt":"Compile the OpenFyde Image for Fydetab Duo by synthesizing official instructions and forum articles, aiming to compile the latest R120 version image. The goal is to compile the latest R120 version image.","text":"Compile the OpenFyde Image for Fydetab Duo by synthesizing official instructions and forum articles, aiming to compile the latest R120 version image. The goal is to compile the latest R120 version image. Fyde OS originally provided a domestic accelerated mirror, but it seems to be unavailable now. Therefore, a VPN is required to complete the following content. System Preparation System: Ubuntu 22.04, AMD Ryzen 5 1600 Six-Core @ 12x 3.2GHz, 64G Install necessary software: sudo apt-get install git gitk git-gui curl xz-utils python3-virtualenv python3-oauth2client lvm2 thin-provisioning-tools repo Install depot_tools: Clone the project: git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git &amp;&amp; cd depot_tools &amp;&amp; git checkout e121d14b12412e95ac833cfd31602b674499ea25 Set environment variables: export PATH=/mnt/hdd1/chromeos/openfyde/depot_tools:$PATH You can also set export DEPOT_TOOLS_UPDATE=0 to avoid tool updates. If the network is stable, this setting is optional. Api Key Preparation Since I am still using a Google account under Fydeos, I applied for a Google API key instead of Fydeos’s. The steps are described in the official documentation General steps: Join the chromium-dev development group Login to https://cloud.google.com/console Create a new project, e.g., openfyde. Select API Services &gt; Library, find the following API projects, and enable them one by one: Cloud Search API Google Drive API Safe Browsing API Time Zone API Go to API Services &gt; Credentials, configure OAuth information as prompted, then create credentials &gt; Create OAuth client ID. I chose the desktop application type (no Others option as described in the documentation). Create credentials &gt; Create API key to obtain the key. Save the client ID, client secret, and API key, and store them in the ~/.googleapikeys file as follows: 123&#x27;google_api_key&#x27;: &#x27;your api key&#x27;,&#x27;google_default_client_id&#x27;: &#x27;your client id&#x27;,&#x27;google_default_client_secret&#x27;: &#x27;your client secret&#x27;, Source Code Clone Create a directory: mkdir -p r120/openfyde Configure Git, otherwise cloning the code will result in errors: 12git config --global user.name &quot;sylens&quot;git config --global user.email &quot;silenseek14@gmail.com&quot; Clone the source code: The R126 version of openFyde/manifest.git is not updated, so only R120 can be compiled. repo sync -j8 requires downloading a large amount of source code, with chromium being the largest at nearly 40G. If the download fails due to network issues, running it again will restart the download. Ensure your VPN has sufficient bandwidth. 1234567repo init -u https://chromium.googlesource.com/chromiumos/manifest.git --repo-url https://chromium.googlesource.com/external/repo.git -b release-R120-15662.Bgit clone https://github.com/openFyde/manifest.git openfyde/manifest -b r120-devln -snfr openfyde/manifest .repo/local_manifestsrepo sync -j8cd openfyde/chromiumgclient sync Entering the Chroot Environment Return to the r120 directory and run cros_sdk to start downloading the SDK and preparing the environment. This process takes a long time. Starting the Compilation Since the current version of archero cannot be used, modifications are needed to avoid errors. After entering chroot, you will be in /mnt/host/source/src/scripts by default. The files that need modification are: /mnt/host/source/src/overlays/overlay-fydetab_duo-openfyde/metadata/layout.conf: Delete archero、tablet、ai-dev in the first line /mnt/host/source/openfyde/overlays/overlay-fydetab_duo-openfyde/profiles/base/parent: Delete the second line’sarchero:base Note: All compilation commands are executed within chroot. Run setup_board --board=fydetab_duo-openfyde It appears that the setup work is not fully completed, but the target directory has been generated. Install capnproto: sudo emerge capnproto Modify /mnt/host/source/chromite/lib/dlc_allowlist.py: Change line 14 to DLC_FACTORY_INSTALL = (r&quot;termina-dlc&quot;, r&quot;sample-dlc&quot;,), i.e., add r&quot;termina-dlc&quot; Run cros build-packages --jobs=4 &#39;--board=fydetab_duo-openfyde&#39; --no-withautotest --autosetgov --no-use-any-chrome After completion, build the image with cros build-image --board=fydetab_duo-openfyde --noenable_rootfs_verification Other Issues During Compilation Compilation interrupted abnormally: Encountered an abnormal interruption once. Re-entering the environment and compiling again caused inexplicable issues. To start over, delete the chroot and out folders, then run cros_sdk again from the beginning. Virtual hard disk settings: As hinted by a forum expert, an additional setting is required: Open /etc/sysctl.conf and add vm.max_map_count=524288. Run sudo sysctl -p.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"openfyde","slug":"openfyde","permalink":"https://silenwang.github.io/en/tags/openfyde/"},{"name":"chromeos","slug":"chromeos","permalink":"https://silenwang.github.io/en/tags/chromeos/"},{"name":"fydeos","slug":"fydeos","permalink":"https://silenwang.github.io/en/tags/fydeos/"}]},{"title":"Random Thoughts on FydeTab Duo","slug":"在搭载FydeOS的FydetabDuo上用Pixi配置工作环境","date":"2024-09-24T23:30:45.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2024/09/24/在搭载FydeOS的FydetabDuo上用Pixi配置工作环境/","permalink":"https://silenwang.github.io/en/2024/09/24/%E5%9C%A8%E6%90%AD%E8%BD%BDFydeOS%E7%9A%84FydetabDuo%E4%B8%8A%E7%94%A8Pixi%E9%85%8D%E7%BD%AE%E5%B7%A5%E4%BD%9C%E7%8E%AF%E5%A2%83/","excerpt":"","text":"When I initially participated in the crowdfunding on Indiegogo, I had already been studying and working on a pure Linux computer for at least six years. My primary systems had changed several times: Ubuntu, Linux Deepin (the ancient version of Deepin OS, which was a downstream of Ubuntu at that time), Linux Mint, Manjaro. Manjaro seemed to be the most problem-free and rolling updated distribution. Yet, it was inevitably hit with dependency issues after upgrades, preventing important work software from opening and thus delaying work. Therefore, after visiting the FydeOS website, I had the idea of trying again to use a stable, crash-free “netbook” for work. After all, in terms of computational stability and information security, it is a more ideal solution to perform all bioinformatics analysis tasks on servers or in the cloud. The local machine only needs the most basic tools (SSH) to connect to the server and then it should be able to handle documents, spreadsheets, and presentations at the most basic level. So, with a little bit of support for the new domestic direction, I joined the crowdfunding for Fydetab Duo, and while waiting, I began to learn about the ChromeOS system, and even later, I tried to work on a Chromebook for a full year. After a long wait, in May 2024, I finally received the 2024 version of Fydetab Duo, an open-source tablet using an RK3588s processor, which officially used the FydeOS system (though I had already obtained a second-hand engineering version from goofish.com two months ago). To be honest, even without comparing to full Linux distributions, but just comparing to ChromeOS, there are currently some issues that make this open-source tablet’s FydeOS uncomfortable to use. Some of these issues come from the Linux subsystem. The biggest problems are twofold: first, there is a probability that the subsystem will lose network connectivity after waking up. Once lost, I need to restart the network service within the subsystem, as well as the virtual machine. I haven’t found a pattern to this problem, and restarting the virtual machine often takes three to five minutes. This is quite annoying because there may be a computational process running in the subsystem, and during the waiting time, I will engage in compilation and environment preparation for various programs, which often require a network connection. If the network suddenly goes haywire… T T The other part comes from the hardware. I currently own two versions of FydeTab: a red engineering version and a gray 2024 retail version. The touchpad on the magnetic keyboard cover of both versions is identified as a mouse and cannot use a series of gestures in ChromeOS, which is very unnatural for me after a year of using ChromeOS. At the same time, the sensitivity of the touchpad needs adjustment; it is extremely prone to accidental touches, causing the mouse to move erratically while typing - a problem that was not present on the Chromebook 2017. Nevertheless, the fact that the company is still in operation and that I have received the device is already quite an achievement. I only hope that these issues can be slowly resolved in the future.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"fydeos","slug":"fydeos","permalink":"https://silenwang.github.io/en/tags/fydeos/"},{"name":"pixi","slug":"pixi","permalink":"https://silenwang.github.io/en/tags/pixi/"}]},{"title":"Deploying Hexo Blog with Pixi on Fydetab Duo","slug":"fydetab-duo-下使用-pixi部署hexo博客","date":"2024-08-04T13:04:05.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2024/08/04/fydetab-duo-下使用-pixi部署hexo博客/","permalink":"https://silenwang.github.io/en/2024/08/04/fydetab-duo-%E4%B8%8B%E4%BD%BF%E7%94%A8-pixi%E9%83%A8%E7%BD%B2hexo%E5%8D%9A%E5%AE%A2/","excerpt":"I’ve had the Fydetab Duo for over a month, and I’m still trying to find ways to make the most use of this device. At the very least, I’d like to write some blogs during my downtime with it. Hence the following exploration.","text":"I’ve had the Fydetab Duo for over a month, and I’m still trying to find ways to make the most use of this device. At the very least, I’d like to write some blogs during my downtime with it. Hence the following exploration. To be honest, as the Chinese version of Chrome OS, Fyde OS, the recommended method for using Linux should be directly through the Linux subsystem. However, within this month, the Linux subsystem seems to still have some issues that affect usage, such as not being able to connect to the internet for no apparent reason after waking up, mouse display anomalies within the subsystem, and relatively slow subsystem startup, which are minor but impactful issues. Plus, compared to Chrome OS, the Fyde OS team still has more lenient restrictions on the host system (at least sudo does not require switching to a different tty), so I tried to deploy the tools I needed on the host system. Installing Pixi To install pixi, additional environment variable configuration is required because under the Chromium OS-based system, not all places can store and run executables. /usr/local/share is the only choice, so you need to add the following content to the environment variable file, and then run curl -fsSL https://pixi.sh/install.sh | bash 12export PATH=/usr/local/share/pixi/bin:$PATHexport PIXI_HOME=/usr/local/share/pixi Installing code-server and gitThe original choice was Microsoft’s official vscode-cli tool, but possibly due to the differences in file structure and environment configuration between Fyde OS and general Linux distributions, it keeps getting stuck at the step of downloading the vscode server when connecting to the service, making it unusable in practice. Therefore, I temporarily used pixi to install code-server. Special Configuration for ARM Platforms Fydeos Tab is an ARM architecture device, so the configuration for pixi needs to include the linux-aarch64 platform. In practice, installing yarn has resulted in unrecognized sub-environment python issues, so Fydetab Duo does not use yarn and reverts to npm. Also, since Fyde OS does not have make, gcc, gxx by default, these components need to be added to the linux-aarch64 dependencies. The system information files in Fyde OS have minor differences from conventional Linux, which can cause hexo’s module to incorrectly parse system information, rendering hexo -v and the hexo g that calls it inoperative. Therefore, it is necessary to add a fixing statement in the deployment task. 12[target.linux-aarch64.tasks]build_blog = &#123;cmd = &quot;npm install -g hexo-cli &amp;&amp; npm install --python=$PWD/.pixi/envs/default/bin/python2 &amp;&amp; sed -i &#x27;34s/distro\\\\[1\\\\]/distro/&#x27; $PWD/.pixi/envs/default/lib/node_modules/hexo-cli/dist/console/version.js&quot;, cwd = &quot;.&quot;&#125; Cloning And Deploying the BlogThese are the additional configurations needed. Simply clone the project and deploy it directly, and you can continue writing blogs. 12git clone https://github.com/SilenWang/silen_blogpixi run build_blog","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://silenwang.github.io/en/tags/hexo/"},{"name":"fydeos","slug":"fydeos","permalink":"https://silenwang.github.io/en/tags/fydeos/"},{"name":"pixi","slug":"pixi","permalink":"https://silenwang.github.io/en/tags/pixi/"},{"name":"fydetab duo","slug":"fydetab-duo","permalink":"https://silenwang.github.io/en/tags/fydetab-duo/"}]},{"title":"Using a new R Jupyter kernel to solve the issue of monocle3 not being able to select developmental starting points in notebooks","slug":"使用新的R语言jupyter内核解决monocle3无法在notebook上选发育起始点的问题","date":"2024-06-23T01:47:03.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2024/06/23/使用新的R语言jupyter内核解决monocle3无法在notebook上选发育起始点的问题/","permalink":"https://silenwang.github.io/en/2024/06/23/%E4%BD%BF%E7%94%A8%E6%96%B0%E7%9A%84R%E8%AF%AD%E8%A8%80jupyter%E5%86%85%E6%A0%B8%E8%A7%A3%E5%86%B3monocle3%E6%97%A0%E6%B3%95%E5%9C%A8notebook%E4%B8%8A%E9%80%89%E5%8F%91%E8%82%B2%E8%B5%B7%E5%A7%8B%E7%82%B9%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"Looking at the creation time of this draft, it was actually June 2024… Now it’s June 2025, and I suddenly understand why so many content creators become “pigeons” (procrastinators). Starting projects is fun, but finishing them is painful… This is one of my few pure bioinformatics posts… The problem originated last year when I needed to run monocle3 for pseudotime analysis, but encountered an annoying issue at the final stage. In monocle3, the starting point for pseudotime trajectory needs to be manually specified by the analyst. During R code execution, it automatically opens a browser where users need to specify the starting point on a temporary webpage, then close the page for the analysis to continue. However, Jupyter’s irkernel doesn’t support this feature. This means I couldn’t complete the analysis directly in Jupyter notebook. This issue was first reported in 2019, but even by 2024 when I needed to do the analysis - five years later - there was still no solution…","text":"Looking at the creation time of this draft, it was actually June 2024… Now it’s June 2025, and I suddenly understand why so many content creators become “pigeons” (procrastinators). Starting projects is fun, but finishing them is painful… This is one of my few pure bioinformatics posts… The problem originated last year when I needed to run monocle3 for pseudotime analysis, but encountered an annoying issue at the final stage. In monocle3, the starting point for pseudotime trajectory needs to be manually specified by the analyst. During R code execution, it automatically opens a browser where users need to specify the starting point on a temporary webpage, then close the page for the analysis to continue. However, Jupyter’s irkernel doesn’t support this feature. This means I couldn’t complete the analysis directly in Jupyter notebook. This issue was first reported in 2019, but even by 2024 when I needed to do the analysis - five years later - there was still no solution… But I did accidentally discover a solution when GitHub recommended me a project called jupyter-xeus. This project aims to develop a new series of Jupyter kernels, including a new R kernel called xeus-r. This new kernel supports interactive mode and can generate a link when running monocle3. Users can click the link to complete point selection, then exit the page to continue analysis. xeus-r is available on conda and can be quickly installed using conda&#x2F;mamba&#x2F;pixi. After installation, you’ll see the corresponding xr kernel in Jupyter’s interface - just select it to use. The code to execute is: 1234library(monocle3)cds &lt;- readRDS(&quot;/Your/rds/contain/monocle3/object.RDS&quot;)options(browser=&quot;firefox&quot;)cds &lt;- order_cells(cds) After running, you’ll see the link to open below the cell. Open it, select the starting point, then close it to continue.","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"jupyter","slug":"jupyter","permalink":"https://silenwang.github.io/en/tags/jupyter/"},{"name":"monocle3","slug":"monocle3","permalink":"https://silenwang.github.io/en/tags/monocle3/"},{"name":"singlecell","slug":"singlecell","permalink":"https://silenwang.github.io/en/tags/singlecell/"},{"name":"xeus","slug":"xeus","permalink":"https://silenwang.github.io/en/tags/xeus/"}]},{"title":"New Tools Solve Old Problems - Using Pixi for Bioinformatics Environment Configuration","slug":"新工具解决老问题-使用pixi进行生物信息环境配置","date":"2024-06-22T01:26:08.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2024/06/22/新工具解决老问题-使用pixi进行生物信息环境配置/","permalink":"https://silenwang.github.io/en/2024/06/22/%E6%96%B0%E5%B7%A5%E5%85%B7%E8%A7%A3%E5%86%B3%E8%80%81%E9%97%AE%E9%A2%98-%E4%BD%BF%E7%94%A8pixi%E8%BF%9B%E8%A1%8C%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/","excerpt":"Bioinformatics is an interdisciplinary field, and the toolkit or technology stack used in bioinformatics is also quite “interdisciplinary”. The level of fragmentation is, in my opinion, absolutely not less than that of Linux distributions… This also brings us a common challenge: the deployment of bioinformatics analysis environments.","text":"Bioinformatics is an interdisciplinary field, and the toolkit or technology stack used in bioinformatics is also quite “interdisciplinary”. The level of fragmentation is, in my opinion, absolutely not less than that of Linux distributions… This also brings us a common challenge: the deployment of bioinformatics analysis environments. Deploying bioinformatics environments is a hassleIn the development process, one would initially select a programming language, subsequently install the appropriate development environment, and thereafter utilize existing environment or package management tools to handle library dependencies.However, bioinformatics, like medical statistics I majored in before, was aimed to solve problem first, using whatever is available, and writing codes only if it is needed… Therefore, encountering multiple languages is a highly likely event. Personally, due to the cutting-edge research field of my previous company… I often have to use tools written by university researchers based on the principle of “whatever works, use it”, and I have come into contact with a bunch of languages: C, C++, Java, Scala, Perl, Python, R, Go, Julia, Nim, Js… If you also count database languages, configuration file syntax, and the like… The quantity could double again… Such a messy technology stack makes deploying and setting up a bioinformatics development environment or deploying bioinformatics analysis workflows a very cumbersome task. It’s important to know that a significant proportion of tools or software in bioinformatics may no longer be maintained after about a year, which means that the compilers and libraries required by the software will remain outdated, and gradually start to conflict with subsequent versions, causing a more terrifying dependency explosion problem than inter-version upgrades of Linux distributions. Then this dependency is not only going to explode in a single language’s toolkit, but there is also a possibility of explosions in toolkits of all languages, and due to the C foundation in Python, R and other scripting languages, there can also be cross explosions… It can make a person’s mentality explode in minutes. However… The problem is far from over, for some adult reasons… Not everyone can get system-level permissions, and when configuring the environment, they can only use the permissions of an ordinary user, so some problems that can be solved by packaging with the distribution’s package manager may have to be solved by recompiling the entire software’s dependencies… This is enough to make a person explode for a month… Based on the aforementioned annoying issues, when I first learned about the tool called conda, I really felt it was a lifesaver… A large number of bioinformatics software are pre-compiled, packaged, and uploaded, ready to use after download and decompression, and it can automatically calculate if dependencies conflict, ensuring the basic use of tools in different languages, which is simply perfect! However, after using it for a about a week, you will find that… due to being based on Python, the speed of conda’s dependency calculation… is really slow. It might take about 20 minutes to calculate dependencies for around 10 tools, which is especially annoying when there are dependency issues… It’s almost like running 20 minutes of calculations, only to find out there’s a problem with the variable names, and no breakpoints are set, so you have to start over… Perhaps this downside is so significant that it affects general usage, which led to the development of the C++ implementation of Conda: Mamba, and then came micromamba, and later today’s main character, pixi. Introduction to PixiIn simple terms, pixi is a multi-language software package manager designed for developers, aimed at making it easy for developers to create and lock down multi-language development environments, making computational results easier to replicate. The simple usage of pixi (via the command line) can be found in its official documentation, and I won’t go into detail here because for me, I mainly use it for environment deployment, where a lot of things are installed at once, so I generally use configuration files for this. Basic Usage of PixiPixi uses configuration files in the toml format (and here we have another one), which is somewhat like ini… but with more complex syntax and the ability to support more features. If you want to manage with toml, after installing pixi, you can create a managed project by using pixi init. This command will generate a configuration file in the current directory, and the newly generated file will contain the following content. 1234567891011[workspace]name = &quot;demo&quot;version = &quot;0.1.0&quot;description = &quot;Add a short description here&quot;authors = [&quot;silenwang &lt;silenseek14@gmail.com&gt;&quot;]channels = [&quot;conda-forge&quot;]platforms = [&quot;linux-aarch64&quot;][tasks][dependencies] It already includes the basic information of the project, tasks, and dependencies, which are the most important and main contents. If tasks are not used, they can be left blank. In the dependencies section, you should write down the desired installations (contents will be retrieved from conda). After saving, you can use pixi install to install the required contents. During the installation process, a lock file will also be generated. The role of this file is similar to the lock files generated by npm or yarn in nodejs, which is used to record the versions of installed software. By managing this lock file along with the project, you can ensure that the same dependencies are installed again when deploying, guaranteeing the reproducibility of software operation or calculations. Handling Multiple Dependencies at OnceAlthough pixi’s software is mainly sourced from conda, it also supports fetching Python packages from pypi. If there are packages not available on conda but can be obtained from pypi, you can add a pypi section in the configuration file. The software within this section will be automatically fetched from pypi and installed after the conda dependencies have been processed, and the obtained packages will also be recorded in the lock file. 12[pypi-dependencies]spats-shape-seq = &quot;*&quot; If there are additional packages that need to be compiled and installed separately, or further processed with scripts after downloading, you can use the task feature to set up tasks. Tasks set up in the task block can be executed by running pixi run TASK_NAME. Although the official documentation does not currently provide a direct way to run specific tasks after dependency processing, you can achieve the goal of automatically proceeding with subsequent compilation and installation tasks after dependency processing by executing the pixi install -a command through a task, as shown below: 1234567891011121314151617181920212223[workspace]name = &quot;demo&quot;version = &quot;0.1.0&quot;description = &quot;Add a short description here&quot;authors = [&quot;silenwang &lt;silenseek14@gmail.com&gt;&quot;]channels = [&quot;conda-forge&quot;, &quot;bioconda&quot;, &quot;milaboratories&quot;, &quot;main&quot;]platforms = [&quot;linux-64&quot;][tasks]install = &#123;cmd = &#x27;pixi install -a&#x27;&#125;deploy = &#123;cmd = &#x27;echo &quot;Deploy done&quot;&#x27;, depends-on = [&quot;instal&quot;] &#125;[feature.python3.dependencies]python = &quot;3.11.*&quot;pip = &quot;*&quot;jupyter = &quot;*&quot;ipykernel = &quot;*&quot;numpy = &quot;*&quot;pandas = &quot;*&quot;statsmodels = &quot;*&quot;scipy = &quot;*&quot;tensorflow = &quot;*&quot;keras = &quot;*&quot; Deploy Multiple Environments at OncePixi also supports the deployment of multiple environments at once, and the general syntax is as follows: 12345678910111213141516171819202122232425262728293031323334[feature.python3.dependencies]python = &quot;3.11.*&quot;pip = &quot;*&quot;jupyter = &quot;*&quot;ipykernel = &quot;*&quot;numpy = &quot;*&quot;pandas = &quot;*&quot;statsmodels = &quot;*&quot;scipy = &quot;*&quot;tensorflow = &quot;*&quot;keras = &quot;*&quot;[feature.python2.dependencies]python = &quot;2.7.*&quot;pip = &quot;*&quot;jupyter = &quot;*&quot;ipykernel = &quot;*&quot;[feature.bioinfo.dependencies]bwa = &quot;*&quot;samtools = &quot;*&quot;bedtools = &quot;*&quot;[feature.rlang.dependencies]r-base = &quot;*&quot;r-irkernel = &quot;*&quot;r-ggplot2 = &quot;*&quot;bioconductor-limma = &quot;*&quot;[environments]python3 = [&quot;python3&quot;]python2 = [&quot;python2&quot;]bioinfo = [&quot;bioinfo&quot;]rlang = [&quot;rlang&quot;] Specify the environment names in environments, and then use blocks like [feature.rlang.dependencies] to manage the dependencies within the environment. Once configured, use pixi install -a to install all environments (using pixi install will only check for dependency issues but will not actually install them). Environment UsageAlthough pixi uses packages from conda, its environment activation and usage methods follow those of poetry and pipenv. To activate an environment, run pixi shell in the directory; if there are multiple environments, add -e to specify the environment name. At the same time, pixi also supports running software within the environment using pixi run (yes, pixi run is used both for running tasks and for running commands within the environment). An ExampleThe following pixi.toml comes from a real single‑cell analysis project. It defines several environments (rplot, analy, and label) and uses R packages such as Seurat and azimuth in the label environment. Similar to the situation mentioned earlier, even after explicitly declaring r-BiocManager, you may still encounter missing data packages such as GenomeInfoDbData, BSgenome.Hsapiens.UCSC.hg38, etc. Therefore, we have configured four separate installation tasks inside [feature.label.tasks] and linked them together via a r_dep task. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667[workspace]authors = [&quot;Sylens Wong &lt;silenseek14@gmail.com&gt;&quot;]channels = [&quot;conda-forge&quot;, &quot;bioconda&quot;, &quot;dnachun&quot;]name = &quot;Single Cell&quot;platforms = [&quot;linux-64&quot;]version = &quot;0.1.0&quot;[environments]rplot = [&#x27;kernel&#x27;, &#x27;rplot&#x27;]analy = [&#x27;kernel&#x27;, &#x27;analy&#x27;]label = [&#x27;kernel&#x27;, &#x27;label&#x27;][feature.kernel.dependencies]ipykernel = &#x27;*&#x27;r-irkernel = &#x27;*&#x27;jupyterlab = &#x27;*&#x27;[feature.rplot.dependencies]r-ggpubr = &#x27;*&#x27;r-ggforce = &#x27;*&#x27;r-ggh4x = &#x27;*&#x27;bioconductor-complexheatmap = &#x27;*&#x27;[feature.label.dependencies]r-base = &#x27;*&#x27;r-azimuth = &#x27;*&#x27;r-seurat = &#x27;5.2.*&#x27;r-SeuratObject = &#x27;5.0.*&#x27;r-BiocManager = &#x27;*&#x27;r-SeuratDisk = &#123;version = &quot;*&quot;, channel = &quot;dnachun&quot;&#125;[feature.label.tasks]GenomeInfoDbData = &#123;cmd = &#x27;Rscript -e &quot;BiocManager::install(\\&quot;GenomeInfoDbData\\&quot;)&quot;&#x27;&#125;BSgenome = &#123;cmd = &#x27;Rscript -e &quot;BiocManager::install(\\&quot;BSgenome.Hsapiens.UCSC.hg38\\&quot;)&quot;&#x27;&#125;EnsDb = &#123;cmd = &#x27;Rscript -e &quot;BiocManager::install(\\&quot;EnsDb.Hsapiens.v86\\&quot;)&quot;&#x27;&#125;JASPAR2020 = &#123;cmd = &#x27;Rscript -e &quot;BiocManager::install(\\&quot;JASPAR2020\\&quot;)&quot;&#x27;&#125;r_dep = &#123;cmd = &#x27;echo &quot;bio dep for R done&quot;&#x27;, depends-on=[&#x27;GenomeInfoDbData&#x27;, &#x27;BSgenome&#x27;, &#x27;EnsDb&#x27;, &#x27;JASPAR2020&#x27;]&#125;[feature.analy.dependencies]snakemake = &#x27;9.8.*&#x27;python = &#x27;3.11.*&#x27;jupyter = &#x27;*&#x27;scanpy = &#x27;*&#x27;gseapy = &#x27;*&#x27;gprofiler-official = &#x27;*&#x27;altair = &#x27;*&#x27;scipy = &#x27;*&#x27;pip = &#x27;*&#x27;pandas = &#x27;*&#x27;openpyxl = &#x27;*&#x27;leidenalg = &#x27;*&#x27;numpy = &#x27;*&#x27;loompy = &#x27;*&#x27;pixi-kernel = &#x27;*&#x27;vegafusion-python-embed = &quot;*&quot;vegafusion = &quot;*&quot;vl-convert-python = &quot;*&quot;pypairs = &#x27;*&#x27;harmonypy = &#x27;*&#x27;upsetplot = &#x27;*&#x27;[feature.analy.pypi-dependencies]singler = &#x27;*&#x27;celldex = &#x27;*&#x27;[tasks]install = &#123;cmd = &#x27;pixi install -a&#x27;&#125; In this configuration, executing pixi run r_dep will automatically install the four data packages mentioned above, thereby ensuring that packages like Seurat can load the required genome annotation information properly. This pattern can easily be extended to other Bioconductor dependency scenarios that need extra data packages.","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"conda","slug":"conda","permalink":"https://silenwang.github.io/en/tags/conda/"},{"name":"pixi","slug":"pixi","permalink":"https://silenwang.github.io/en/tags/pixi/"}]},{"title":"Still Alive But Pretty Tiring","slug":"还活着就是比较累","date":"2023-12-24T14:47:47.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2023/12/24/还活着就是比较累/","permalink":"https://silenwang.github.io/en/2023/12/24/%E8%BF%98%E6%B4%BB%E7%9D%80%E5%B0%B1%E6%98%AF%E6%AF%94%E8%BE%83%E7%B4%AF/","excerpt":"","text":"Time flies… There are not many days left in 2023, but I haven’t written anything from August to December… Actually, I’ve learned a lot of new things these months, both at work and outside of it. But I haven’t had time to organize or write about them, and I might forget them soon… So when I saw a blogger’s post earlier (I can’t find the source, hhhhh), they were right. Many tech bloggers get caught up in fiddling with various blog frameworks and flashy plugins, and end up neglecting the act of writing itself due to distractions and mistakes. I will continue to stick to writing, regardless of how good dual-link notes or other tools are. Continuously writing and organizing is the most important thing. After all, work never ends; it’s just busy and busier. I still need to worry and be anxious, and even more so because life has become busier with more trivial matters, leading to increased anxiety…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"Complaining","slug":"Complaining","permalink":"https://silenwang.github.io/en/tags/Complaining/"}]},{"title":"Chromebook Tinkering Report","slug":"chromebook折腾报告","date":"2023-08-16T11:11:11.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2023/08/16/chromebook折腾报告/","permalink":"https://silenwang.github.io/en/2023/08/16/chromebook%E6%8A%98%E8%85%BE%E6%8A%A5%E5%91%8A/","excerpt":"Last year, I suddenly felt unhappy with the old laptop lying on my bed. It was a relatively expensive purchase for me (a big expense six months after graduation), and it had keyboard and touchpad issues that required two repairs costing another 1000 yuan each. Given its high cost, I didn’t want to leave it idle, so I decided to give fydeos another chance. After some research, I found that both deepin and fydeos have evolved significantly in recent years. Fydeos has become more user-friendly, and they now offer Android as well as a Linux subsystem, making daily use much more accessible. Therefore, I decided to buy a Lenovo Chromebook Duet.","text":"Last year, I suddenly felt unhappy with the old laptop lying on my bed. It was a relatively expensive purchase for me (a big expense six months after graduation), and it had keyboard and touchpad issues that required two repairs costing another 1000 yuan each. Given its high cost, I didn’t want to leave it idle, so I decided to give fydeos another chance. After some research, I found that both deepin and fydeos have evolved significantly in recent years. Fydeos has become more user-friendly, and they now offer Android as well as a Linux subsystem, making daily use much more accessible. Therefore, I decided to buy a Lenovo Chromebook Duet. Chip Architecture and Device AvailabilityDon’t doubt my mental state; although the content above and the title below might seem like I’m drunk, they are actually logical!Firstly, I know that fydeos has become quite different from the original ChromeOS. Buying a Chromebook to run fydeos is not advisable because fydeos is a company that primarily serves enterprise businesses rather than individual consumers. They don’t have many hardware products they design and maintain long-term. At the time I had this idea, the old ITN devices were no longer available for purchase, and the Fydetab Duo crowdfunding campaign hadn’t started yet. The second-hand FYDEOS version was also quite expensive. Therefore, I decided to buy a cheap ChromeOS tablet to try it out. After all, there are so many of them in the seafood market; if it works well, I’ll use it; otherwise, I won’t sell it (and end up losing 200 yuan). When choosing the trial device, I also considered for a long time before deciding on an ARM architecture Duet. This was mainly to get a better Android experience. When using fydeos, I found that this type of system’s Android is not completely isolated from the host system; files generated by Android can be easily read from the host system, and files from the host system can also be right-clicked and shared with Android applications. This is much more convenient than I expected. Additionally, all devices on the host, including the camera, are directly callable by Android applications, allowing for full functionality of Android apps to supplement the ecosystem. However, many domestic apps do not support x86 architecture, causing them to crash or fail to open entirely, making them unusable. Therefore, if I buy an ARM architecture device, it might be able to completely replace my daily notebook and be even lighter, which would be great. Device Positioning and Old Love New RomanceHowever,事实证明我真的很容易在电子产品上上手，过去如此，现在如此，未来估计还是如此。 In the two days after I started using the Chrome Duet, I immediately felt that it had significant problems. A cheap netbook’s configuration naturally won’t be great, although an ARM architecture CPU allows Android apps to run smoothly and efficiently. However, wanting to use it as a work device while only being able to run one app fluently is clearly not enough… The Chrome Duet can run WeChat and DingTalk well, but when I open the browser, either WeChat or DingTalk will be terminated, requiring them to be restarted each time. Switching tabs in Chrome also takes 30 seconds, which is extremely frustrating even if my boss doesn’t notice me multitasking. A 4G memory for a mobile device might seem adequate, but given the bloatware of domestic apps… it feels like I’ve already failed. Apart from obvious application issues, there were hardware problems as well. The Chrome Duet is a two-in-one tablet; if we only consider its main body, its weight and battery life are even better than my previously owned Huawei tablet. However… when adding the keyboard case… the overall weight of the device becomes confusing… So you say it’s light, but it’s not really that light. In terms of typing, since it’s a keyboard case with a stand-up form factor, this device is inconvenient for use on a lap without a table. Therefore, in scenarios where there’s no desk, it’s immediately discarded… It feels like losing the soul of a writing and social office device. The most frustrating thing is its size—10.1 inches means that the accompanying keyboard is much smaller than a regular notebook. I don’t mind the rebound effect, but switching between a 10.1-inch small keyboard and a 14-inch regular-sized keyboard is really annoying, similar to playing塞尔达 on an NS and then switching to an Xbox controller for Monster Hunter on PC—relearning how to use my newly installed ten fingers… In summary, from hardware to software, it failed to meet my expectations for the Duet. In the end, I self-sabotaged it, but I didn’t stop being reckless. Even though there were so many problems with the Duet, I gradually discovered a surprising fact during this trial period: most of my work can actually be done using just a browser. My Work RequirementsI am a bioinformatics professional, and my usual work involves four main tools: Writing scripts: VScode &#x2F; Jupyter Copying code: Chrome browser Logging into terminals: Whatever distribution comes with it Writing files: WPS &#x2F; Office全家桶 During the period when I was fighting against (not falling in love with) the Duet, I gradually realized that in today’s rapidly evolving web technology, all of these can be achieved through a browser. Jupyter and VScode can run in the browser, and VScode has an integrated terminal emulator. WPS&#x2F;Office has also started to layout its web applications. Although not all features are available, it is much more complete than when ChromeOS first came out 10 years ago. It’s like when I first used Logo on a green screen back then, which was like a mint imitation of the initial deepin version. Now, many distributions have Chinese support, and software has increased significantly, making configuration easier. Even Manjaro, which can now run Steam comfortably, feels more relaxed… So, I decided to continue being reckless. After sending away the Duet, I switched to a Pixelbook 2017—then it went wrong (the touchpad was faulty, and I didn’t want to spend more money on repairs, so I chose one with backlighting to treat my强迫症). Pixelbook 2017 Application CompatibilityUnlike the Duet, the Pixelbook 2017 and other high-end Chromebooks use Intel chips (although I don’t understand why, there’s nothing I can do), so I started using this device with its sleek appearance (ignoring the B-side carrier runway and yellow palm rest). However, its battery life didn’t meet expectations. As expected, all four main tools could be relatively easily used. However, the problem ultimately fell on communication software… just like the biggest issue faced by Linux distributions when they first came out. Although X86 architecture Pixelbooks can install international versions of WeChat and DingTalk through Play, these apps have poor compatibility. They often crash or fail to use many necessary web-based features, requiring Linux applications to save the day. However, this brings new problems with Linux. Small issues include that Linux applications cannot call device cameras or host desktops like Android apps; they are more like running separate apps in a virtual machine. The bigger problem is that if I rely so much on Linux, why don’t I just format ChromeOS and install Manjaro on this device? So Why DuetAfter trying out an experience that was good but not great, I began to question myself: why do I need to hold onto such a device? Is it for showing off? (Then what?) Is it because of fewer interfaces, higher heat generation, or lower battery life than expected? I thought about this for a long time, holding the 1.1kg laptop and looking at its 12.3-inch 2.5K screen… Well, the high-resolution screen is still good!","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"chromeos","slug":"chromeos","permalink":"https://silenwang.github.io/en/tags/chromeos/"},{"name":"chromebook","slug":"chromebook","permalink":"https://silenwang.github.io/en/tags/chromebook/"}]},{"title":"Build a ChatGLM2 Service Based on the chatglm.cpp Project Using Singularity","slug":"使用singularity构建一个基于chatglm-cpp项目的ChatGLM2服务","date":"2023-08-16T07:08:46.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2023/08/16/使用singularity构建一个基于chatglm-cpp项目的ChatGLM2服务/","permalink":"https://silenwang.github.io/en/2023/08/16/%E4%BD%BF%E7%94%A8singularity%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%9F%BA%E4%BA%8Echatglm-cpp%E9%A1%B9%E7%9B%AE%E7%9A%84ChatGLM2%E6%9C%8D%E5%8A%A1/","excerpt":"Actually, I’ve known about singularity for quite some time. As a container specifically designed for HPC (High Performance Computing), it’s always been something I wanted to try out. However, just like other NGS technologies outside of Illumina, singularity hasn’t gained much traction and it seems even harder to compete with Kubernetes, which is widely adopted by most cloud vendors… Of course, this doesn’t really matter to me right now… We’re still at least three or five years away from going cloud-based, so using it locally makes perfect sense.","text":"Actually, I’ve known about singularity for quite some time. As a container specifically designed for HPC (High Performance Computing), it’s always been something I wanted to try out. However, just like other NGS technologies outside of Illumina, singularity hasn’t gained much traction and it seems even harder to compete with Kubernetes, which is widely adopted by most cloud vendors… Of course, this doesn’t really matter to me right now… We’re still at least three or five years away from going cloud-based, so using it locally makes perfect sense. So let’s start with the most basic step of building an image. The Singularity image build file (Definition File) is quite different from Dockerfile; it’s not a script that runs sequentially but rather a configuration file similar to INI files, partitioned into sections for configuration. The necessary keywords for building an image include: Bootstrap: Used to define where the base image comes from. There are several types, such as dockerhub or singularity’s own library. From: The name of the base image to build from. %post: A series of commands used to download files, install software, compile software, etc., during the image build process. Given that chatglm.cpp has excellent dependency management, as long as you have a C++ compiler ready, you can install it using pip. Therefore, writing the definition file is quite simple: 1234567Bootstrap: libraryFrom: debian%post apt-get update -y &amp;&amp; apt-get upgrade -y apt-get install -y cmake gcc g++ pip pip install &#x27;chatglm-cpp[api]&#x27; Besides these necessary parts, there are other useful sections that might be needed in the future: %files: Used to copy files into the container during the build process. %environment: Specifies environment variables for the container. %runscript: Specifies a script to run when using the run subcommand to execute the container. %startscript: Specifies a script to run when using the instance (seems to be used for starting services on boot) subcommand to execute the container. %test: Specifies commands to run in the container after it’s built to check if the build was successful. %labels: Fills in metadata information for the image. %help: Provides usage instructions for the image. With the def file, you can start building by running the command (using fakeroot allows non-root user builds and requires certain settings; see official documentation). The built image won’t be stored locally like Docker images but will generate an sif file. 1singularity build --fakeroot chatglm.cpp.singularity.sif build.def To enable the built image, you can use the following command: 12345singularity exec \\ --bind model:/model/ \\ --env MODEL=/model/chatglm2-ggml.q4_0.bin \\ chatglm.cpp.singularity.sif \\ uvicorn chatglm_cpp.openai_api:app --host 0.0.0.0 --port 8877","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"sinularity","slug":"sinularity","permalink":"https://silenwang.github.io/en/tags/sinularity/"},{"name":"container","slug":"container","permalink":"https://silenwang.github.io/en/tags/container/"}]},{"title":"Deploying RStudio with Singularity and a Conda Environment","slug":"使用singularity部署Rstudio并指定conda环境下的R","date":"2023-07-26T16:13:50.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2023/07/26/使用singularity部署Rstudio并指定conda环境下的R/","permalink":"https://silenwang.github.io/en/2023/07/26/%E4%BD%BF%E7%94%A8singularity%E9%83%A8%E7%BD%B2Rstudio%E5%B9%B6%E6%8C%87%E5%AE%9Aconda%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84R/","excerpt":"Today, I needed to deploy an RStudio server for someone else. Here are the steps and key points of the deployment.","text":"Today, I needed to deploy an RStudio server for someone else. Here are the steps and key points of the deployment. IntroductionRunning RStudio requires root permissions and cannot be installed directly using conda or mamba like Jupyter. Therefore, this deployment used Singularity, which is a container technology I had never used before. While Sigularity differs from Docker or Podman in design, it is primarily designed for HPC environments with different permission management. However, these differences do not significantly affect its use in this case. 1. Container PreparationAlthough Sigularity and Docker are different container technologies, Sigularity provides comprehensive compatibility support for Docker images. Built Docker containers can be easily converted to the sif format used by Singularity. For this deployment, I referenced the instructions from the rstudio-server-conda project and pulled the Docker image using singularity pull docker://rocker/rstudio:4.2. 2. Necessary File PreparationI used the following bash script to generate several directories and configuration files. 12345mkdir -p run var-lib-rstudio-serverprintf &#x27;provider=sqlite\\ndirectory=/var/lib/rstudio-server\\n&#x27; &gt; database.confecho &quot;auth-minimum-user-id=100&quot; &gt; rserver.confecho &quot;session-default-working-dir=/home/silen/Rstudio/Workspace&quot; &gt; rsession.confecho &quot;session-default-new-project-dir=/home/silen/Rstudio/Workspace&quot; &gt;&gt; rsession.conf This script generates run and var-lib-rstudio-server directories, as well as database.conf, rserver.conf, and rsession.conf configuration files. The auth-minimum-user-id=100 setting in rserver.conf is crucial because the RStudio service runs under the rstudio user with UID 999 in the container. If this user does not exist or if the permissions are not switched to this user, the container will fail to run. I referenced a post on GitHub (#18) and used the --server-user parameter to specify that the service should be run using the current user. However, by coincidence, my current user’s UID is less than 513, while the default auth-minimum-user-id value is 1000, which prevents users with UIDs less than 1000 from running the service (apparently for security reasons). Therefore, I needed to change the parameter value in rserver.conf to allow the service to start normally. 3. Starting the Container with Specified ParametersI used the following command to start the service. The main commands bind the previously created directories and configuration files to the container so that the service can read them. Additionally, I specified the IP address and port for easy access. Since this is a shared use scenario, I did not set up authentication. I also referenced the rstudio-server-conda project to specify the conda environment used by RStudio, so there’s no need to reinstall R packages within the container. 1234567891011singularity exec \\ --bind run:/run \\ --bind var-lib-rstudio-server:/var/lib/rstudio-server \\ --bind database.conf:/etc/rstudio/database.conf \\ --bind rserver.conf:/etc/rstudio/rserver.conf \\ --bind rsession.conf:/etc/rstudio/rsession.conf \\ --bind /home/silen/R-Plot:/etc/R-Plot \\ --env CONDA_PREFIX=/etc/R-Plot \\ --env RSTUDIO_WHICH_R=/etc/R-Plot/bin/R \\ rstudio_4.2.sif \\ /usr/lib/rstudio-server/bin/rserver --www-address=0.0.0.0 --www-port=7788 --server-user=$(whoami) After starting, the interface looks like this: 4. NotesSigularity feels more similar to snap and flathub software packaging projects, where the host and container are not completely isolated from each other. After starting RStudio, it can directly access the user’s Home directory on the host machine. I’m not sure if this is a special behavior of this container or if Sigularity behaves similarly in general. We will need to do more dependency analysis and containerization for our internal analysis processes… It seems we still need to learn more about the principles and usage of Singularity.&#96;&#96;&#96;","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"conda","slug":"conda","permalink":"https://silenwang.github.io/en/tags/conda/"},{"name":"rstudio","slug":"rstudio","permalink":"https://silenwang.github.io/en/tags/rstudio/"},{"name":"singularity","slug":"singularity","permalink":"https://silenwang.github.io/en/tags/singularity/"}]},{"title":"Using Codespace for Free to Write Hexo Blogs","slug":"白嫖codespace用来写hexo博客","date":"2023-07-23T17:10:23.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2023/07/23/白嫖codespace用来写hexo博客/","permalink":"https://silenwang.github.io/en/2023/07/23/%E7%99%BD%E5%AB%96codespace%E7%94%A8%E6%9D%A5%E5%86%99hexo%E5%8D%9A%E5%AE%A2/","excerpt":"Recently, out of… well, let’s say a bit of dissatisfaction, I sold my previously purchased Lenovo Chrome Duet2, but then I bought a Pixelbook 2017… Now that I have this device, I still want to tinker with it for a while, trying to find a way to use this Pixelbook 2017 as a replacement for my work computer for general office tasks. In the process of practising, the main issue wasn’t resolved, but I did come up with a few practical by-products… writing Hexo blogs using Codespace is one of them.","text":"Recently, out of… well, let’s say a bit of dissatisfaction, I sold my previously purchased Lenovo Chrome Duet2, but then I bought a Pixelbook 2017… Now that I have this device, I still want to tinker with it for a while, trying to find a way to use this Pixelbook 2017 as a replacement for my work computer for general office tasks. In the process of practising, the main issue wasn’t resolved, but I did come up with a few practical by-products… writing Hexo blogs using Codespace is one of them. Now I Can Write Blogs AnywhereGitHub Codespace is a cloud-based development service launched by GitHub in the era of cloud computing. This service can allocate a container (which can be customized) for users, then directly pull the project code from GitHub into the container, and assign users a VSCode web client that can access the container’s content. This allows users to directly write and debug code on servers provided by GitHub, essentially developing entirely in the cloud. I guess the implementation of the project is similar to coder. Such a great tool, of course, is not free. Currently, GitHub offers 15GB of free storage and 120 hours of core time (see official details). This storage and time may not seem like much, but based on current usage, core time seems not to be counted unless it’s being used, so for very lightweight projects, it’s actually quite sufficient. This is just perfect for writing blogs! The code for my Hexo blog is not even as much as the themes I’ve used, and the compile step required is just to generate static web pages, which takes only a few seconds. Operating in the cloud can also solve the previous issue of being able to write blogs only on my office computer (I only set up the environment on my office computer and didn’t want to do it again), it’s ready to use upon opening. And I’ve tried, the static web page preview generated by hexo s can even be accessed normally! This way, it’s like those blog platforms where you can write with just a browser. (Although stop blogging did not due to device issue) Issues with UsageCurrently, the only two issues are network and image management within the blog. 1. Image IssuesI used to write blogs using picgo and upgit for image uploading and automatic link acquisition. After testing, the most convenient VSCode-picgo plugin cannot be used, even after installing xclip, it can’t recognize the image content in the clipboard, probably because it’s in a cloud container, while the clipboard with the image is local to me… upgit can be used, by downloading the software into the Linux container of ChromeOS, and configuring it properly. 2. NetworkThis issue needs no elaboration… The same problem arises when trying to access services outside the mainland. Currently, the editor part is actually fine, because it’s not a real-time response, so typing is quite smooth… It’s just that I don’t know if I’ll find out I can’t save while writing. However, the command line part does feel noticeably laggy, but there are not many command lines, just hexo g and hexo d, so I can bear with it, and if I can’t publish now, I can do it another day.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"github","slug":"github","permalink":"https://silenwang.github.io/en/tags/github/"},{"name":"codespace","slug":"codespace","permalink":"https://silenwang.github.io/en/tags/codespace/"}]},{"title":"In the Guidance of chatGPT, Write a Tool to Parse Literature Using chatGPT's API","slug":"在chatGPT的指导下写一个调用chatGPT的API进行文献解析的工具","date":"2023-03-27T01:26:48.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2023/03/27/在chatGPT的指导下写一个调用chatGPT的API进行文献解析的工具/","permalink":"https://silenwang.github.io/en/2023/03/27/%E5%9C%A8chatGPT%E7%9A%84%E6%8C%87%E5%AF%BC%E4%B8%8B%E5%86%99%E4%B8%80%E4%B8%AA%E8%B0%83%E7%94%A8chatGPT%E7%9A%84API%E8%BF%9B%E8%A1%8C%E6%96%87%E7%8C%AE%E8%A7%A3%E6%9E%90%E7%9A%84%E5%B7%A5%E5%85%B7/","excerpt":"This title is a bit too nested… but the fact is, I suddenly felt capable! I finally have a chance to become a full-stack developer! To showcase my capabilities enhanced by chatGPT, I decided to develop a tool that uses chatGPT to read papers (I’ve also considered using it for meta-analysis).","text":"This title is a bit too nested… but the fact is, I suddenly felt capable! I finally have a chance to become a full-stack developer! To showcase my capabilities enhanced by chatGPT, I decided to develop a tool that uses chatGPT to read papers (I’ve also considered using it for meta-analysis). Start DiggingAbout two weeks ago, I had the idea of doing this. Previously, I registered an account and tried using it to assist in writing a static service with token verification. Later, I saw my colleague share chatPDF and mentioned that he uses chatGPT to quickly read papers and write reviews. At that time, I recalled the pain of reading papers here over three years and thought about developing a tool to accelerate my reading using chatGPT to assist in my work, which could also serve as a demo for popularity and potentially gain several hundred stars, bringing me closer to becoming an independent full-stack developer who has always wanted. Plus, before, I couldn’t do it because learning everything from scratch was too time-consuming and lacked the ability. With chatGPT’s assistance, it should be much easier. Various PitfallsThen, let’s get down to business! (Three-minute热度 is most擅长 this), I thought that with such an external aid, developing a simple demo wouldn’t take more than three days? So, over the past two weeks, I’ve been going through various pitfalls… Paying Money Is So Difficult…I didn’t expect the first pit to be paying money… After registering for a new OpenAI account, I received 30 dollars of free API credits for trial use. However, it seems that too many users have registered, and they are reducing or even stopping the free credits… When I registered, my account had no credits at all, so I couldn’t use the core API functionalities. I had to pay out of pocket… But it’s not as simple as that; OpenAI does not provide services for mainland China developers… Therefore, my Visa card couldn’t be registered… I had to apply for a virtual credit card… Then this virtual credit card… I’m not sure if it’s a legitimate virtual currency… The largest Depay requires using a certain type of virtual currency to recharge??? Virtual currencies make me feel scared; I dare not mess with them. In the end, I found a large amount that I could pay directly (I might buy塞尔达 later), and finally solved the problem (it seems it was still resolved by paying money). After topping up, I started developing! emmmmmm… Then after half a day… Limitations of chatGPTchatGPT can execute various tasks, but this doesn’t change the fact that it is a text generation AI. Its fundamental function is to generate fluent and error-free natural language text. Initially, when using it, I didn’t encounter many problems (perhaps because the questions were simple), but as usage frequency increased, the frequency and severity of encountering issues also rose significantly. In general, I feel that due to asking overly broad questions or asking questions that may not have answers, I receive various irrelevant answers from chatGPT. This leads to two issues: During actual development, it wasn’t as fast as expected because I adopted a very new development framework (Pynecone). Its introduction is beautiful; once developed, frontend&#x2F;backend&#x2F;API are all included, and multi-platform quick deployment is possible. However, due to its immaturity, chatGPT couldn’t give me any suggestions. I had to learn the documentation and official examples to complete necessary functional implementations. Moreover, it took a lot of time on some very basic functionalities… Making chatGPT execute specific tasks smoothly isn’t easy… And I’m using gpt-3.5-turbo (which didn’t have version 4 at the start; now I can’t queue up), so it’s not as easy to specify the return answer format with gpt4 (I want to use JSON format for easier parsing). Therefore, “programming towards prompts” is no joke… To use it well, debugging prompts is still very necessary. In summary, chatGPT is strong, but… even gpt4, I believe, isn’t as powerful as the Microsoft promotional video suggests… To use it smoothly, you need to have some usage skills and become shaped like chatGPT (misunderstanding). Switching Development FrameworksContinuing from the previous text… Pynecone really isn’t mature enough… After a week and a half, I couldn’t take it anymore and switched to Gradio… How to say it? The tool and framework’s Matthew effect shouldn’t be violated casually… Especially since my intention was to gain popularity, finding the fastest implementation method is more important than overthinking. SinGAN you sin!Although just a joke, I really didn’t expect… I got stuck for an entire afternoon and one night while deploying the demo on HuggingFace. HuggingFace is a great platform (a space provides 8 cores and 16GB, what a charitable oil tycoon), but not allowing a new project to be completely empty really made it difficult for me as a beginner… I experienced manual conflict resolution, forced code pushes, and ultimately compromised by separating the original project’s code from HuggingFace. I really couldn’t get any effective help on this issue from chatGPT. Not Enough TimeWorse still, there was rain on my parade… (The road to becoming a national treasure is irreversible) Help Provided by chatGPTAlthough I’ve complained that chatGPT isn’t as good as it seems, the benefits it brings are real. In this project, the help provided by chatGPT includes but is not limited to: Helping me understand similar open-source code quickly, allowing me to quickly grasp other implementation methods. Answering various usage questions for Gradio (a mature tool can provide many practical suggestions). Assisting in generating English README (although no one reads it). Increasing my confidence that “I can do it!” Feeling the Pain of Independent DevelopmentThese two weeks also deeply made me feel the pain of independent developers or personal自媒体. Similarly, as a literature reading assistant, the chatPaper project and my content both started disclosing code on GitHub two weeks ago (my case was uploading code incrementally…), but now it has over 4000 stars… I… This feeling is probably similar to the anxiety of starting a自媒体, only to find that no one watches your videos. Project AddressHowever, regardless of this, the purpose of my project is to develop something useful that I personally feel good about. If popularity can’t be gained… then it’s fine… My project is called ResearchGPT, and the demo is hosted on Huggingface at Demo. Those interested can try it out~ Of course, this blog post also embedded the demo.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://silenwang.github.io/en/tags/AI/"},{"name":"chatGPT","slug":"chatGPT","permalink":"https://silenwang.github.io/en/tags/chatGPT/"}]},{"title":"chatGPT is indeed useful","slug":"chatGPT确实好用","date":"2023-02-23T18:24:06.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2023/02/23/chatGPT确实好用/","permalink":"https://silenwang.github.io/en/2023/02/23/chatGPT%E7%A1%AE%E5%AE%9E%E5%A5%BD%E7%94%A8/","excerpt":"chatGPT has been popular for so long, but I only created an account yesterday when I wanted to use it to generate some content. As many people online have said, while it may be unreliable on professional questions, for writing code - since I ultimately need to run and test it myself - practice yields true knowledge. Having a roughly correct direction is already helpful!","text":"chatGPT has been popular for so long, but I only created an account yesterday when I wanted to use it to generate some content. As many people online have said, while it may be unreliable on professional questions, for writing code - since I ultimately need to run and test it myself - practice yields true knowledge. Having a roughly correct direction is already helpful! The background of this problem is: I used mkdocs to generate a technical documentation for company use (similar to my manual), which is a static webpage that can be viewed directly by adding an HTTP server I want to embed this documentation into an existing system using iframe Since this documentation is important, it has certain confidentiality requirements, so it cannot be accessed directly and needs some verification After discussing with IT colleagues, the final solution was that they would give me a token on the first request, I would use this token to verify against their API, and return the content only if the verification passes. I only know basic flask and fastAPI, and flask is simpler to use, so in summary, I need to write a program that: Uses flask to build an HTTP server with the same functionality as Python’s simple http.server, allowing users to access the static website created by mkdocs This server needs verification functionality - receiving a token from the request to verify whether the user is legitimate After the first verification, requests for internal jumps within the static website are handled by this service itself, so to maintain continuous verification, I need to save the token myself and complete the verification Although I’ve used flask and dash before, I actually don’t have much concept of web-related technologies. Previous experience testing caddy reverse proxy made me think this program was certainly implementable, but I wasn’t sure exactly how. The colleague’s mention of saving tokens myself made sense conceptually but I didn’t know how to implement it. This is where chatGPT came into play. I described my requirements roughly and asked how to write the code I wanted. It didn’t give me a perfect solution matching all my requirements, but provided a code example that let me test and figure out how the program should be written. For parts I didn’t understand, I could ask follow-up questions or request specific explanations. While its answers weren’t completely correct (and sometimes due to my unclear questions), through discussion I gradually found the implementation I wanted. In the end, I completed this simple program in one morning. While the program is simple, in the past I would have needed to: Find several Chinese web pages to see if there were ready examples Guess the English description methods Browse about 10 English posts on stackoverflow Look for answers to specific parts of my problem Try to assemble a workable solution… This process wouldn’t be painful - it’s a learning process after all - but certainly not efficient… Now, as long as I can basically describe what I want, chatGPT can directly give me a code example to debug and learn from, saving most of the time searching Chinese and English materials and jumping straight to debugging with examples. The efficiency improvement is transformative… Suddenly I feel that with its help, learning some frontend tricks… writing my own websites and apps is no longer a dream… update20220227Today chatGPT has too many requests and is temporarily unavailable (not sure about paid users). Seems like early adoption brings benefits… play with it while you can before free access disappears… Appendix: Complete conversation process about this issueYou:How to use flask to build an HTTP server that uses tokens for verification?ChatGPT:To use Flask to build an HTTP server that uses tokens for verification, you can use Flask’s Flask-JWT extension. This extension provides JSON Web Token (JWT) authentication mechanism for Flask applications.First install required extensions:12pip install Flaskpip install Flask-JWTThen write Flask application code as follows:1234567891011121314151617181920212223242526272829303132333435from flask import Flask, jsonify, requestfrom flask_jwt import JWT, jwt_required, current_identityfrom werkzeug.security import safe_str_cmpclass User(object): def __init__(self, id, username, password): self.id = id self.username = username self.password = passwordusers = [ User(1, &#x27;user1&#x27;, &#x27;password1&#x27;), User(2, &#x27;user2&#x27;, &#x27;password2&#x27;)]def authenticate(username, password): for user in users: if user.username == username and safe_str_cmp(user.password.encode(&#x27;utf-8&#x27;), password.encode(&#x27;utf-8&#x27;)): return userdef identity(payload): user_id = payload[&#x27;identity&#x27;] return next((user for user in users if user.id == user_id), None)app = Flask(__name__)app.config[&#x27;SECRET_KEY&#x27;] = &#x27;super-secret&#x27;jwt = JWT(app, authenticate, identity)@app.route(&#x27;/protected&#x27;)@jwt_required()def protected(): return jsonify(&#123;&#x27;user_id&#x27;: current_identity.id, &#x27;username&#x27;: current_identity.username&#125;)if __name__ == &#x27;__main__&#x27;: app.run()You:This flask server will receive a token when processing requests. I need to use this token to verify against another server, and only return static content after verification passes. How to implement this?ChatGPT:To use the received token to verify against another server in Flask, you can use Python’s requests library to conveniently send HTTP requests to other servers and get responses.Example:12345678910111213141516import requestsfrom flask import Flask, requestapp = Flask(__name__)def verify_token(token): response = requests.get(&#x27;https://example.com/verify-token&#x27;, headers=&#123;&#x27;Authorization&#x27;: &#x27;Bearer &#x27; + token&#125;) return response.status_code == 200@app.route(&#x27;/&#x27;)def index(): token = request.headers.get(&#x27;Authorization&#x27;).split(&#x27; &#x27;)[1] return &#x27;Hello, world!&#x27; if verify_token(token) else &#x27;Invalid token&#x27;if __name__ == &#x27;__main__&#x27;: app.run()You:If I want to return not a simple website but pre-generated static pages like those from mkdocs or gitbook, how to implement?ChatGPT:To return pre-generated static pages, use Flask’s send_from_directory method to serve files from a specific directory.Example assuming static files are in static/docs:12345678910from flask import Flask, send_from_directoryapp = Flask(__name__)@app.route(&#x27;/docs/&lt;path:filename&gt;&#x27;)def serve_docs(filename): return send_from_directory(&#x27;static/docs&#x27;, filename)if __name__ == &#x27;__main__&#x27;: app.run()You:How does flask get parameters from GET requests?ChatGPT:In Flask, use request.args to access GET parameters. It’s a dict containing all parameters and their values.Example for URL http://localhost:5000/search?q=python&amp;page=1:123456789101112from flask import Flask, requestapp = Flask(__name__)@app.route(&#x27;/search&#x27;)def search(): query = request.args.get(&#x27;q&#x27;) page = request.args.get(&#x27;page&#x27;) return f&#x27;Searching for &quot;&#123;query&#125;&quot; on page &#123;page&#125;&#x27;if __name__ == &#x27;__main__&#x27;: app.run()You:The above code can only access first-level files under static&#x2F;docs&#x2F;. How to access files in subdirectories?ChatGPT:To access files in subdirectories, use os.path.join to combine paths:1234567891011121314import osfrom flask import Flask, send_from_directoryapp = Flask(__name__)@app.route(&#x27;/&lt;path:filename&gt;&#x27;)def serve_docs(filename): if filename == &#x27;&#x27;: filename = &#x27;index.html&#x27; path = os.path.join(&#x27;static&#x27;, filename) return send_from_directory(path, filename)if __name__ == &#x27;__main__&#x27;: app.run()You:In flask, how to redirect ‘&#x2F;‘ to ‘&#x2F;index.html’?ChatGPT:Use Flask’s redirect function:12345678910from flask import Flask, redirectapp = Flask(__name__)@app.route(&#x27;/&#x27;)def redirect_to_index(): return redirect(&#x27;/index.html&#x27;)if __name__ == &#x27;__main__&#x27;: app.run()You:How to generate a recommended secret key for flask?ChatGPT:Use Python’s secrets module to generate secure random keys:1234import secretsapp = Flask(__name__)app.secret_key = secrets.token_hex(16) # 16-byte random stringYou:How to implement token expiration in flask?ChatGPT:For token expiration, consider using JWT (JSON Web Tokens) with the cryptography library:1234567891011121314151617181920import datetimeimport jwtapp = Flask(__name__)app.config[&#x27;JWT_SECRET_KEY&#x27;] = &#x27;mysecretkey&#x27;app.config[&#x27;JWT_EXPIRATION_DELTA&#x27;] = datetime.timedelta(seconds=3600)@app.route(&#x27;/login&#x27;)def login(): payload = &#123; &#x27;sub&#x27;: &#x27;1234567890&#x27;, &#x27;exp&#x27;: datetime.datetime.utcnow() + app.config[&#x27;JWT_EXPIRATION_DELTA&#x27;] &#125; token = jwt.encode(payload, app.config[&#x27;JWT_SECRET_KEY&#x27;], algorithm=&#x27;HS256&#x27;) return &#123;&#x27;access_token&#x27;: token.decode(&#x27;UTF-8&#x27;), &#x27;token_type&#x27;: &#x27;Bearer&#x27;&#125;@app.route(&#x27;/protected&#x27;)@jwt_required()def protected(): return &#123;&#x27;data&#x27;: &#x27;Protected resource&#x27;&#125;","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"python","slug":"python","permalink":"https://silenwang.github.io/en/tags/python/"},{"name":"chatGPT","slug":"chatGPT","permalink":"https://silenwang.github.io/en/tags/chatGPT/"},{"name":"flask","slug":"flask","permalink":"https://silenwang.github.io/en/tags/flask/"}]},{"title":"The Update of DingTalk for Linux Is Mixed","slug":"钉钉Linux版本次的更新喜忧参半","date":"2023-02-03T17:27:49.000Z","updated":"2026-02-10T14:03:13.692Z","comments":true,"path":"2023/02/03/钉钉Linux版本次的更新喜忧参半/","permalink":"https://silenwang.github.io/en/2023/02/03/%E9%92%89%E9%92%89Linux%E7%89%88%E6%9C%AC%E6%AC%A1%E7%9A%84%E6%9B%B4%E6%96%B0%E5%96%9C%E5%BF%A7%E5%8F%82%E5%8D%8A/","excerpt":"DingTalk for Linux, although not updated frequently, has been continuously solving problems. However, if it could solve problems without introducing new major issues, it would be better …","text":"DingTalk for Linux, although not updated frequently, has been continuously solving problems. However, if it could solve problems without introducing new major issues, it would be better … Currently, I have updated to the latest version 1.6.0.230113-2 from AUR. After the update, Chinese input works normally without any environment variable settings (both the program and mini-programs are fine), and there is no jaggedness in the interface when not at 50% integer scaling… However, a more serious issue has been introduced… The meeting interface cannot be opened at all, whether initiating a new meeting or entering an existing one. Clicking does nothing, which severely impacts normal use. I hope it can be resolved soon…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://silenwang.github.io/en/tags/Linux/"},{"name":"DingTalk","slug":"DingTalk","permalink":"https://silenwang.github.io/en/tags/DingTalk/"}]},{"title":"New Year's Flag Fell Down","slug":"新年的Flag倒了","date":"2023-02-02T16:25:13.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2023/02/02/新年的Flag倒了/","permalink":"https://silenwang.github.io/en/2023/02/02/%E6%96%B0%E5%B9%B4%E7%9A%84Flag%E5%80%92%E4%BA%86/","excerpt":"Last month, I was complaining that things were getting worse year by year, and hoped this year would be better, but my Flag fell down so quickly, Fxxk… It’s really annoying and never stops…","text":"Last month, I was complaining that things were getting worse year by year, and hoped this year would be better, but my Flag fell down so quickly, Fxxk… It’s really annoying and never stops…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"Daily","slug":"Daily","permalink":"https://silenwang.github.io/en/tags/Daily/"}]},{"title":"Sprucing Up My GitHub Profile","slug":"给Github首页做点面子工作","date":"2023-01-27T22:15:30.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2023/01/27/给Github首页做点面子工作/","permalink":"https://silenwang.github.io/en/2023/01/27/%E7%BB%99Github%E9%A6%96%E9%A1%B5%E5%81%9A%E7%82%B9%E9%9D%A2%E5%AD%90%E5%B7%A5%E4%BD%9C/","excerpt":"After upgrading my blog, I decided to do some cosmetic work on my GitHub profile…","text":"After upgrading my blog, I decided to do some cosmetic work on my GitHub profile… Currently, you can create a special repository that matches your username, like SilenWang/SilenWang. The README.md in this repository will be displayed on the user’s GitHub homepage, essentially giving them a customizable dashboard. I currently have my GitHub statistics and commonly used tool icons displayed there. GitHub StatisticsThis is quite simple; it uses an API: 1&lt;img width=&quot;500px&quot; alt=&quot;GitHub Stats&quot; src=&quot;https://github-readme-stats.vercel.app/api?username=SilenWang&amp;count_private=true&amp;show_icons=true&quot;/&gt; Tool IconsI used Shields.io for this. It allows you to request various labeled icons from the website’s API via a GET request. For example, the Jupyter label is: 1&lt;img alt=&quot;Jupyter&quot; src=&quot;https://img.shields.io/badge/-Jupyter-f37524?style=flat-square&amp;logo=Jupyter&amp;logoColor=white&quot; /&gt; I dug out all the icons I could find for tools I use… The available icons are quite rich, but not everything is available. You can see the existing ones in this file. Rendering Effect PostscriptAfter decorating it, I tried to use the same labels on my “About Me” page. However, Hexo seems to force line breaks when rendering HTML tags, causing all labels to be in a single column, which is ugly (unattractive) … I spent half an hour trying to find a solution but couldn’t figure it out. So, I just used tables to box each label for now. I’ll look into how to fix this later.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"GitHub","slug":"GitHub","permalink":"https://silenwang.github.io/en/tags/GitHub/"}]},{"title":"Blog Upgrade in 2023","slug":"个人博客2023年升级","date":"2023-01-26T03:28:32.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2023/01/26/个人博客2023年升级/","permalink":"https://silenwang.github.io/en/2023/01/26/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A22023%E5%B9%B4%E5%8D%87%E7%BA%A7/","excerpt":"","text":"After… I can’t even remember how many years, my blog has been upgraded again. The theme is still Volantis, which is simple to use but comes with many powerful features! Update Content Upgraded Volantis to the latest stable version 5.7.7. Updated the “About” page according to the theme upgrade method, and now it can properly display links for two friends (frenemies) . Enabled the comment plugin utterances, which allows users to leave messages after logging into their Github accounts! Installed a plugin to enable internal article links. Installed a plugin to change Bilibili video link format (from embedded player to card). Installed a plugin to change Github Repo display format (card style). Updated the “About Me” page, using various theme tags to slightly beautify it. Added a pronunciation plugin similar to萌娘百科, just to have some fun and make myself happy~ New Plugins Used1. Utterances CommentsThe Volantis theme provides 14 different comment systems. After configuring according to the documentation in _config.volantis.yml, it can be used. Originally, I planned to use the localized version of utterances called beaudar, but encountered problems that I couldn’t understand. So, I gave up. When configuring for the first time, Github will prompt you to install the utterances Github App to use it. I don’t know if I need to install it once as the owner or if each commenter needs to install it… 2. Bilibili Video CardI have previously inserted some of my own videos into the blog, but the playback window is very restricted. Clicking a few times will still redirect to Bilibili. It’s better to use links instead. Using hexo-bilibili-card makes the links look nicer and is easier to insert: Before: 1&lt;iframe src=&quot;//player.bilibili.com/player.html?aid=91147423&amp;cid=155618081&amp;page=1&quot; width=&quot;700&quot; height=&quot;500&quot; scrolling=&quot;no&quot; border=&quot;0&quot; frameborder=&quot;no&quot; framespacing=&quot;0&quot; allowfullscreen=&quot;true&quot;&gt; &lt;/iframe&gt; Now: 1&#123;% bilicard av91147423 %&#125; 3. Github Repo CardOn the “About Me” page… I have very simply displayed a few of my not-so-impressive repos…. So, using the theme’s built-in tags to achieve it: 1&#123;% ghcard &#x27;SilenWang/silenwang.github.io&#x27; %&#125; Additional Plugin Installation On the “About Me” page… I have very simply displayed a few of my not-so-impressive repos…. So, I found a plugin to make them look slightly better… The plugin name is hexo-github-repo-tag, and the usage method is:1&#123;% githubrepo &#x27;SilenWang/silenwang.github.io&#x27; %&#125;When deploying, there was a small problem… This plugin uses the API to grab necessary information from the project page to generate corresponding cards. Therefore, when running hexo g, it needs to access Github… If the network is not good at that time, you can only try again multiple times…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://silenwang.github.io/en/tags/hexo/"}]},{"title":"Games I've Played in the Last Three Years","slug":"我这3年玩过的游戏","date":"2023-01-24T02:18:48.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2023/01/24/我这3年玩过的游戏/","permalink":"https://silenwang.github.io/en/2023/01/24/%E6%88%91%E8%BF%993%E5%B9%B4%E7%8E%A9%E8%BF%87%E7%9A%84%E6%B8%B8%E6%88%8F/","excerpt":"In 2019, to play Monster Hunter XX, I bought a 3DS. From then on, I started buying various devices to play specific games… I formed a PC group for Dragon’s Dogma, bought a PSV for Persona 4 Golden, and got an NSL for Re:Zero. The most expensive one was the Quest2 for Rhythm Heaven. At the end of 19, I planned to record what games I played each year… But fate is not on my side… The pandemic came, and my work suddenly became much more demanding… Playing games had to rely on late-night spending, so I didn’t have the energy to do much reflection, only expressing a little bit of feeling about some games.","text":"In 2019, to play Monster Hunter XX, I bought a 3DS. From then on, I started buying various devices to play specific games… I formed a PC group for Dragon’s Dogma, bought a PSV for Persona 4 Golden, and got an NSL for Re:Zero. The most expensive one was the Quest2 for Rhythm Heaven. At the end of 19, I planned to record what games I played each year… But fate is not on my side… The pandemic came, and my work suddenly became much more demanding… Playing games had to rely on late-night spending, so I didn’t have the energy to do much reflection, only expressing a little bit of feeling about some games. So, three years passed… Since getting an NSL, I basically stopped playing 3DS, PSV, and PC. For some reason, from being a PC+mobile player, I became a console player… Over the past three years, the games I’ve played are roughly: Game Platform Time Completion Monster Hunter World: Iceborne PC 2020 Only released Persona 4 Golden PSV 2020 Yes Odin Field PSV 2020 Yes Luminous Village PSV 2020 Yes Dragon’s Crown PSV 2020 No Ys IV: Blazing Tower of Seres PSV 2020 Yes Ys VIII: Dandelion Day PSV 2020 No Courage of Heart 3DS 2020 No Rhythm Heaven 3DS 2020 No Metroid: Return of the Classic 3DS 2020 Yes Limit Break 2: The End of Humanity 3DS 2020 Yes Cooking Mama 2 PC 2021 No Invisible Guardian PC 2021 Yes Double Dragon PC 2021 Yes Catsuit PC 2021 Yes Limit Break 3: Zero Hour 3DS 2021 Yes Re:Zero - Starting Life in Another World 2 3DS 2021 Yes The Last of Us Part II 3DS 2021 Yes Monster Hunter Rise NS 2021 Yes Xenoblade Chronicles NS 2021 Yes Infinite Space NS 2021 Yes Bloodborne: Night of the Sacrifice NS 2021 Yes AI: Dreamscape NS 2021 Yes The Legend of Zelda: Breath of the Wild NS 2021 No Diablo III NS 2021 No Rhythm Heaven Quest2 2021 No Xuan Zhi Ji Xia Zhuan PC 2022 No Monster Hunter Rise:曙光 NS 2022 Only released Metroid: Survival Fear NS 2022 Yes Hades NS 2022 No Splatoon 3 NS 2022 Yes Mario Kart 8 NS 2022 None Thirteen Machines Defense Circle NS 2022 Yes Kunlun: The Rise of the Emperor NS 2022 No Rainbow Islands 2 NS 2022 Yes NieR: Automata NS 2022 No Xenoblade Chronicles 2 NS 2022 Yes Xenoblade Chronicles 3 NS 2022 Yes Magical Night NS 2022 No Persona 5 Royal NS 2022 No Hmm… It seems that no matter how busy my work is, I don’t seem to play games for too little time (after all, I have to spend late nights playing… Among these games, the ones with the most feelings are [Re:Zero - Starting Life in Another World 2](Re:Zero - Starting Life in Another World 2), [Persona](Persona 4 Golden and Persona 5 Royal), and INVALID POST SLUG PROVIDED Metroid. I’ll wait for NieR, Xuan Zhi Ji Xia Zhuan, Xenoblade Chronicles, Xenoblade Chronicles 3, Magical Night, and Persona 5 Royal to be completed or reach a certain level before filling in the blanks.","categories":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/categories/Gaming/"}],"tags":[{"name":"Playing games","slug":"Playing-games","permalink":"https://silenwang.github.io/en/tags/Playing-games/"}]},{"title":"Persona 4 and 5","slug":"女神异闻录4和5","date":"2023-01-22T23:30:08.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2023/01/22/女神异闻录4和5/","permalink":"https://silenwang.github.io/en/2023/01/22/%E5%A5%B3%E7%A5%9E%E5%BC%82%E9%97%BB%E5%BD%954%E5%92%8C5/","excerpt":"Persona is probably the second game series that made me buy a gaming console, initially being brainwashed by the “P5 is the best” meme for a year. Later, I watched Lex’s P4 recommendation video and couldn’t resist buying a PS Vita. If not for P4G later landing on PC, leaving a legacy for Persona across platforms (it did land on PC last year), I might have bought a second-hand PS4 for P5 at some point in the future…","text":"Persona is probably the second game series that made me buy a gaming console, initially being brainwashed by the “P5 is the best” meme for a year. Later, I watched Lex’s P4 recommendation video and couldn’t resist buying a PS Vita. If not for P4G later landing on PC, leaving a legacy for Persona across platforms (it did land on PC last year), I might have bought a second-hand PS4 for P5 at some point in the future… Last year when I got P5, I also wanted to finish it within the same year, spending my life savings. However… the content is really extensive… By the end of the third semester, I had already spent nearly 80 hours and still haven’t finished it (not because I went off to play Splatoon 3!…) Although I didn’t finish it, I can feel that most people have the same feeling: P5 excels in terms of system design, while P4 resonates more with my heart. It’s not that I don’t like the additional storylines in the Royal Edition compared to the Golden Edition; rather, I find the themes or intentions of P5 less appealing. Perhaps because I’m already a jaded adult… I can’t fully immerse myself in the protagonists’ perspectives. The protagonists’ abilities awaken from their will to resist oppression, definition, and constraints by society, yearning to express their true selves. However, one of the bigger issues is that the antagonists in P5 are written as extremely stereotypical and polarized figures. A group of corrupt adults resort to any means necessary for their desires, solving problems in a very cool but unrealistic way: directly distorting the minds and thoughts of others. This method, when thought about, isn’t much different from traditional RPGs where power is used to persuade, essentially similar to using violence to achieve justice. Therefore, I think that when Akechi initially expressed his opinion on TV wasn’t wrong (although this character has many problems), the Phantom Thieves can indeed be very dangerous. I feel that this ability is essentially similar to how Death Note was obtained by Ryuk, and if the holder isn’t sufficiently enlightened, they will inevitably be used for evil purposes. Of course, some of these issues are also mentioned in the plot (e.g., Mishima, and Ryuji). P4 handles these aspects much better because its theme is acceptance and growth. The protagonists’ abilities awaken from accepting their true selves, while enemies come from characters denying or rejecting themselves (although there are people’s biases). Every time a Shadow is defeated, it deepens the protagonist’s understanding of themselves, as well as the bonds between companions, naturally strengthening their relationships rather than achieving justice through power, as in P5. The protagonists don’t actually grow (growth comes solely from cooperative play). However, this is unavoidable. A good story has already been used by P4… It can’t be done again; we can only hope that future works will balance both aspects better.","categories":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/categories/Gaming/"}],"tags":[{"name":"Idle Gaming","slug":"Idle-Gaming","permalink":"https://silenwang.github.io/en/tags/Idle-Gaming/"}]},{"title":"Registering New Python or R Kernels in Jupyter","slug":"在Jupyter中注册新的Python或R内核","date":"2023-01-17T12:53:40.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2023/01/17/在Jupyter中注册新的Python或R内核/","permalink":"https://silenwang.github.io/en/2023/01/17/%E5%9C%A8Jupyter%E4%B8%AD%E6%B3%A8%E5%86%8C%E6%96%B0%E7%9A%84Python%E6%88%96R%E5%86%85%E6%A0%B8/","excerpt":"When testing, it’s often necessary to create a new conda environment and then install the jupyter kernel within that environment for use with notebooks. Re-registering new kernels each time can be cumbersome, so here is a record of the process…","text":"When testing, it’s often necessary to create a new conda environment and then install the jupyter kernel within that environment for use with notebooks. Re-registering new kernels each time can be cumbersome, so here is a record of the process… Preparing and Registering Python Kernel Activate the environment you want to register the kernel in. 123conda activate YOUR_ENVconda install jupyter ipykernelpython -m ipykernel install --user --name ENV_NAME Preparing and Registering R Kernel Activate the environment you want to register the kernel in. 12conda activate YOUR_ENVconda install jupyter r-irkernel 1IRkernel::installspec(name = &#x27;REG_NAME&#x27;, displayname = &#x27;ENV_NAME&#x27;)","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"jupyter","slug":"jupyter","permalink":"https://silenwang.github.io/en/tags/jupyter/"}]},{"title":"Long Live! Tencent Officially Released New Linux QQ?","slug":"腾讯官方做新LinuxQQ了","date":"2023-01-15T17:22:50.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2023/01/15/腾讯官方做新LinuxQQ了/","permalink":"https://silenwang.github.io/en/2023/01/15/%E8%85%BE%E8%AE%AF%E5%AE%98%E6%96%B9%E5%81%9A%E6%96%B0LinuxQQ%E4%BA%86/","excerpt":"At one point, the biggest obstacle to my daily use of Linux distributions was the lack of QQ. Back then, I was a third-year student and QQ was an essential software for both learning and life. Although smartphones had already come into being, transferring files between phones and Linux computers was quite inconvenient at the time, and there wasn’t even a WPS Linux version… So, I eventually switched back to Windows.","text":"At one point, the biggest obstacle to my daily use of Linux distributions was the lack of QQ. Back then, I was a third-year student and QQ was an essential software for both learning and life. Although smartphones had already come into being, transferring files between phones and Linux computers was quite inconvenient at the time, and there wasn’t even a WPS Linux version… So, I eventually switched back to Windows. Last year, for some unknown reason, Tencent actually released an official Linux QQ. Although it’s based on Electron, it doesn’t seem to be using an open-source project from the internet; instead, it appears to have used the layout of the Mac version? The current interface is quite appealing: Functionally, basic chat and file transfer work fine,表情包 display is also no problem, spaces and short videos are available, but other fancy features are missing for now. It’s currently quite minimalistic… So… When will the one that once claimed to be minimalistic take action?","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://silenwang.github.io/en/tags/linux/"},{"name":"qq","slug":"qq","permalink":"https://silenwang.github.io/en/tags/qq/"}]},{"title":"A Key Difference Between Pandas in Python and R","slug":"Pandas下Apply和R下的一项重要区别","date":"2023-01-15T00:30:41.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2023/01/15/Pandas下Apply和R下的一项重要区别/","permalink":"https://silenwang.github.io/en/2023/01/15/Pandas%E4%B8%8BApply%E5%92%8CR%E4%B8%8B%E7%9A%84%E4%B8%80%E9%A1%B9%E9%87%8D%E8%A6%81%E5%8C%BA%E5%88%AB/","excerpt":"I currently rarely use R for data processing or cleaning because my daily work involves a lot of string extraction&#x2F;processing, which is difficult to do in R. Additionally, the error tracking in R is very demanding on code proficiency and experience, making it almost unbearable for me to write and maintain R code (I tried at a previous company…). However, recently one of my colleagues asked me to use R to complete such tasks because he only knows R. In the process of copying code, I discovered another reason why I don’t need to use R for this type of work…","text":"I currently rarely use R for data processing or cleaning because my daily work involves a lot of string extraction&#x2F;processing, which is difficult to do in R. Additionally, the error tracking in R is very demanding on code proficiency and experience, making it almost unbearable for me to write and maintain R code (I tried at a previous company…). However, recently one of my colleagues asked me to use R to complete such tasks because he only knows R. In the process of copying code, I discovered another reason why I don’t need to use R for this type of work… The content that needs to be processed is essentially taking a subset of the data frame and then generating new columns based on the row content of the subset, before concatenating these subsets. I have done this countless times with Pandas… Therefore, I gradually mastered the use of apply, group, pivot_table, and melt functions in Pandas, and there is nothing that these functions cannot solve for me, except for my poor coding ability, which makes the code inefficient… However, performing the same operation in R is much more difficult… After all, it’s two different languages. Trying to find an equivalent or similar function in R to handle data as I do with Pandas is very challenging, and even when there are equivalents like apply, they still cause problems that I cannot solve, leading me to revert back to using a for loop… The processing to be done involves selecting a subset sub from the dataset data. This sub actually contains two parts: A and B. The task is to perform row-wise judgment, returning part A if the content of A is not missing, otherwise returning part B, with each row’s return value forming a new data frame. This can be easily achieved in Pandas by applying a function to the subset, then writing a function to judge the row and return the result. After apply completes, you directly get the desired data frame. However, there is a huge problem here: the data types of elements within a vector must be the same. When I collected numerical values into the same vector, non-string elements were automatically converted to strings, so although it could run, the newly generated four columns are all strings. To try and solve this issue, I replaced vectors with list in the function, resulting in an intermediate version. Then I thought of another problem: t() transposes data as a matrix because R’s columns are individual vectors. If you don’t do it this way, transposing will result in one column having multiple data types… Therefore, I searched online and copied a piece of code to avoid using t(), resulting in the following version: 1234567891011121314# Function to return the required contentmkSel &lt;- function(rec)&#123; mkA &lt;- rec[[&#x27;MarkerA&#x27;]] mkB &lt;- rec[[&#x27;MarkerB&#x27;]] if (is.na(mkB)) &#123; row &lt;- list(mkA, rec[[&#x27;ColA1&#x27;]], rec[[&#x27;ColA2&#x27;]], rec[[&#x27;ColA3&#x27;]]) &#125; else &#123; row &lt;- list(mkB, rec[[&#x27;ColB1&#x27;]], rec[[&#x27;ColB2&#x27;]], rec[[&#x27;ColB3&#x27;]]) &#125; names(row) &lt;- c(&#x27;Marker&#x27;,&#x27;Tag1&#x27;, &#x27;Tag2&#x27;,&#x27;Tag3&#x27;) return(row)&#125;sub[,c(&#x27;Marker&#x27;,&#x27;Tag1&#x27;, &#x27;Tag2&#x27;,&#x27;Tag3&#x27;)] &lt;- do.call(rbind.data.frame, apply(sub, 1, mkSel, simplify=T)) However, after testing, I discovered the biggest problem… apply function… it simply cannot return a data frame; it can only return a matrix, and matrices are inherently of the same type… Even the entire family of functions does not have one that can meet my needs. So no matter how hard I tried, using apply to achieve my goal was impossible. In the end, I reverted back to the humble for loop to solve the problem… Generate an empty data frame, specify the types for each column, read each row of the sub data frame to generate a new data frame, and finally merge the two data frames according to their indices… So in summary, R’s apply was not designed with this usage in mind; it can only use basic functions… Through a more cumbersome (needing to write several lines at a time, while also seeming less efficient and less reusable) way of achieving what I do with Pandas… Of course, objectively speaking, my proficiency in R is far inferior to that of Pandas. I haven’t used many useful R packages (like dplyr), so maybe R can complete these tasks well… But… I couldn’t find any relevant materials?","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://silenwang.github.io/en/tags/Pandas/"},{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"}]},{"title":"clash_nps self-built inner net penetration using VPN","slug":"clash-nps自建内网穿透用vpn","date":"2023-01-08T22:03:20.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2023/01/08/clash-nps自建内网穿透用vpn/","permalink":"https://silenwang.github.io/en/2023/01/08/clash-nps%E8%87%AA%E5%BB%BA%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F%E7%94%A8vpn/","excerpt":"Since the start of the pandemic, I have had some remote work needs. Today, I finally put together a relatively convenient and nearly VPN-like experience for accessing inner networks.","text":"Since the start of the pandemic, I have had some remote work needs. Today, I finally put together a relatively convenient and nearly VPN-like experience for accessing inner networks. Past Used SchemesMy initial requirement was simply to log in to the target server through SSH. Therefore, I initially bought a VPS as a jumpboard machine. On the target machine, I used SSH or autossh to start reverse proxy services, allowing me to access the 22 port of the target machine via the port on the VPS. The biggest advantage of this scheme is its simplicity; SSH is something that all Linux systems come with by default. Additionally, once connected via SSH, you can use it for reverse proxying, theoretically enabling access to any inner network IP and port. However, its drawbacks are that as more access needs arise, each new IP&#x2F;port requires logging into the target machine again and running a reverse proxy command. This makes it no longer simple or user-friendly… Furthermore, if power is lost and needs to be restarted… All these steps have to be repeated… Extremely troublesome. Later, IT started enabling NPS (Network Penetration Server), which is an inner net penetration tool written in Go. Installation and usage are extremely simple. After setting up the server and client, if there are new access requirements, you can create a private link on the NPS management page to set it up. If encountering restarts, restarting the NPS client is sufficient. Additionally, with administrative permissions, the NPS client can also be configured to start automatically. This saves a lot of work. However, when too many things need to be accessed, each port still requires running an NPC process locally. Furthermore, because it maps ports to the local machine, inner network addresses become 127.0.0.1, which can cause some services to have issues (e.g., Gitea’s content). Recently, while checking the NPS management interface, I learned that the Socks proxy mode can be used to access various inner net resources and implement VPN functionality. Therefore, I tried it out again and finally achieved accessing inner network resources using the Socks mode and Clash client, without affecting external internet traffic. Current Linux SchemeRequired Software&#x2F;Hardware A VPS with a public IP, referred to as VPS, which also needs to be able to configure port forwarding permissions on this VPS. A host within the target inner net that can access the VPS. This machine is referred to as jumpboard, and the request for accessing the inner net actually originates from this machine. NPS server&#x2F;client software, which can be obtained from GitHub here. A Clash client software that can intercept global traffic. For Linux, you can use the Linux version of Clash For Windows or Clash Verge. Note that there was a permission issue with Clash Verge’s Tun mode before, but I’m not sure if it has been fixed. Implementation Steps Set up NPS on the VPS and start the client on the jumpboard. Then configure the Socks proxy settings in the NPS service interface. For specific operations, refer to this blog. Note down the account and password for the Socks service. Install Clash For Windows and follow the official documentation to enable Tun mode (global mode, which will create a virtual network card to intercept all traffic on your machine). Since I already have a converted configuration file for Clash, open the existing configuration file and manually add the Socks5 service from the VPS. Also, fill in the Clash rules yourself. An example configuration segment is as follows (the rules should be written at the beginning because once a match is found, it won’t continue matching): 1234567891011121314151617proxies: - name: NPS_SOCKS server: 11.11.11.111 port: 2233 type: socks5 username: socks5 password: socks123456proxy-groups: - name: SOCKS5 type: select proxies: - NPS_SOCKS - DIRECTrules: - IP-CIDR,192.168.0.111/32,NPS_SOCKS Tip: Additionally, if the rule matches a single address, it should be written as XXX.XXX.XXX.XXX/32. If it matches an entire subnet, it should be written as XXX.XXX.XXX.0/16. Of course, writing a specific address with /16 will cause the entire subnet to be proxied. Restart the Clash service to access. Current ChromeOS &#x2F; ChromiumOS &#x2F; Android SchemeFor these systems, only the Clash client changes. Use the Clash for Android app from the store. For ChromeOS, you need to first enable the Android subsystem and then install it from the store (before installation, set the system proxy; after installation, close the system proxy and use Clash client to route traffic). Since Android’s VPN inherently intercepts all traffic, you don’t need to configure mode settings. Just configure the rules directly. However, by default, the client does not proxy requests to inner network IPs. Find the Settings &gt; Network &gt; Bypass Private Networks option and disable it. Future PlansSince using Clash for rule-based traffic diversion, this scheme can actually set up simultaneous access to multiple inner net resources, as long as the IP addresses do not conflict. In the future, I will change my home network’s IP segment directly…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"nps","slug":"nps","permalink":"https://silenwang.github.io/en/tags/nps/"},{"name":"clash","slug":"clash","permalink":"https://silenwang.github.io/en/tags/clash/"}]},{"title":"One Year Worse Than the Last","slug":"一年比一年糟糕","date":"2023-01-08T21:27:46.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2023/01/08/一年比一年糟糕/","permalink":"https://silenwang.github.io/en/2023/01/08/%E4%B8%80%E5%B9%B4%E6%AF%94%E4%B8%80%E5%B9%B4%E7%B3%9F%E7%B3%95/","excerpt":"Recalling 2022, it was basically full of bad things. Work was even more tiring, achievements were fewer, health was worse, spirits were more depressed, free time would be less soon, and even finding a barbecue place several times became difficult… Can’t it get better… I hope the new year won’t be even worse…","text":"Recalling 2022, it was basically full of bad things. Work was even more tiring, achievements were fewer, health was worse, spirits were more depressed, free time would be less soon, and even finding a barbecue place several times became difficult… Can’t it get better… I hope the new year won’t be even worse…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"2023","slug":"2023","permalink":"https://silenwang.github.io/en/tags/2023/"}]},{"title":"My Chromebook Journey","slug":"chromebook入坑","date":"2022-10-16T21:44:36.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2022/10/16/chromebook入坑/","permalink":"https://silenwang.github.io/en/2022/10/16/chromebook%E5%85%A5%E5%9D%91/","excerpt":"I have always been interested in the status of domestic Linux systems (although I haven’t used any long-term except for an ancient version of Linux Deepin), so when Flint OS was still called FydeOS, I paid attention to this Chromium OS-based special edition. This also led me to understand why Chrome OS, which has a mysterious high share overseas in the past, is popular.","text":"I have always been interested in the status of domestic Linux systems (although I haven’t used any long-term except for an ancient version of Linux Deepin), so when Flint OS was still called FydeOS, I paid attention to this Chromium OS-based special edition. This also led me to understand why Chrome OS, which has a mysterious high share overseas in the past, is popular. Chrome OS is developed by Google and its features include being lightweight, simple, and heavily relying on cloud services. In other words, it’s an operating system designed for low-performance netbooks, with the idea that most users don’t need strong performance to run a lot of professional local software; they just need a device that can access the required services through the web. Therefore, Chrome OS was originally designed not to run any services locally, and all functions needed to be accessed through website services. The browser Pro Max Plus system (then received a lot of complaints). From its development, it seems this idea might not be entirely wrong, but… with the addition of Linux subsystems and Android subsystems, completely cloud-based systems are still too early. However, this doesn’t mean it’s too early for my work… My work involves handling large amounts of gene sequencing data, which is too huge to process using mobile devices yet. Therefore, there has always been a need in my work to separate actual computational tasks from other work. On the other hand, many important tools I use, such as Jupyter, RStudio, and VSCode, already have mature server versions. After setting up corresponding services, I started thinking about whether it would be possible to complete daily work requirements using a Chromebook or a device similar to Chrome OS (as mentioned earlier). So, in the following days, I first tried FydeOS(x86), then bought Lenovo Duet, and finally entered the Chromebook pit… Previously, I had tried setting up self-hosted cloud services to prepare for working on Chromebooks only. However, like the countless pitfalls I’ve encountered over 10 years using Linux distributions and even experiencing life-threatening losses before being able to use them smoothly, there are still many issues with using such a minimalist system for work at this stage… I’ll list these problems one by one later…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"chromeos","slug":"chromeos","permalink":"https://silenwang.github.io/en/tags/chromeos/"},{"name":"chromebook","slug":"chromebook","permalink":"https://silenwang.github.io/en/tags/chromebook/"},{"name":"fydeos","slug":"fydeos","permalink":"https://silenwang.github.io/en/tags/fydeos/"}]},{"title":"Setting Up a Cloud Development Environment","slug":"搭建远程开发环境","date":"2022-09-24T21:55:29.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2022/09/24/搭建远程开发环境/","permalink":"https://silenwang.github.io/en/2022/09/24/%E6%90%AD%E5%BB%BA%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/","excerpt":"A long time ago, I was interested in a special operating system made in China called FydeOS. It is based on ChromiumOS and is a Chinese version of ChromeOS. The main idea behind this operating system is that many people do not need high-performance terminal computers for work; they just need an entry-level device that can access essential work services through a browser.","text":"A long time ago, I was interested in a special operating system made in China called FydeOS. It is based on ChromiumOS and is a Chinese version of ChromeOS. The main idea behind this operating system is that many people do not need high-performance terminal computers for work; they just need an entry-level device that can access essential work services through a browser. This concept is indeed very interesting, making me think about the different philosophies of version control software like Git and SVN. Git follows a distributed model where each developer has a complete copy of the project code with their own branches. On the other hand, SVN uses a centralized model where users access only the parts they need through a service. Personally, I believe that neither distributed nor centralized models are inherently better; they have different applications depending on the situation. In my current work, everything is in a completely distributed manner without any backups, with both the working environment and content located on my personal or company computers. If one day my device fails… it would take quite some time to recover. Additionally, if I need to urgently handle something but the device is not nearby or has no power… there’s nothing much I can do except worry. So, I also want to try whether my current work can be fully cloud-based. That is, deploying a series of self-hosted services and completing all work through browsers or other development tools that support remote projects, with actual execution happening on the server side. All Software Needed for My WorkIn bioinformatics, it’s essentially about writing scripts, creating statistical graphs, refining graphics, and writing various kinds of documents. The software I actually use includes: Code writing&#x2F;testing: VSCode, Jupyter Environment deployment: conda, docker File organization: System自带的文件管理器 Document processing: WPS series Literature review: Zotero Image processing: GIMP What is Needed to Set Up a Cloud Development EnvironmentTo achieve a fully cloud-based work environment, ideally, all the above software should have self-hosted cloud versions that can solve the following issues: Code writing&#x2F;testing: code-server, Jupyter is already a service and can be accessed remotely. Environment deployment: Remote SSH login or find a web terminal emulator. File organization: Nextcloud, firerun Document processing: only office server Literature review: paperpile (paid) In summary, the only thing left is image editing… I haven’t found any self-hosted solutions yet. I’ll continue to keep an eye on this. AttemptML-Hub &#x2F; ML-Workspace ConfigurationML-Hub &#x2F; ML-Workspace is a container-based multi-network service integration project that includes JupyterHub with many plugins, code-server, and a simple file manager. If not for user permission issues when running Docker, it would instantly solve about 60-70% of the problems… Both are relatively easy to use; after installing Docker or Podman, you can start the services directly. Intranet Penetration (nps)nps is a Go language-based… composite network tool? One of its functions is intranet penetration. nps includes both server and client components. It’s important to note that the nps server acts as an intermediary station, connecting various clients together. Therefore, when setting it up, you first need a host with a public IP address to deploy the nps server. Then, the machines to be accessed need to connect to the nps server using the client npc. Other clients can then use the nps intermediary to connect to the target client. In summary… unlike VNC, RDP, and SSH, where the machine being accessed runs a service, this method uses the nps server as an intermediary. HTTPS Proxy SettingsIf you want to host the built services under a domain and access them directly via the domain, for security reasons, HTTPS is still necessary. Domain registration can be done by finding the cheapest available one first, then using Caddy for reverse proxying. After much trial and error, I ended up with this configuration. 1234567891011121314&#123; http_port 8888 https_port 8384&#125;:8384 &#123; reverse_proxy localhost:7788 tls /etc/caddy/domain.top.cer /etc/caddy/domain.top.key&#125;your.domain.top:8384 &#123; reverse_proxy localhost:7788 tls /etc/caddy/domain.top.cer /etc/caddy/domain.top.key&#125; Firewall Settings (iptables)As before… for security reasons, a firewall should still be enabled. I also found some tutorials on how to use iptables (CENTOS 8开启iptables), but this is quite outdated. Many new systems now use other firewall management programs. Docker访问宿主机端口方法 iptables -I INPUT -p tcp --dport 80 -j ACCEPT iptables -I INPUT -p tcp --dport 443 -j ACCEPT iptables -I INPUT -p tcp --dport 7788 -j REJECT iptables -I INPUT -p tcp -s 127.0.0.1 --dport 7788 -j ACCEPT","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"https","slug":"https","permalink":"https://silenwang.github.io/en/tags/https/"},{"name":"jupyter","slug":"jupyter","permalink":"https://silenwang.github.io/en/tags/jupyter/"},{"name":"cloud","slug":"cloud","permalink":"https://silenwang.github.io/en/tags/cloud/"},{"name":"firewall","slug":"firewall","permalink":"https://silenwang.github.io/en/tags/firewall/"},{"name":"iptables","slug":"iptables","permalink":"https://silenwang.github.io/en/tags/iptables/"}]},{"title":"Thoughts on Tinkering with Graph Databases","slug":"倒腾图数据库时想到的","date":"2022-08-25T01:44:46.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2022/08/25/倒腾图数据库时想到的/","permalink":"https://silenwang.github.io/en/2022/08/25/%E5%80%92%E8%85%BE%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93%E6%97%B6%E6%83%B3%E5%88%B0%E7%9A%84/","excerpt":"Recently, while organizing our materials for using graph databases, I thought of a problem. Actually, from a performance perspective, graph databases may not necessarily achieve better performance. For us, the significance lies in simplifying the requirements for database design.","text":"Recently, while organizing our materials for using graph databases, I thought of a problem. Actually, from a performance perspective, graph databases may not necessarily achieve better performance. For us, the significance lies in simplifying the requirements for database design. We have a lot of data with many interrelated aspects. For those of us who are not in the database-related industry, although it takes effort to clarify the actual logical relationships between the information we need to record, this is something that must and can be done. However, designing a suitable library and table to store these things… that’s really a daunting task… At this point, graph databases have an absolute advantage. Because… as long as the logic is clear, you can simply store it according to this logical relationship… So… the greatest significance of using this thing… is saving database structure design, and if you find that your logic isn’t quite right, changing nodes is also easier… That’s all","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"Graph Database","slug":"Graph-Database","permalink":"https://silenwang.github.io/en/tags/Graph-Database/"}]},{"title":"Using Marp","slug":"Marp的使用","date":"2022-06-22T15:40:28.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2022/06/22/Marp的使用/","permalink":"https://silenwang.github.io/en/2022/06/22/Marp%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"In daily work, there are always times when you need to create a presentation document to show project progress or results. However, creating a PPT is quite time-consuming (my obsessive-compulsive disorder always makes me adjust the font and position). I’ve never used any of the fancy elements and features of PPTs, so I found Marp to save some effort.","text":"In daily work, there are always times when you need to create a presentation document to show project progress or results. However, creating a PPT is quite time-consuming (my obsessive-compulsive disorder always makes me adjust the font and position). I’ve never used any of the fancy elements and features of PPTs, so I found Marp to save some effort. Marp is a project that generates presentation documents using markdown syntax. With basic markdown syntax, you can quickly create a presentable document (it’s actually quite ugly), supporting generation into pptx, pdf, and web pages. I’ve been using it for about a month, and the experience has been good (the audience might find it eye-straining). Basic Usage of Marp in VScodeThe Marp project has multiple available programs, but the most convenient one is definitely the VScode plugin. No special settings are required; just install and use it. After installation, when you open a markdown file, an additional button will appear in the upper right corner of the editor. You can switch the current file to Marp mode. Then you can start writing… Since it’s all basic markdown syntax, there’s not much to pay attention to except for using --- to separate the content of each page. For more details, see the official documentation. Use Cases for ReferenceInserting Images Left&#x2F;RightIn my presentations, I often need to include an image and then provide some explanations. This side-by-side arrangement looks more reasonable, but basic markdown syntax doesn’t support this… You can achieve it using Marp’s background image syntax. Here, contain automatically adjusts the size of the image; otherwise, it usually exceeds the slide range. The example uses right placement, but you can also use left to place it on the left side. For vertical alignment, contain doesn’t seem to work. 12345678---marp: true---## A statistics chart is on the right- The image is randomly found![bg contain right](https://seaborn.pydata.org/_images/grouped_barplot.png) Footnotes and Page NumbersBy adding two settings at the beginning, you can enable footnotes and page numbers. However, the position seems to be unchangeable using markdown syntax. 123456---marp: truefooter: 20220610---## Displaying page numbers","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"Marp","slug":"Marp","permalink":"https://silenwang.github.io/en/tags/Marp/"}]},{"title":"About Reading More Papers Than Thesis in the Last Two Years","slug":"关于我这两年工作看论文比读研还多这件事","date":"2022-04-05T16:10:03.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2022/04/05/关于我这两年工作看论文比读研还多这件事/","permalink":"https://silenwang.github.io/en/2022/04/05/%E5%85%B3%E4%BA%8E%E6%88%91%E8%BF%99%E4%B8%A4%E5%B9%B4%E5%B7%A5%E4%BD%9C%E7%9C%8B%E8%AE%BA%E6%96%87%E6%AF%94%E8%AF%BB%E7%A0%94%E8%BF%98%E5%A4%9A%E8%BF%99%E4%BB%B6%E4%BA%8B/","excerpt":"It was a night when I felt overwhelmed by reading papers (March 29, 2022). As I browsed through the messy directory tree in Zotero… it suddenly occurred to me how many papers I had read over the past few years? Exporting from emails… I was shocked… 561 entries, 419 PDFs downloaded… really… If I had gone straight for a PhD, I probably wouldn’t have read so many papers.","text":"It was a night when I felt overwhelmed by reading papers (March 29, 2022). As I browsed through the messy directory tree in Zotero… it suddenly occurred to me how many papers I had read over the past few years? Exporting from emails… I was shocked… 561 entries, 419 PDFs downloaded… really… If I had gone straight for a PhD, I probably wouldn’t have read so many papers. So I decided to take a closer look at the information of these papers. Steps TakenIt was a night when I felt overwhelmed by reading papers (March 29, 2022). As I browsed through the messy directory tree in Zotero… it suddenly occurred to me how many papers I had read over the past few years? Exporting from emails… I was shocked… 561 entries, 419 PDFs downloaded… really… If I had gone straight for a PhD, I probably wouldn’t have read so many papers. Exported the bibliography from Zotero as an ris file Used rispy to parse the ris file into a dataframe, retaining the type of literature, authors, year of entry, abstract, and publication journal. Retained entries where the type of literature was a journal article. Used wordcloud to create word clouds for authors and publishing journals. Combined the abstracts into a single string and used wordcloud to self-segment and create a word cloud. Created a bar chart for the year of entry. The approximate results are shown below: Author Word Cloud From the graph, we can see a few prominent authors. Let’s take a brief look at them: Griffith Malachi, Griffith Obi L.: These two Griffiths are twins and come from the Griffith Lab. This lab specializes in applying bioinformatics to various aspects of cancer. They have developed a series of related software&#x2F;databases. Sette Alessandro: From the Sette Lab, an expert in immunology. His research mainly focuses on immune studies of pathogens, especially viruses. Since the outbreak of COVID-19, most of his published research seems to be related to COVID. Morten Nielsen: A researcher from the Technical University of Denmark (DTU). His main area of focus is likely algorithm development for immune recognition. The prominent appearance in the graph might be due to my collecting multiple papers related to affinity prediction. His university profile can be found here. Nir Hacohen: A researcher at the Broad Institute, focusing on immunology according to his profile. I don’t have a particular impression of this author yet. Bjoern Peters: From the Peters Lab, focusing on bioinformatics in immunology. He is from the same La Jolla Institute for Immunology as Sette Alessandro. Eilon Barnea: From Meytal Landau’s Lab. His main research seems to be computational and experimental identification of immune peptides? Arie Admon: From the Arie Admon Lab, mainly researching proteomics. I think I collected some papers related to mass spectrometry identification. Michal Bassani-Sternberg: An expert in tumor biology and tumor immunology. According to her profile, she mainly uses mass spectrometry as a research tool. Anthony Purcell: From the Monash University Research Profile. His research direction is bioinformatics applications in tumor immunity and autoimmune diseases. Anne Searls De Groot: Co-founder of EpiVax. Such a big name… I think it was because there were many papers from their company that I read at the time… Ugur Sahin: CEO of BioNTech… hmm, also brought in when reading company literature. Catherine J Wu: Researcher focusing on neoantigens. Recently, several research results led by her have been included… Additionally, she has assisted the Broad Institute in tumor immunity-related research. Journal Word Cloud In terms of journals… it’s still dominated by bioinformatics, with a lot from biotechnology and immunology due to the application direction. Abstract Word Cloud The abstract word cloud clearly shows the directions of my recent work… peptides, neoantigens, T cells… Year of Entry Statistics As for the statistics on the year of entry… it’s clear why I feel overwhelmed by reading papers recently… That’s all.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"papers","slug":"papers","permalink":"https://silenwang.github.io/en/tags/papers/"},{"name":"zotero","slug":"zotero","permalink":"https://silenwang.github.io/en/tags/zotero/"}]},{"title":"R Plotting Issues Record","slug":"使用ggpubr做图的一些问题","date":"2022-03-13T03:12:07.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2022/03/13/使用ggpubr做图的一些问题/","permalink":"https://silenwang.github.io/en/2022/03/13/%E4%BD%BF%E7%94%A8ggpubr%E5%81%9A%E5%9B%BE%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/","excerpt":"Recently, while helping a colleague create plots, I uncovered some new issues and solutions related to ggpubr and survival analysis packages. Here are the records.","text":"Recently, while helping a colleague create plots, I uncovered some new issues and solutions related to ggpubr and survival analysis packages. Here are the records. survminer Plot Saving IssueThe survminer package includes a convenient function called ggsurvplot, which allows for easy creation of survival curves with group significance tests, as well as frequency tables below the plot. However, this function returns a ggsurvplot object rather than a ggplot() object. This object contains survObj$plot for the survival curve and survObj$table for the frequency table. Therefore, if you need to adjust or modify the plot using ggplot syntax, you should operate on the objects within these two components. ggpubr Still Has Bug with Significance Test Annotations in Faceted PlotsAs of December last year, there is still a bug in ggpubr where box plots (other facets may also have this issue) append significance test annotations incorrectly after faceting. The data and annotation positions are incorrect. You need to manually draw individual plots and then combine them. Table Annotations Inside the PlotTo be filled in later. Multiple Ways to Implement Significance AnnotationsDue to a complex requirement from a client, where significance tests should be based on a reference group with two groups instead of one, ggpubr‘s built-in functions cannot meet this need. I spent considerable time researching how to manually implement it, and different methods for various requirements are recorded here: To be filled in later.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"},{"name":"ggplot","slug":"ggplot","permalink":"https://silenwang.github.io/en/tags/ggplot/"},{"name":"ggpubr","slug":"ggpubr","permalink":"https://silenwang.github.io/en/tags/ggpubr/"},{"name":"survminer","slug":"survminer","permalink":"https://silenwang.github.io/en/tags/survminer/"}]},{"title":"Dash Implementation for Interactive Data Point Selection and Calculation","slug":"Dash交互处理数据","date":"2022-03-13T03:09:08.000Z","updated":"2026-02-10T14:03:13.683Z","comments":true,"path":"2022/03/13/Dash交互处理数据/","permalink":"https://silenwang.github.io/en/2022/03/13/Dash%E4%BA%A4%E4%BA%92%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE/","excerpt":"A while ago, I implemented an interesting interactive data processing case using Dash. Here, I’ll record it.","text":"A while ago, I implemented an interesting interactive data processing case using Dash. Here, I’ll record it. The problem was that there were many data entries, and the client wanted to perform metric calculations based on these data entries. However, they didn’t want to use all the data; instead, they needed to draw the data points on a scatter plot and then select a subset of these points from the graph to perform metric calculations using the corresponding data entries. My first reaction was to check if Dash had any relevant features. To my surprise, it does! See this official documentation The implementation turned out to be quite simple. I just needed to add feedback for selection events: 123456789101112131415161718192021222324252627282930313233343536373839404142434445# Drawing the graphimport plotly.express as pxdata_b1_p1 = pd.read_csv(&#x27;1st.part1.fmt.csv&#x27;)fig_b1_p1 = px.scatter(data_b1_p1, x=&#x27;FFPE_X&#x27;, y=&#x27;FFPE_Y&#x27;, color=&#x27;Pos&#x27;, opacity=0.5)fig_b1_p1 = fig_b1_p1.update_layout( width=600, height=600).update_traces( mode=&#x27;markers&#x27;, marker_size=2)# Setting up the framework structure for Dash app components; here, I only place a div to hold the graphapp.layout = html.Div([ html.Div([ dcc.Markdown(&quot;demo title&quot;), dcc.Graph( id=&#x27;fig_b1_p1&#x27;, figure=fig_b1_p1 ), html.Pre(id=&#x27;sel_b1_p1&#x27;) ],])# Defining the data processing functiondef show_mean(selectedData, base): if selectedData: selData = pd.DataFrame(selectedData[&#x27;points&#x27;]).rename(columns=&#123;&#x27;x&#x27;: &#x27;FFPE_X&#x27;, &#x27;y&#x27;: &#x27;FFPE_Y&#x27;&#125;) selData = selData.merge(base, how=&#x27;left&#x27;, on=[&#x27;FFPE_X&#x27;, &#x27;FFPE_Y&#x27;]).drop( columns=[&#x27;curveNumber&#x27;, &#x27;pointNumber&#x27;, &#x27;pointIndex&#x27;, &#x27;FFPE_X&#x27;, &#x27;FFPE_Y&#x27;, &#x27;Pos&#x27;, &#x27;OID&#x27;, &#x27;Marker&#x27;] ) mean = nanmean(selData[&#x27;col1&#x27;]) return f&quot;&#123;mean&#125;&quot; else: return &#x27;Waiting for selection&#x27;# Defining the callback function@app.callback( Output(&#x27;sel_b1_p1&#x27;, &#x27;children&#x27;), [Input(&#x27;fig_b1_p1&#x27;, &#x27;selectedData&#x27;)])def parse1(selectedData): return show_mean(selectedData, data_b1_p1) In summary, using a Plotly interactive graph, which has built-in region select and lasso select functionalities, the selected data points are stored in the selectedData attribute of the graph. Using Dash’s decorator, you can pass this data to your function. However, it’s important to note that the returned data is only for drawing the graph; for example, if it’s a scatter plot, you’ll get the x and y coordinates in list-of-dictionaries format. You need to convert these into a DataFrame and merge them with the original data based on the coordinates to know which data entries correspond to the selected points, after which you can perform the calculations and return the results. The original data is too large to write here; I’ll update this blog with an example using another dataset later.&#96;&#96;&#96;","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"ploty","slug":"ploty","permalink":"https://silenwang.github.io/en/tags/ploty/"},{"name":"dash","slug":"dash","permalink":"https://silenwang.github.io/en/tags/dash/"},{"name":"python","slug":"python","permalink":"https://silenwang.github.io/en/tags/python/"}]},{"title":"Basic Usage of Scrapy","slug":"Scrapy使用","date":"2022-03-13T03:01:14.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2022/03/13/Scrapy使用/","permalink":"https://silenwang.github.io/en/2022/03/13/Scrapy%E4%BD%BF%E7%94%A8/","excerpt":"I previously tried to use Scrapy to crawl information from a drug website, but I didn’t make any records at the time. This time, I helped a classmate crawl some data, so I decided to record it this time.","text":"I previously tried to use Scrapy to crawl information from a drug website, but I didn’t make any records at the time. This time, I helped a classmate crawl some data, so I decided to record it this time. IntroductionScrapy is a Python framework specifically designed for web crawling. Compared to manually sending requests and parsing information using libraries like requests, Scrapy provides built-in functionalities such as request handling, response parsing, exception handling, task queue management, etc., allowing us to perform data crawling tasks in a more professional manner. Of course, the cost is… the learning curve has increased slightly because we need to understand some basic concepts and working principles of this framework. I initially tried to follow the tutorial on Coder’s Tutorial website… but I was completely confused by the diagrams: Case StudyTo simplify things… I only recorded the parts of the actual operation that needed to be understood. 1. Create a New ProjectFirst, Scrapy as a framework does not require users to write code line by line from scratch; instead, it allows us to write operation-related methods within an existing template. When starting a new project, use scrapy startproject demo to create a new project. After creation, the project directory contains many files. You don’t need to understand all of them; just be familiar with the following: demo/spiders: The directory where spider files are stored. Initially, if no spiders have been created, this directory will only contain __init__.py. The specific content of the spiders will be introduced later. demo/settings.py: The configuration file for the crawler, including request headers, anti-crawling protocols, task intervals, cookie settings, etc. Most settings can be understood by reading the comments in the file; just set them accordingly. demo/pipelines.py: Corresponds to the Item Pipeline shown in the example diagram. This is where we define how items are processed. When using the built-in item object to collect target information, this needs to be set up. demo/items.py: The file where items are defined. Similar to above, when using items, they need to be defined in this file (mainly defining what should be stored in an item). 2. Create a SpiderSpider files need further generation using scrapy genspider. For example, running scrapy genspider dSpider www.sample.com will generate a spider file named dSpider.py in the demo/spiders/ directory. The content of this file is roughly as follows: 12345678910import scrapyclass DspiderSpider(scrapy.Spider): name = &#x27;dSpider&#x27; allowed_domains = [&#x27;www.sample.com&#x27;] start_urls = [&#x27;http://www.sample.com/&#x27;] def parse(self, response): pass Using the spider essentially involves rewriting methods within this generated spider file. For instance, here is what I wrote: 12345678910111213141516171819202122import scrapyimport jsonfrom JsDemo.items import JsdemoItemfrom math import ceilclass DemoSpider(scrapy.Spider): name = &#x27;demo&#x27; allowed_domains = [&#x27;www.cssn.net.cn&#x27;] start_urls = [f&#x27;http://www.cssn.net.cn:8000/standards/?a104=IX-ISO&amp;orderby=&amp;post_publicyear=&#123;year&#125;&#x27; for year in range(1960, 2021)] def parse(self, response): pMax = ceil(json.loads(response.text)[&#x27;count&#x27;] / 20) for pNum in range(1, pMax+1): yield scrapy.Request(f&#x27;&#123;response.request.url&#125;&amp;page=&#123;pNum&#125;&#x27;, callback=self.parse_next) def parse_next(self, response): obj = json.loads(response.text) item = JsdemoItem() item[&#x27;raw&#x27;] = obj[&#x27;results&#x27;] yield item The parse method defines how the spider processes the response content after visiting a URL. There are multiple ways to handle this. If you want to follow Scrapy’s recommended approach, you can extract the necessary information from the response content and store it in an item, which will then be processed by the Item Pipeline. This way, the collected items will be packaged as items and enter the Item Pipeline for further processing and saving. If you prefer not to use the Item Pipeline, you can directly handle the data within the parse method, such as creating a DataFrame and writing the content directly to a file. In this case, there is no need to go through the pipeline. The choice of which approach to take depends on your specific task requirements. 3. Process ItemsIf you choose to use items, you also need to write processing logic in demo/pipelines.py: 123456789101112131415161718192021222324252627282930313233343536373839404142434445# Define your item pipelines here## Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html# useful for handling different item types with a single interfacefrom itemadapter import ItemAdapterfrom pandas import DataFrameimport jsonclass JsdemoPipeline: def __init__(self): self.full = [] self.col_map = &#123; &#x27;yf001&#x27;: &#x27;Identification Number&#x27;, &#x27;a000&#x27;: &#x27;Status&#x27;, &#x27;a104&#x27;: &#x27;Publishing Unit&#x27;, &#x27;a104name&#x27;: &#x27;Publishing Unit Name&#x27;, &#x27;a100&#x27;: &#x27;Standard Number&#x27;, &#x27;yf100&#x27;: &#x27;Other Standard Numbers&#x27;, &#x27;a298&#x27;: &#x27;Chinese Title (For Reference)&#x27;, &#x27;a301&#x27;: &#x27;Original Title&#x27;, &#x27;a826&#x27;: &#x27;ICS Classification Number&#x27;, &#x27;a101&#x27;: &#x27;Publication Date&#x27; &#125; def open_spider(self, spider): self.file = open(&#x27;items.jl&#x27;, &#x27;w&#x27;) def process_item(self, item, spider): self.full.extend(item[&#x27;raw&#x27;]) line = json.dumps(item[&#x27;raw&#x27;]) + &quot;\\n&quot; self.file.write(line) return item def close_spider(self, spider): print(&quot;#########run##########&quot;) df = DataFrame(self.full)[self.col_map.keys()].rename(columns=self.col_map) df.to_csv(&#x27;full.data.csv&#x27;, index=0) self.file.close() The open_spider method is executed when the spider starts. It typically opens a file for writing or connects to a database for initialization. The process_item part defines how each item returned by the spider should be processed. In this case, I store the original JSON string in a file and replace keys to create a dictionary. The close_spider method is executed when the spider has finished collecting all information. Here, I convert the dictionary into a DataFrame and write it to a file, closing the file handle. 4. Other SettingsIf you need to change request headers, set up proxies, or configure access intervals to prevent IP bans, you can modify the demo/settings.py file accordingly. 5. Run the SpiderThis is the simplest part. Navigate to the project directory and run scrapy crawl demo. You can then observe the running status; logs will be printed directly on the screen, allowing for debugging based on the logs. SummaryThat’s it for the basic usage. In summary, we need to define what website the spider should crawl, and in the parse method, we specify how to parse the content of the request response. ExtensionThe case study I recorded has a prerequisite that the responses obtained through simple GET or POST requests contain the data we want to retrieve. However, with the development of web technologies, more and more websites are no longer static HTML sites; most websites now have some content loaded via JavaScript, or even entire pages rendered by frameworks like React or Vue. In such cases, simple requests cannot fetch the desired data. I will record this in a future post. That’s it~","categories":[{"name":"Script","slug":"Script","permalink":"https://silenwang.github.io/en/categories/Script/"}],"tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"https://silenwang.github.io/en/tags/Scrapy/"},{"name":"Crawler","slug":"Crawler","permalink":"https://silenwang.github.io/en/tags/Crawler/"}]},{"title":"DingTalk for Linux Just Works Now","slug":"钉钉Linux版能正常用了","date":"2022-02-22T22:22:22.000Z","updated":"2026-02-10T14:03:13.692Z","comments":true,"path":"2022/02/22/钉钉Linux版能正常用了/","permalink":"https://silenwang.github.io/en/2022/02/22/%E9%92%89%E9%92%89Linux%E7%89%88%E8%83%BD%E6%AD%A3%E5%B8%B8%E7%94%A8%E4%BA%86/","excerpt":"Although DingTalk is somewhat of a tool that exploits people, it’s still necessary for work. It’s already better to have one less annoyance choice (speaking of which, Tencent). After all, being exploited is bad enough, but being both exploited and annoyed… being able to eliminate one annoyance is already an improvement.","text":"Although DingTalk is somewhat of a tool that exploits people, it’s still necessary for work. It’s already better to have one less annoyance choice (speaking of which, Tencent). After all, being exploited is bad enough, but being both exploited and annoyed… being able to eliminate one annoyance is already an improvement. Thanks to FlyInWind on AUR providing the solution, the Linux version of DingTalk can now be used normally on Arch-based systems. Currently, all components I need are working fine (chat, approval in the workbench, documents, project management, online document analysis). Video conferencing should be tried this week; if it also works… then everything will be perfect. Currently, the developers have likely incorporated FlyInWind’s solution directly into the build files. The DingTalk installed via AUR automatically deletes libgtk-x11-2.0.so and other two files. As long as you add the following code to the startup script according to dbh625’s instructions, it will work perfectly. 123export XMODIFIERS=&quot;@im=fcitx&quot;export QT_IM_MODULE=&quot;fcitx&quot;export QT_QPA_PLATFORM=xcb Looking forward to the day when all commonly used software has a fully functional Linux version.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"DingTalk","slug":"DingTalk","permalink":"https://silenwang.github.io/en/tags/DingTalk/"}]},{"title":"Graphviz used with draw.io","slug":"Graphviz配合draw-io使用","date":"2021-10-20T01:48:35.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2021/10/20/Graphviz配合draw-io使用/","permalink":"https://silenwang.github.io/en/2021/10/20/Graphviz%E9%85%8D%E5%90%88draw-io%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Since I learned Graphviz, I often use it to create flowcharts or simple diagrams. Most of the time, using it is quite convenient because the flowcharts are small and only used for illustration, not for display. However, when the flowchart is complex and there’s a need for both beautification and presentation, it becomes more troublesome. Therefore, I found an alternative approach: first use Graphviz to create the rough sketch, then use graphviz2drawio to convert the written Graphviz into a format that can be imported into drawio for manual arrangement. In practice, this solution isn’t particularly convenient either… The flowchart framework is there after import, but the original shapes from Graphviz (like circles and squares) are not retained. Additionally, some objects in Graphviz do not exist in drawio, so using Graphviz to create sketches can only retain basic skeletons; a lot of adjustments are still needed. Therefore, in the future, either learn another language for using drawio or find another similar tool that can import Graphviz and further edit it.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"graphviz","slug":"graphviz","permalink":"https://silenwang.github.io/en/tags/graphviz/"},{"name":"drawio","slug":"drawio","permalink":"https://silenwang.github.io/en/tags/drawio/"}]},{"title":"Metroid","slug":"密特罗德","date":"2021-10-18T00:17:50.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2021/10/18/密特罗德/","permalink":"https://silenwang.github.io/en/2021/10/18/%E5%AF%86%E7%89%B9%E7%BD%97%E5%BE%B7/","excerpt":"For me, Metroid is a very magical game. Although I know it’s one of the pioneers of the Metroidvania genre, I only played Castlevania when I was young and didn’t have much interest in this series…","text":"For me, Metroid is a very magical game. Although I know it’s one of the pioneers of the Metroidvania genre, I only played Castlevania when I was young and didn’t have much interest in this series… Until the early 2020s when I was in Guangzhou for two weeks due to the COVID-19 pandemic. My laptop had no games to play, so I only brought my 3DS home and didn’t know what to play. That’s when I tried Samus Returns on the 3DS. At first, I didn’t find it particularly interesting because I’ve played this type of game before, and playing action games on the 3DS is quite tiring… But what makes Metroid so magical is that even though I don’t think it’s very fun, I couldn’t help but… beat it all the way through… This is similar to The Legend of Zelda: A Link Between Worlds on 3DS. Both are not games I particularly want to continue playing, but once I start playing them, I… just beat them all the way through… What made me re-evaluate this series again was when I started playing Bloodstained… Bloodstained is a classic combination of Castlevania: Symphony of the Night and Aria of Sorrow, and everyone called it a spiritual sequel to Castlevania. But after actually playing it, I realized that I didn’t like Bloodstained as much as I liked Aria of Sorrow. Apart from its poor disk reading performance on the Switch, I clearly felt that Bloodstained had heavier RPG elements and more combat compared to other games in the series. At the same time, I also found that compared to combat… I actually prefer the exploration part of these games… So I bought Metroid Dread (生存恐惧) at the original price… After spending a week beating it, I felt that its experience was much better for me than Bloodstained… I hope there will be a sequel to Metroid in the future…","categories":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/categories/Gaming/"}],"tags":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/tags/Gaming/"}]},{"title":"The Great Ace Attorney 2, Exciting but Leaving You Speechless","slug":"大逆转裁判2","date":"2021-10-17T23:48:59.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2021/10/17/大逆转裁判2/","permalink":"https://silenwang.github.io/en/2021/10/17/%E5%A4%A7%E9%80%86%E8%BD%AC%E8%A3%81%E5%88%A42/","excerpt":"During the Mid-Autumn Festival, I didn’t want to work overtime and finished the last two cases of The Great Ace Attorney 2 in one go. The script is not inferior to the Phoenix Wright trilogy, and the second-to-last case, “Future Science and the Return of Souls,” left a deeper impression on me…","text":"During the Mid-Autumn Festival, I didn’t want to work overtime and finished the last two cases of The Great Ace Attorney 2 in one go. The script is not inferior to the Phoenix Wright trilogy, and the second-to-last case, “Future Science and the Return of Souls,” left a deeper impression on me… This case involves two researchers. One of them, during his student days, stole corpses to sell to the university’s medical school for survival and research funding… The other researcher firmly believed in his theory but lacked funding to verify it, and was exploited by a businessman with ulterior motives to defraud government research grants. What impressed me were two points: Students stealing corpses for medical research, even being instructed by teachers to do so, is indeed something that occurred in the history of medicine. The production team researched this and incorporated it into the story, which was quite thoughtful. The experiences of the two researchers were completely different, but they both reflected a problem: young researchers need not only talent but also resources or large amounts of money for scientific research. This contradiction still exists today… I wonder where the production team got their inspiration to write such a story with some social commentary. Although I personally think that The Great Ace Attorney’s script is as brilliant as the Phoenix Wright trilogy, when considering its deeper meaning… The Great Ace Attorney seems more profound and darker. The Phoenix Wright trilogy contains many unrealistic elements, such as trials involving parrots and telephones, along with various exaggerated characters, making it fun to play without any issues. The Great Ace Attorney is very different. While I think the last case’s plot was somewhat disconnected… the rest of the cases felt more like solving real mysteries: the diplomatic dilemma in the case related to the British Assassin, the challenges faced by international students like Natsume Soseki, the difficulties faced by young researchers, and the emphasis on evidence, especially physical evidence, in court. All these elements make The Great Ace Attorney feel more like solving real cases rather than a carefully designed courtroom debate game. I hope there will be a sequel!","categories":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/categories/Gaming/"}],"tags":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/tags/Gaming/"}]},{"title":"ssh no longer supports rsa by default","slug":"ssh默认不支持rsa了","date":"2021-10-11T21:06:49.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2021/10/11/ssh默认不支持rsa了/","permalink":"https://silenwang.github.io/en/2021/10/11/ssh%E9%BB%98%E8%AE%A4%E4%B8%8D%E6%94%AF%E6%8C%81rsa%E4%BA%86/","excerpt":"Today, after upgrading Manjaro, the most basic ssh login suddenly had issues, and the prompts were as follows: 1234Unable to negotiate with UNKNOWN port 65535: no matching host key type found. Their offer: ssh-rsa,ssh-dsslost connectionsign_and_send_pubkey: no mutual signature supported Upon investigation, I found out…well, OpenSSH deems the ssh-rsa encryption method insecure and has directly disabled this key type for logins by default starting from 8.8…","text":"Today, after upgrading Manjaro, the most basic ssh login suddenly had issues, and the prompts were as follows: 1234Unable to negotiate with UNKNOWN port 65535: no matching host key type found. Their offer: ssh-rsa,ssh-dsslost connectionsign_and_send_pubkey: no mutual signature supported Upon investigation, I found out…well, OpenSSH deems the ssh-rsa encryption method insecure and has directly disabled this key type for logins by default starting from 8.8… I initially thought that Manjaro lags behind Arch by several versions, so I went to the Archwiki for a solution, but it wasn’t mentioned yet… After searching for half an hour, I founded that I could add the following lines in ~&#x2F;.ssh&#x2F;config to resolve the issue: 123Host * PubkeyAcceptedKeyTypes +ssh-rsa HostKeyAlgorithms +ssh-rsa The first line means it applies to all hosts, the second line adds ssh-rsa back into the allowed use range, and the third line specifies that all hosts should use the ssh-rsa algorithm’s key.Through actual testing, both lines are required; without the second line, it prompts that there is no ssh-rsa type, and without the third line, it prompts sign_and_send_pubkey: no mutual signature supported. That’s all~","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"ssh","slug":"ssh","permalink":"https://silenwang.github.io/en/tags/ssh/"},{"name":"openssh","slug":"openssh","permalink":"https://silenwang.github.io/en/tags/openssh/"},{"name":"manjaro","slug":"manjaro","permalink":"https://silenwang.github.io/en/tags/manjaro/"}]},{"title":"Building a Task Monitoring Web App with Dash and Flask","slug":"基于dash与flask搭建一个任务监控web-app","date":"2021-06-21T01:24:23.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2021/06/21/基于dash与flask搭建一个任务监控web-app/","permalink":"https://silenwang.github.io/en/2021/06/21/%E5%9F%BA%E4%BA%8Edash%E4%B8%8Eflask%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BB%BB%E5%8A%A1%E7%9B%91%E6%8E%A7web-app/","excerpt":"I’ve been extremely busy lately, with too much work to finish… so I haven’t had much motivation to write. However, this time I’ve applied what I’ve learned in the past six months to build my first web app with database operations, which is worth commemorating. Although I’m short on time, I’ll start with the text and add images later.","text":"I’ve been extremely busy lately, with too much work to finish… so I haven’t had much motivation to write. However, this time I’ve applied what I’ve learned in the past six months to build my first web app with database operations, which is worth commemorating. Although I’m short on time, I’ll start with the text and add images later. This app implements functionality similar to NCBI Blast, allowing users to submit analysis tasks through a web interface and monitor their status from a task list. The app provides a very basic analysis interface where users can select necessary input files and configuration files (previously logged into the database) to start the analysis task and track its progress from the monitoring bar. Although this web interface is not intended for end-users, it serves as a demo for front-end IT communication and will be used by me for testing in the future. The real utility lies in the built-in Web API, which responds to frontend requests to perform actual business tasks. Module IntroductionAlthough the app’s functionality is basic, it involves several components and took considerable time to implement. This post focuses on problem-solving during the implementation process and will cover basic operations of new modules (like Dash and Python MongoDB operations) in future posts. Web APIThe API is built using Flask because it is described as lightweight and simple compared to Django. Since we are not building a large, high-concurrency, high-computation API, I believe Flask is simpler for our needs. Web InterfaceThe web interface is built using Dash, which is similar to Shiny in R. However, there seems to be little presence of Dash in the Chinese bioinformatics community, with most materials being English machine translations. While English materials are also limited, they mainly rely on official documentation and Q&amp;A. Since I am now primarily using Python, using a Python-based tool like Dash is simpler than mixing it with R. Additionally, although my app is simple, it is several orders of magnitude more complex than the bioinformatics scripts I used to write. Therefore, Python’s error tracking is incredibly helpful, saving me a lot of time during debugging. Another point worth mentioning is that Dash evolved from the Plotly plotting package and provides web apps built on Flask. This makes Dash + Flask an excellent combination for my current application scenario because I can write both Dash functions and Flask functions in one main program file, reducing redundancy while ensuring consistency between what I test and demonstrate and what colleagues obtain through the API. Task ControlTask control is based on Snakemake. While I won’t go into detail here, after using it for a long time, I have a better understanding of the statement “Snakemake is Python code” in its documentation. This feature allows me to implement custom functionalities not currently available in Snakemake by writing my own functions within Snakemake. For example, automatic task status updates in this application. File and Task Records (Database)Task and file records are stored using MongoDB for data storage. If we were only building this app, we wouldn’t need any database; simple files or structured directories could suffice. Using a database was purely for practice, as I have other tasks that will require it later. Choosing MongoDB over SQLite was also due to future work considerations, so I took the opportunity to practice with MongoDB now. API DocumentationAPI documentation was not initially part of the app’s planning. Recently, I’ve built many simple web services for internal analysis systems, and maintaining separate documents for each API is inconvenient. Additionally, different APIs might be assigned to different people, making a single comprehensive document difficult to maintain. Therefore, I considered whether there was a way to generate API documentation directly within a single Web-App. After some research, I chose flasgger, which introduces a series of APIs into Flask, allowing authors to write API documentation directly in Python code. The generated documentation can be accessed via Swagger UI directly from the app path. App OverviewThis app’s logic is quite simple: it accepts requests from the frontend, forwards them to the backend to start analysis processes, and updates task statuses in the database during process execution. The frontend then retrieves the program run status either periodically or based on requests. A rough information flow diagram is shown below. Implementation HighlightsAlthough this app is simple, there are issues that need additional solutions during implementation: Ensuring consistency between Dash-App and Flask-API processing, meaning that after testing with Dash-App, no additional changes are needed for the Flask-API part. Mixing Dash-App&#x2F;Flask-App&#x2F;API documentation within a single App, rather than building multiple Web-Apps and redirecting between them. Utilizing Snakemake to control processes and update their status in the database without setting up a separate monitoring process. Consistency Between Dash-App and Flask-API ProcessingThis is relatively easy to implement… by separating actual business processing functions from Dash and Flask functions, meaning that both Dash and Flask response functions format data into a unified format before passing it to an actual business function. The processed data is then reformatted for Dash or API-specified formats: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@app.callback( [ Output(&#x27;info-div&#x27;, &#x27;children&#x27;) ], [ Input(&#x27;upload-info&#x27;, &#x27;contents&#x27;) ], [ State(&#x27;upload-info&#x27;, &#x27;filename&#x27;) ] )def step2_dash(content, filename): &#x27;&#x27;&#x27; Dash response function &#x27;&#x27;&#x27; # Need to parse target path from file if content: _, content_stream = content.split(&#x27;,&#x27;) info_data = step2(content_stream, filename) json_fmt = dumps(info_data, sort_keys=True, indent=4, separators=(&quot;,&quot;, &quot;:&quot;), ensure_ascii=False) return [html.Pre(f&#x27;Parsing successful, extracted information from the information table:\\n&#123;json_fmt&#125;&#x27;)] else: return [html.Pre(&#x27;No parsing result&#x27;)]@app.server.route(&#x27;/step2&#x27;, methods=[&#x27;POST&#x27;])def step2_flask(): &#x27;&#x27;&#x27; Flask response function &#x27;&#x27;&#x27; data = loads(flask.request.form.get(&#x27;data&#x27;)) fileUrl = f&#x27;&#123;data[&quot;dowloadFileUrl&quot;]&#125;?type=&#123;data[&quot;type&quot;]&#125;&amp;fileName=&#123;data[&quot;fileName&quot;]&#125;&#x27; filename = re.findall(r&#x27;fileName=(.+)&#x27;, fileUrl)[0] r = requests.get(fileUrl) info_data = step2(r.content, filename, decode=False) res_data = &#123;&#x27;errCode&#x27;:0, &#x27;data&#x27;: info_data, &#x27;msg&#x27;: &#x27;&#x27;&#125; json_fmt = dumps(res_data, sort_keys=True, ensure_ascii=False) return json_fmt.encode(encoding=&quot;UTF-8&quot;)def step2(content_stream, filename, decode=True): &#x27;&#x27;&#x27; Actual processing part of step2, split to receive different requests &#x27;&#x27;&#x27; if decode: decoded = base64.b64decode(content_stream) else: decoded = content_stream info_data = data_format(decoded, filename) return info_data Mixing Dash-App&#x2F;Flask-App&#x2F;API DocumentationMixing Flask and API documentation is relatively simple because Flasgger is essentially a Flask + API documentation combination. Mixing Dash with Flask is also not difficult; the Dash object has a server attribute that stores the Flask App object, so any part that needs to use this object can directly use app.server: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import dashapp = dash.Dash(&quot;Demo&quot;)app.server.config[&#x27;SWAGGER&#x27;] = &#123; &#x27;title&#x27;: &#x27;Mass Spectrometry Information Entry/Report Generation Service Documentation&#x27;, &#x27;uiversion&#x27;: 2&#125;swagger = Swagger(app.server)app.layout = html.Div( [ html.H1(&#x27;Demo&#x27;), html.Div([ dcc.Upload( id=&#x27;upload-info&#x27;, children=html.Div([ &#x27;Upload here&#x27;, ]), ), html.Div([ html.Pre(&#x27;&#x27;), ], id=&#x27;info-div&#x27;) ]) ],)@app.callback( [ Output(&#x27;info-div&#x27;, &#x27;children&#x27;) ], [ Input(&#x27;upload-info&#x27;, &#x27;contents&#x27;) ], [ State(&#x27;upload-info&#x27;, &#x27;filename&#x27;) ] )def step2_dash(content, filename): &#x27;&#x27;&#x27; Dash response function &#x27;&#x27;&#x27; return [html.Pre(&#x27;parsed&#x27;)]@app.server.route(&#x27;/step2&#x27;, methods=[&#x27;POST&#x27;])def step2_flask(): &#x27;&#x27;&#x27; Flask response function --- tags: - Step2 parameters: - name: dowloadFileUrl in: query type: string required: true description: File download URL responses: 200: description: Returns parsed data schema: properties: errorCode: type: number description: Status code default: 0 data: type: string description: Task submission key default: parsed msg: type: string description: Empty string when normal, error message if status code is not 0 default: &quot;&quot; &#x27;&#x27;&#x27; return &#x27;parsed&#x27; Utilizing Snakemake to Control Processes and Update StatusInitially, I considered using Snakemake’s built-in onstart and onerror features to implement this, as I essentially needed task start and end information submitted to the database. However, after actual implementation, I found that these features are effective for the entire Snakefile, executing once at the start of the total task and once when it fails, without providing responses for individual tasks. Therefore, I had to do it myself… At this point, we can leverage the “Snakemake is Python code” feature. When specifying the shell keyword in a rule, it actually calls Snakemake’s internal API shell function to execute system-level commands, so writing run: shell(&#39;BASH_SCRIPT&#39;) is essentially the same (there are some minor differences in program behavior that we can ignore). However, by using this approach, we can add additional Python code before and after the system command to implement our desired functionality, such as updating task status before and after execution: 1234567891011121314rule step1: input: done = &#x27;&#123;rid&#125;/analysis.done&#x27; output: txt = &#x27;&#123;rid&#125;/done.txt&#x27;, run: # update_status is the function to update status, rid is the task ID update_status(rid, &#x27;step1&#x27;, &#x27;running&#x27;) shell( &#x27;&#x27;&#x27; cp &#123;input.done&#125; &#123;output.txt&#125; &#x27;&#x27;&#x27; ) update_status(rid, &#x27;step1&#x27;, &#x27;done&#x27;) This conveniently achieves our goal. Additionally, for error handling, we don’t need to do this because the error status in this application does not require granularity down to individual rules; onerror is sufficient. However, one thing to note is that Snakemake actually supports a service called papernote, which specifically collects task statuses from the main control flow. This means Snakemake has something similar built-in, but we would need to read the relevant code to understand how it works and whether there are better internal APIs available. Follow-upLast year, I interviewed a bioinformatics engineer who was unable to return to the company during the pandemic. He learned coding at home and built an analysis submission&#x2F;monitoring service. Now that I’ve also done it… looking back, it’s not difficult, but it requires time and external motivation to push me to learn. Additionally, practice is important; only through practice can we discover actual problems and solve them, thereby improving our skills. I need to keep working hard…","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"dash","slug":"dash","permalink":"https://silenwang.github.io/en/tags/dash/"},{"name":"python","slug":"python","permalink":"https://silenwang.github.io/en/tags/python/"},{"name":"flask","slug":"flask","permalink":"https://silenwang.github.io/en/tags/flask/"},{"name":"mongoDB","slug":"mongoDB","permalink":"https://silenwang.github.io/en/tags/mongoDB/"},{"name":"pymongo","slug":"pymongo","permalink":"https://silenwang.github.io/en/tags/pymongo/"},{"name":"flasgger","slug":"flasgger","permalink":"https://silenwang.github.io/en/tags/flasgger/"}]},{"title":"Flashing Mokee ROM on My Nuts Pro","slug":"刷我的坚果Pro魔趣","date":"2021-02-12T20:56:03.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2021/02/12/刷我的坚果Pro魔趣/","permalink":"https://silenwang.github.io/en/2021/02/12/%E5%88%B7%E6%88%91%E7%9A%84%E5%9D%9A%E6%9E%9CPro%E9%AD%94%E8%B6%A3/","excerpt":"I’ve been using my Nuts Pro for over three years, and everything is slow, and the storage is always running out. It’s time to make a final effort… Desperate times call for desperate measures, so I’m going to try flashing it. If it works well, I can keep using it for another year and a half. If not, I’ll have to buy a new one with my end-of-year bonus.","text":"I’ve been using my Nuts Pro for over three years, and everything is slow, and the storage is always running out. It’s time to make a final effort… Desperate times call for desperate measures, so I’m going to try flashing it. If it works well, I can keep using it for another year and a half. If not, I’ll have to buy a new one with my end-of-year bonus. PreparationTalking about the last time I flashed my phone, that was a long time ago… Back then, Android phones were just starting to become popular, and people liked playing with different custom ROMs. Now, unless you have a special need or can’t stand your original system anymore, most people probably don’t flash their phones anymore. Flashing has also become more complicated now… At least my phone needs a specific cable. Of course, I’m grateful to those who are compiling and maintaining the ROM for this device. So, here’s what you’ll need: A 9008 flashing cable (available on Taobao for 10 yuan) INVALID POST SLUG PROVIDED Tool for flashing TWRP (QPST for Windows, qdl for Linux, install with yay -S qdl, it’s an AUR package) The corresponding base package and system package for Nuts Pro (RADIO-odin* and MK90.0-odin*) StepsThe steps are basically following the developer’s tutorial, but since I’m not a Windows user, the flashing tool is different (although I tried both, and the Windows tool didn’t work for me, it only confirmed that my phone was in 9008 mode). Also, because of the inconsistency between the cable I bought and what’s mentioned in the tutorial, the actual way to enter 9008 mode is not consistent with the tutorial (the button on my cable stays pressed when you press it once, and then pops back when you press it again). As for backing up files before flashing… I only backed up my WeChat chat records. The rest can be ignored… After all, I haven’t been making phone calls or sending text messages in a long time… Also, my phone was updated to the latest official system before flashing, and there had never been any previous flashing. Put base package and system package on SD cardMy phone has a 32GB SD card. I put both the base package and the system package directly on the SD card. It should be possible to put them in internal storage as well, since I saw an option during the flashing process. However, since I haven’t tried it yet, I’m not sure. Run qdl on your computer and wait for the phone to connectDownload TWRP, install qdl, unzip it, enter the unzipped directory, and run the following command until you see Waiting for QDL tty.... If it fails, try running it again. 1sudo qdl prog_emmc_firehose_8953_ddr.mbn rawprogram_unsparse.xml Put phone in 9008 mode and start flashing Long press the power button to shut down. Connect the 9008 cable to your computer’s USB port. Press the volume + and - buttons simultaneously. Keep the 9008 cable button in an unpressed state, then insert it into the phone. The red light on the front left corner will briefly flash. When you see the red light, press the button immediately. After that, the phone’s light will keep flashing red, indicating that it has entered 9008 mode. You can release the volume key at this point. If everything goes smoothly, you should see TWRP being flashed on your computer terminal. Wait for it to finish. Flash Mokee ROMFollow the tutorial completely. I did exactly what was written without doing anything extra… It took about 5-10 minutes. After that, restart and set up the system as needed. Short-term experience&#x2F;problemsAfter using it for two days, I feel that it’s much smoother than the original system most of the time, but not as good as expected… It seems that the CPU can’t keep up with the performance demands of domestic apps. However, since apps can be moved to the SD card, it should be fine for another year. The problems I’ve encountered so far: When deleting fingerprints, you need to rename them first before deleting. Processes are killed very aggressively. If you don’t operate for a while, non-background processes will definitely hang. Of course, this is what many comments say… It’s the result of domestic vendors not following Google’s requirements when developing apps… A common problem with Android-like systems using domestic apps. Small lags still exist, especially when there are many Bilibili danmu. PostscriptAfter flashing, the system level is very smooth, but occasionally there are problems within commonly used apps… Then I suddenly thought about WeChat’s “evolution”… Hmm… So I tried replacing old versions of the app… Well, that would be great. Watching videos on Bilibili no longer lags, and the startup speed of Alipay has visibly increased. What can I say… I’ll write a list of historical app versions later…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"android","slug":"android","permalink":"https://silenwang.github.io/en/tags/android/"},{"name":"phone","slug":"phone","permalink":"https://silenwang.github.io/en/tags/phone/"},{"name":"Mokee","slug":"Mokee","permalink":"https://silenwang.github.io/en/tags/Mokee/"}]},{"title":"Flashing Mokee on My Nutro Pro","slug":"给我的坚果Pro刷魔趣","date":"2021-02-12T20:56:03.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2021/02/12/给我的坚果Pro刷魔趣/","permalink":"https://silenwang.github.io/en/2021/02/12/%E7%BB%99%E6%88%91%E7%9A%84%E5%9D%9A%E6%9E%9CPro%E5%88%B7%E9%AD%94%E8%B6%A3/","excerpt":"","text":"I’ve had my Nutro Pro for over three years now, and it’s been slowing down significantly. The storage space is also running low, making it hard to use comfortably. So, I decided to give it one last shot by flashing a new ROM. If it works out well, I’ll continue using it for another year or so. Otherwise, I’ll have no choice but to buy a new phone with my end-of-year bonus. Tools NeededSpeaking of the last time I flashed my phone, it was quite some time ago. Back then, custom ROMs were all the rage as people not only enjoyed their phones but also played around with different operating systems. Nowadays, unless you have a specific need or your original system is really slow and unresponsive, most people probably won’t consider flashing their phones. Flashing has become more complicated than it used to be; in fact, I needed a special cable for this task. Fortunately, there are people who compile and maintain ROMs for this device. Here’s what you’ll need: A 9008 flash cable (about 10 RMB on Taobao, with a button that stays pressed when clicked and releases when clicked again) TWRP for Nutro Pro Flashing tools (QPST for Windows, qdl for Linux; you can install qdl using yay -S qdl, which is an AUR package) The base and system packages for Nutro Pro (you can find them at MokeeDev) Flashing StepsThe flashing process follows the instructions provided by the developer, but since I’m not a Windows user, I used different tools than what was recommended. Additionally, due to inconsistencies between the cable I bought and the one mentioned in the tutorial, the actual method of entering 9008 mode was slightly different (the button on my cable stays pressed for a moment before releasing). As for backing up files before flashing… I only backed up my WeChat chat records using the official WeChat backup feature. Other data can be left as is since I don’t use phone calls or SMS messages much anymore. My phone was already running the latest official system at the time of flashing, and I hadn’t performed any previous flashes. Place Base and System Packages on SD CardI have a 32GB SD card in my phone, so I placed both the base and system packages directly on it. It should be possible to place them on the internal storage as well since there’s an option during flashing, but I haven’t tried it yet. Run qdl on Computer and Wait for Phone InsertionDownload TWRP and install qdl. Extract the TWRP files and navigate to the extracted directory. Run the following command to see “Waiting for QDL tty…”: 1sudo qdl prog_emmc_firehose_8953_ddr.mbn rawprogram_unsparse.xml Enter 9008 Mode on Phone and Start Flashing Long-press the power button to turn off the phone. Connect the 9008 cable to the USB port of your computer. Simultaneously press the volume up and down keys. Keep the 9008 cable button unpressed while inserting it into the phone. The top-left indicator light on the front will briefly flash red; once you see this, immediately press the button. After that, the indicator light will keep flashing red, indicating that the phone is in 9008 mode. You can release your fingers from the volume keys at this point. If everything goes smoothly, you should see the TWRP being flashed on your computer terminal. Wait for it to finish. Flashing MokeeFollow the instructions provided by MokeeDev as closely as possible. It took me about 5-10 minutes to complete the flashing process. After rebooting into the system and making necessary settings, you can start using it. Short-term Usage Experience&#x2F;Minor IssuesI’ve been using it for two days so far, and I feel that it’s significantly more responsive than the original system, though not as good as expected. The system-level performance is excellent, but app performance still has some issues. It seems that the CPU can’t keep up with the performance demands of domestic apps. However, since apps can be moved to the SD card, it should last me another year without issues. The current problems I’ve encountered are: Deleting fingerprints requires renaming the fingerprint first before deletion. Force stopping processes is very aggressive; if you don’t interact with the phone for a while, non-background processes will crash. Some comments suggest that this is due to domestic vendors not following Google’s requirements when developing apps, which is a common issue for apps similar to the original Android system. Occasional small lags, especially when Bilibili has many弹幕. AfterwordThe system-level performance after flashing is excellent, but there are still occasional issues within commonly used apps. Then I remembered the “evolution” process of WeChat… So, I tried replacing it with an older version… It worked! Bilibili no longer lags, and the startup speed of Alipay is noticeably faster. After realizing that newer versions of apps might be causing performance issues, I tried replacing them with older versions… It worked! Bilibili no longer lags, and the startup speed of Alipay is noticeably faster. This makes me think about how apps have evolved over time, often becoming more resource-intensive. Well… what can I say… Here’s a list of historical versions of the app that might help others facing similar performance issues…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"android","slug":"android","permalink":"https://silenwang.github.io/en/tags/android/"},{"name":"phone","slug":"phone","permalink":"https://silenwang.github.io/en/tags/phone/"},{"name":"Mokee","slug":"Mokee","permalink":"https://silenwang.github.io/en/tags/Mokee/"}]},{"title":"Theme Upgrade","slug":"主题升级","date":"2021-02-12T20:26:57.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2021/02/12/主题升级/","permalink":"https://silenwang.github.io/en/2021/02/12/%E4%B8%BB%E9%A2%98%E5%8D%87%E7%BA%A7/","excerpt":"","text":"2020 was too busy… In the middle of the year, I discovered that the material-x theme had been significantly upgraded, but I didn’t have time to update it. By the time 2021 arrived, I gave up on upgrading from the original theme and decided to start a new project based on the documentation. Hexo Installation and SetupTurns out my new computer didn’t even have hexo installed… This was a good opportunity to test whether hexo-cli or hexo is necessary. After testing, I found that hexo-cli is sufficient: 1234sudo pacman -S npmsudo npm install -g hexo-climkdir neo_bloghexo init Volantis Theme and Other Plugin InstallationThe installation method for themes has also changed according to the official documentation. Now, it’s done using npm, rather than cloning the project into the themes directory of the blog project. 12npm i hexo-theme-volantisnpm i hexo-generator-search hexo-generator-json-content Editing Configuration FilesThe configuration files have also changed (apparently due to changes in hexo). The main change is that theme configurations are no longer recommended to be directly modified in themes/yourTheme/_config.yml or node_modules/yourTheme/_config.yml. Instead, create a _config.yourTheme.yml file at the same level as _config.yml, and write the configuration changes you need into this file. This way, the content of your file will overwrite the content in the theme configuration file (it feels like the JSON configuration files in VSCode). The documentation says that this approach has the benefit that when updating the theme, any configuration changes you make won’t be lost. Of course, during actual use, I found that the color settings for the Volantis theme didn’t take effect after modifying _config.yourTheme.yml locally. In the end, I had to change the theme’s configuration file for it to work. After making the necessary changes to the theme content, copy over the original _config.yml, and replace the entire directory with the new blog directory to deploy as usual. Adding DocumentationAdditionally, this time I added documentation. The content hasn’t been updated… It was just a test to see if soft links could be used directly to upload generated documentation. After testing, it didn’t work. For now, I’m temporarily too lazy to learn how to update the blog while updating the documentation. I’ll do it manually for now and figure it out later…","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://silenwang.github.io/en/tags/hexo/"}]},{"title":"Installation and Usage of mSINGS","slug":"mSINGS的安装和使用","date":"2020-09-06T16:46:00.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2020/09/06/mSINGS的安装和使用/","permalink":"https://silenwang.github.io/en/2020/09/06/mSINGS%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","excerpt":"mSINGS is a software tool used to detect MSI (Microsatellite Instability). Its advantage seems to be that it can be used for tumor-only samples.","text":"mSINGS is a software tool used to detect MSI (Microsatellite Instability). Its advantage seems to be that it can be used for tumor-only samples. mSINGS project is hosted on Bitbucket instead of GitHub, and the project has been continuously updated… However, the installation guide written by the author seems a bit too technical… And it also does not have an all-in-one installer, so following the documentation for installation can be a bit confusing… The core of this project is written in Python, and its core functionality involves reading mpileup files generated by samtools for analysis. Therefore, the essential dependencies should be Python3.6 and samtools. (The git is probably used to clone the project.) Since the author specifies the Python version, he recommends using a Python virtual environment (to prevent future explosions). However, since I am not familiar with virtualenv, I will use miniconda instead, which is easier to get started. In summary: creating an environment&#x2F;installing necessary dependencies -&gt; installing the module -&gt; testing. Creating an Environment&#x2F;Installing Necessary Dependencies 1conda create -p /path/to/soft/mSINGS/conda python=3.6 git samtools Installing the Module 1234conda activate /path/to/soft/mSINGS/conda # Enter the environment to install the module itselfgit clone https://bitbucket.org/uwlabmed/msings.git # Clone the project as recommended by the authorcd msings # Enter the directorypython setup.py install # Install the software itself Testing The author mentioned how to create a baseline (which should determine which MSI loci need scanning based on existing data). However, since I am just testing, I will use the files prepared by the project (doc/ directory). Additionally, it’s important to note that the author clearly stated that the input BAM file needs to be aligned to a reference genome without chr strings (provided by GATK), so I found an FQ data for alignment. If the BAM file already meets the requirements, you can skip this step. Another thing to note is that the author specifies in the script run_msings.sh that it activates the virtual environment. Since I did not follow his instructions, the line # source msings-env/bin/activate in the script needs to be commented out. 12345678910111213141516171819set -e# Do not do sort, because mSINGS&#x27; analysis process includes sorting.../path/to/apps/bwa-0.7.12/bwa mem \\ -R &#x27;@RG\\tID:group1\\tSM:TUMOR\\tPL:illumina\\tLB:lib1\\tPU:unit1&#x27; \\ -M -t 16 \\ /path/to/Human_Genome_GRCh37_FASTA/human_g1k_v37.fasta \\ /path/to/soft/mSINGS/run_test/test.fq.R1.gz \\ /path/to/soft/mSINGS/run_test/test.fq.R2.gz \\| /usr/local/bin/samtools view \\ -Sb - \\&gt; test.bamecho &quot;test.bam&quot; &gt; bam_list # Because the script specified that a file with written BAM paths is required, I can only specify it like thissh /path/to/mSINGS/msings/scripts/run_msings.sh \\ bam_list \\ /path/to/mSINGS/msings/doc/mSINGS_TCGA.bed \\ /path/to/mSINGS/msings/doc/mSINGS_TCGA.baseline \\ /path/to/Human_Genome_GRCh37_FASTA/human_g1k_v37.fasta The final result will be in the Combined_MSI.txt file, which should contain the results of each BAM file combined together. Of course, I only tested with one BAM file, so I’m not sure what the structure of the merged results will look like… I tested this sample with 1,166 target MSI loci, and 27 were detected as unstable, representing 2% instability, resulting in a MSS (Microsatellite Stable) status. 12345Position testunstable_loci 27passing_loci 1166msing_score 0.0232msi status NEG Finally, let’s take a look at the author’s script run_msings.sh: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#!/bin/bashset -e# source msings-env/bin/activate# BAM_LIST is a file of absolute paths to each bam fileBAM_LIST=$1;BEDFILE=$2;MSI_BASELINE=$3;REF_GENOME=$4;# Check for required variables:if [ -z &quot;$BAM_LIST&quot; ]; then echo &quot;BAM_LIST is unset&quot; &amp;&amp; exit ; else echo &quot;BAM_LIST is set to &#x27;$BAM_LIST&#x27;&quot;; fiif [ -z &quot;$BEDFILE&quot; ]; then echo &quot;BEDFILE is unset&quot; &amp;&amp; exit ; else echo &quot;BEDFILE is set to &#x27;$BEDFILE&#x27;&quot;; fiif [ -z &quot;$MSI_BASELINE&quot; ]; then echo &quot;MSI_BASELINE is unset&quot; &amp;&amp; exit ; else echo &quot;MSI_BASELINE is set to &#x27;$MSI_BASELINE&#x27;&quot;; fiif [ -z &quot;$REF_GENOME&quot; ]; then echo &quot;REF_GENOME is unset&quot; &amp;&amp; exit ; else echo &quot;REF_GENOME is set to &#x27;$REF_GENOME&#x27;&quot;; fi# &quot;multiplier&quot; is the number of standard deviations from the baseline required to call instabilitymultiplier=2.0 # &quot;msi_min_threshold&quot; is the maximum fraction of unstable sites allowed to call a specimen MSI negative msi_min_threshold=0.2# &quot;msi_max_threshold&quot; is the minimum fraction of unstable sites allowed to call a specimen MSI positivemsi_max_threshold=0.2for BAM in `sed &#x27;/^$/d&#x27; $BAM_LIST`; do SAVEPATH=$(dirname $BAM) BAMNAME=$(basename $BAM) PFX=$&#123;BAMNAME%.*&#125; mkdir -p $SAVEPATH/$PFX echo “Starting Analysis of $PFX” &gt;&gt; $SAVEPATH/$PFX/msi_run_log.txt; date +&quot;%D %H:%M&quot; &gt;&gt; $SAVEPATH/$PFX/msi_run_log.txt; echo &quot;sorting bam of $PFX&quot; &gt;&gt; $SAVEPATH/$PFX/msi_run_log.txt; date +&quot;%D %H:%M&quot; &gt;&gt; $SAVEPATH/$PFX/msi_run_log.txt; samtools sort -o $SAVEPATH/$PFX/$PFX.sorted.bam $BAM &amp;&amp; samtools index $SAVEPATH/$PFX/$PFX.sorted.bam echo &quot;Making mpileups&quot; &gt;&gt; $SAVEPATH/$PFX/msi_run_log.txt; date +&quot;%D %H:%M&quot; &gt;&gt; $SAVEPATH/$PFX/msi_run_log.txt; samtools mpileup -f $REF_GENOME -d 100000 -A -E -l $BEDFILE $SAVEPATH/$PFX/$PFX.sorted.bam | awk &#x27;&#123;if($4 &gt;= 6) print $0&#125;&#x27; &gt; $SAVEPATH/$PFX/$PFX.mpileup echo &quot;MSI Analyzer start&quot; &gt;&gt; $SAVEPATH/$PFX/msi_run_log.txt; date +&quot;%D %H:%M&quot; &gt;&gt; $SAVEPATH/$PFX/msi_run_log.txt; msi analyzer $SAVEPATH/$PFX/$PFX.mpileup $BEDFILE -o $SAVEPATH/$PFX/$PFX.msi.txt echo &quot;MSI calls start&quot; &gt;&gt; $SAVEPATH/$PFX/msi_run_log.txt; date +&quot;%D %H:%M&quot; &gt;&gt; $SAVEPATH/$PFX/msi_run_log.txt; msi count_msi_samples $MSI_BASELINE $SAVEPATH/$PFX -m $multiplier -t $msi_min_threshold $msi_max_threshold -o $SAVEPATH/$PFX/$PFX.MSI_Analysis.txt echo “Completed Analysis of $PFX” &gt;&gt; $SAVEPATH/$PFX/msi_run_log.txt; date +&quot;%D %H:%M&quot; &gt;&gt; $SAVEPATH/$PFX/msi_run_log.txt;doneecho &quot;Creating summary analysis file for all samples&quot; &gt;&gt; $SAVEPATH/msi_run_log.txt;msi count_msi_samples $MSI_BASELINE $SAVEPATH -m $multiplier -t $msi_min_threshold $msi_max_threshold -o $SAVEPATH/Combined_MSI.txt It looks a bit long, but the content is actually quite simple. The author also provided some basic comments. Essentially, it involves sorting each input BAM file and generating mpileup files, then using the msi program (which was installed by python setup.py install) for analysis and data aggregation.&#96;&#96;&#96;","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"mSINGS","slug":"mSINGS","permalink":"https://silenwang.github.io/en/tags/mSINGS/"}]},{"title":"Flask Usage Record","slug":"Flask使用记录","date":"2020-08-09T01:25:45.000Z","updated":"2026-02-10T14:03:13.683Z","comments":true,"path":"2020/08/09/Flask使用记录/","permalink":"https://silenwang.github.io/en/2020/08/09/Flask%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/","excerpt":"Due to project requirements, I finally took the time to actually look into how to use Flask. Here’s a record of what I learned.","text":"Due to project requirements, I finally took the time to actually look into how to use Flask. Here’s a record of what I learned. Since my task this time was relatively simple - just create a Flask server that can accept requests and call Python code to compute results - I only dealt with the most basic usage. Of course, since I’m not familiar with HTML and JavaScript, my understanding here is purely based on my own experience… it may contain many inaccuracies and misleading descriptions… Creating a Flask Service (App)All operations start with creating a Flask App. It’s quite simple - just instantiate the Flask object. 12345from flask import Flaskapp = Flask(__name__)if __name__ == &#x27;__main__&#x27;: app.run(host=&#x27;0.0.0.0&#x27;) Save this as a Python file, and then run python app.py to start the service. Setting the host parameter to &#39;0.0.0.0&#39; allows all IPs to access this service. Binding Requests to FunctionsFlask can convert requests for pages into calls to Python functions. Specifically, use the @app.route decorator before the corresponding function. For &#39;/&#39;, it corresponds to index.html, and for something like &#39;/login&#39;, it corresponds to login.html. Of course, I found that the HTML files don’t necessarily need to exist; you can directly return HTML strings in the response. I prepared a page explaining how to use the API, which I read in and returned as is. 12345@app.route(&#x27;/&#x27;)def index(): with open(&#x27;index.html&#x27;) as f: content = f.read() return content Handling POST RequestsFor my program, I needed to receive some parameters and uploaded file contents for further processing. Since the file content can’t be sent via GET, I only handled POST. Flask’s built-in request object makes it convenient to get all information from the request. 1234567891011from io import BytesIOfrom flask import requestfrom iNeo_MS import sheet_parse@app.route(&#x27;/parse&#x27;, methods=[&#x27;POST&#x27;])def parse(): args = request.args method = args[&#x27;method&#x27;] byteData = request.files.get(&#x27;file&#x27;) data = sheet_parse(BytesIO(byteData.read()), method) return str(data.to_dict(&#x27;records&#x27;)) Testing the API with PythonTesting a POST service can’t be done directly through a browser, so you need to install specialized plugins… but my home network is unstable… so I still rely on Python. The specific method is using the post method from the requests module. Place the parameters in the params parameter and read the file content before sending it via the files parameter. import requests params = &#123; &#39;method&#39;: &#39;pNovo&#39;, &#125; with open(&quot;data/results.res&quot;, &quot;rb&quot;) as f: content = f.read() files = &#123; &quot;file&quot; :(&#39;result.res&#39;, content), &#125; response = requests.post( &#39;http://127.0.0.1:5000/parse&#39;, params=params, files=files","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Flask","slug":"Flask","permalink":"https://silenwang.github.io/en/tags/Flask/"},{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"}]},{"title":"Hexo One-Time Multi-Repository Deployment","slug":"hexo一次性多仓库部署","date":"2020-08-02T01:19:38.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2020/08/02/hexo一次性多仓库部署/","permalink":"https://silenwang.github.io/en/2020/08/02/hexo%E4%B8%80%E6%AC%A1%E6%80%A7%E5%A4%9A%E4%BB%93%E5%BA%93%E9%83%A8%E7%BD%B2/","excerpt":"","text":"Recently, GitHub’s access has been increasingly problematic. Sometimes it’s even difficult to check my blog posts… So I also have a copy hosted on the domestic Git hosting service Gitee. However, this raises a question: how can I update multiple places at once? Firstly, set up the repository on Gitee. The basic method is similar to GitHub. The only thing you need to note is that Gitee’s default project display is different from GitHub. GitHub uses USERNAME.github.io, while Gitee uses USERNAME. Since Gitee has a feature to fork GitHub projects, when creating the project, choose to fork the original GitHub blog repository and then change the repository name to USERNAME (as shown in the example after migration… so it displays that the project has been created). Then, on the project page, select the service menu to set up access from USERNAME.gitee.io to view the blog. Next, set up SSH access to Gitee. This is similar to GitHub and won’t be expanded here. Then, configure the _config.yaml file in the project. In the deployment section, add the following configuration so that you can deploy twice at once. 123456789deploy: - type: git repo: git@github.com:SilenWang/silenwang.github.io.git branch: master - type: git repo: git@gitee.com:silenwang/silenwang.git branch: master Originally, this should have been it… but when testing, another issue arose. My home network can no longer directly connect to GitHub, so the deployment to GitHub fails… Therefore, I searched for ways to add a proxy to Git. Since my configuration uses SSH protocol to connect to GitHub, after checking, it’s necessary to add a ProxyCommand setting in the SSH configuration file as follows: 1234Host github.com User git HostName github.com ProxyCommand nc -x 127.0.0.1:1080 %h %p After adding this, I tried hexo d again and it successfully deployed. That’s all for now~","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://silenwang.github.io/en/tags/hexo/"},{"name":"gitee","slug":"gitee","permalink":"https://silenwang.github.io/en/tags/gitee/"}]},{"title":"pandas Common Operations Record","slug":"pandas常用操作记录","date":"2020-07-26T00:36:49.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2020/07/26/pandas常用操作记录/","permalink":"https://silenwang.github.io/en/2020/07/26/pandas%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E8%AE%B0%E5%BD%95/","excerpt":"","text":"As my current work requires a lot of data statistics and processing, and considering the convenience in error handling and third-party extensibility, Python is much more convenient than R. Therefore, I have started moving my data processing from R to Pandas. In this post, I will record some common operations used in actual work and note down any issues encountered. 1. apply Related1.1. Basic UsageSimilar to Pandas and R, there is only one apply function, no apply family. The apply function essentially loops through the table row by row. Using apply instead of directly itterrow is mainly for parallel considerations. There are many third-party modules that can parallelize apply operations, so initially writing code in this way saves the trouble of rewriting functions later when parallelizing. Of course, another point is to separate the main processing logic from detailed execution content, which to some extent improves code readability. However, personally, it’s more of a habit issue… If the person reading the code never writes like this… Then using apply actually increases the difficulty of reading the code for others… Currently, my approach is generally to write the actual calculation part as a general function and then use an anonymous function to pass parameters when calling apply. 1234def plus(x, y): return x + ydata[&#x27;applyed&#x27;] = data[&#x27;2apply&#x27;].apply(lambda x: plus(x[&#x27;x&#x27;], x[&#x27;y&#x27;])) Of course, if you need to perform many row-wise operations at the same time, it’s better to write a separate func_apply() function to prevent repeated traversal of the data. 1.2. Returning Multiple ColumnsThe conventional usage of apply is to return a numerical value to form one column of data. However, in actual applications, I often need to return multiple columns. Therefore, I found a solution online: 12345def plus(x, y): return x + y, x - ydata[&#x27;x+y&#x27;], data[&#x27;x-y&#x27;] = zip(*data[&#x27;2apply&#x27;].apply(lambda x: plus(x[&#x27;x&#x27;], x[&#x27;y&#x27;]))) 2. Groupby Related2.1. Merging Multiple Rows into a Single Row of DataThis operation is mainly used to aggregate multiple rows of data into a single row result. I remember that R has a specific way to handle this, and Pandas should have it too… However, I couldn’t find it at the time, so I came up with my own solution. This method essentially retrieves data subsets based on the unit to be processed, performs statistics on these subsets, and generates new rows of data to fill in a new table. This approach currently seems inefficient, and because it uses a for loop, parallelization is not as simple as apply. 123456789result_list = []for k, sub_data in data.groupby(by=[&#x27;Grp1&#x27;, &#x27;Grp2&#x27;]): grp1, grp2 = k result_list.append(&#123; &#x27;Stat1&#x27;: sum(sub_data[&#x27;Var1&#x27;]), &#x27;Stat2&#x27;: min(sub_data[&#x27;Var1&#x27;]), &#125;)result = pd.DataFrame(result_list) 3. Missing Value Handling Related3.1. Missing Values on ReadYou can replace multiple values with missing ones using the na_values parameter: 1df = pd.read_table(FILE, sep=&quot;\\t&quot;, header=0, na_values=[&#x27;.&#x27;, &#x27;&#x27;]) 3.2. Missing Values on OutputWhen outputting, you can specify what to fill in for missing values through parameters: 1df.to_csv(FILE, sep=&quot;\\t&quot;, header=0, na_values=[&#x27;.&#x27;, &#x27;&#x27;]) 3.3. Determining if a Value is MissingIn Pandas, if a missing value is generated, it uses the nan object from NumPy, which is different from R’s NA, NULL, and Python’s built-in None. The type of nan is actually float… Therefore, using logical judgment like A == nan will not get True results… I haven’t found a reason for this. However, to determine if a value is missing, it’s best to use the isnull method provided by Pandas. Other modules like math, numpy have built-in missing value judgment functions, but these functions have a fatal flaw… They only accept parameters of type float… So if I use them, I still need to add an additional type judgment layer, which is quite troublesome. Therefore, using the isnull method provided by Pandas is the most convenient. 4. Most Common Output ParametersBecause I habitually output TSV files, when using Pandas to output files, I usually write it like this: 1df.to_csv(FILE_PATH, sep=&quot;\\t&quot;, index=0, na_rep=&quot;.&quot;) It’s worth noting that if the result table is a pivot table, there is a high probability of needing to output the index. In such cases, index=0 should be removed. 5. Other Possible Operations5.1 Column Type Conversion1df.astype(&#123;&#x27;col1&#x27;: &#x27;int32&#x27;&#125;)","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://silenwang.github.io/en/tags/Pandas/"}]},{"title":"Monster Hunter World: Iceborne Completion Diary","slug":"猛汉王狩猎日记-冰原开荒完结纪念","date":"2020-02-23T22:21:16.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2020/02/23/猛汉王狩猎日记-冰原开荒完结纪念/","permalink":"https://silenwang.github.io/en/2020/02/23/%E7%8C%9B%E6%B1%89%E7%8E%8B%E7%8B%A9%E7%8C%8E%E6%97%A5%E8%AE%B0-%E5%86%B0%E5%8E%9F%E5%BC%80%E8%8D%92%E5%AE%8C%E7%BB%93%E7%BA%AA%E5%BF%B5/","excerpt":"Originally planned to grind hard during the Spring Festival… but ended up working overtime most of the time, just finished the expansion now… And foreseeable busy work is coming… Taking advantage of not officially returning to work yet, leaving some memories here…","text":"Originally planned to grind hard during the Spring Festival… but ended up working overtime most of the time, just finished the expansion now… And foreseeable busy work is coming… Taking advantage of not officially returning to work yet, leaving some memories here…","categories":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/categories/Gaming/"}],"tags":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/tags/Gaming/"},{"name":"Monster Hunter","slug":"Monster-Hunter","permalink":"https://silenwang.github.io/en/tags/Monster-Hunter/"}]},{"title":"Cracked an APK","slug":"破解了一个apk","date":"2020-02-02T21:51:39.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2020/02/02/破解了一个apk/","permalink":"https://silenwang.github.io/en/2020/02/02/%E7%A0%B4%E8%A7%A3%E4%BA%86%E4%B8%80%E4%B8%AAapk/","excerpt":"From childhood to adulthood, my biggest motivation has always been playing games and looking cool… Today I wanted to play Monster Hunter 4G but couldn’t get back to Guangzhou, so I played it on my 5-year-old 3DS. Since it’s an old game that wasn’t very popular in the Chinese community at the time (Ping’s Dex was inconvenient due to system issues), it’s hard to find Chinese resources. To play the game better… I turned to an app that requires watching ads to earn points, which can then be used to exchange for specific equipment functions.","text":"From childhood to adulthood, my biggest motivation has always been playing games and looking cool… Today I wanted to play Monster Hunter 4G but couldn’t get back to Guangzhou, so I played it on my 5-year-old 3DS. Since it’s an old game that wasn’t very popular in the Chinese community at the time (Ping’s Dex was inconvenient due to system issues), it’s hard to find Chinese resources. To play the game better… I turned to an app that requires watching ads to earn points, which can then be used to exchange for specific equipment functions. This app’s monetization is based on watching ads to earn points, which are then used to exchange for specific equipment functions. My original goal was to make the points infinite or make the usage of exchanged functions unlimited. I thought there should be a variable in the program code that decreases each time a function is used. By removing this line of code, I could achieve unlimited usage. So let’s move on to the operation. Unpacking and Parsing APKActually, an APK file is just a compressed package. You can directly unzip it. The software package after extraction should include many parts, with the actual executable file being *.dex, which is the compiled execution file. To parse it, you need to convert this file into jar. I used enjarify, which comes from GitHub; you can clone it and use it. 12unzip your.apk/path/tool/enjarify.sh classes.dex Next, you need to view the Java code. I used jd-gui, which is available in the Arch User Repository (aur/jd-gui-bin). Drag the file into it. This is purely reading the code. Since the code wasn’t written by me and the author might have added some obfuscation to protect themselves, it may be difficult to locate. So I thought of using log tracking: First, use adb shell to connect to the phone terminal and run pm list packages to view the name of the target app. According to the name, capture logs for the specified app with adb logcat device_id|grep you.app.name. On the phone, operate this app, such as entering the paid page of the app, so you can see in the logs which class was called. After locating it, it’s quite tedious… Guess the logic of the code in the specified file (since I don’t understand Java), and then try to find a solution, De-compiling APK and Modifying CodeI used apktools, which you can install via sudo pacman -S android-apktool after adding the Arch Linux China source. After installation, use apktool d your.apk to decompile it. After decompilation, enter the folder generated by the file, where there will be a smali folder with the same directory structure as the Java project but in smali assembly language code instead of java. Although the languages are different (and some implementation logic is quite different), achieving the same thing, so according to the Java code I read earlier, you can find the corresponding part in the file and modify it accordingly using smali syntax. Re-packaging and SigningAfter modifying the code files, you can re-generate the APK file with the modified folder: apktool b -o your.mod.apk your_path. However, the newly packaged APK cannot be used directly. It needs to be signed… The signing process involves generating a key for information first, then signing it. I found this process too complicated and instead found a quick-signing project on GitHub called uber-apk-signer, which will use an internal test signature to quickly sign the unregistered APK file, allowing you to install it. 1java -jar uber-apk-signer-1.1.0.jar --apks your.mod.apk After that, test the modified app. If it doesn’t achieve the desired effect, you can modify it again. Actually, I didn’t achieve my initial goal because I couldn’t find where the value decreases in the code (since I really don’t understand Java), but I found a conditional check for whether points are sufficient when making purchases. So I directly bypassed this check and let the point exchange complete, achieving my goal in a different way… The only drawback is that… I still have to enter the purchase page and make many exchanges… Well, since doing it this way isn’t very good for the original author… So I won’t give the app name or the actual code. I wonder if this skill will be effective in strange places again…","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Apktools","slug":"Apktools","permalink":"https://silenwang.github.io/en/tags/Apktools/"},{"name":"crack","slug":"crack","permalink":"https://silenwang.github.io/en/tags/crack/"}]},{"title":"Extracting Outliers from ggplot Boxplots and Removing Them","slug":"从ggplot图形中提取数据","date":"2020-01-05T17:56:20.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2020/01/05/从ggplot图形中提取数据/","permalink":"https://silenwang.github.io/en/2020/01/05/%E4%BB%8Eggplot%E5%9B%BE%E5%BD%A2%E4%B8%AD%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE/","excerpt":"Today I encountered a plotting problem and learned a new small trick, so I’ll record it here.","text":"Today I encountered a plotting problem and learned a new small trick, so I’ll record it here. ggplot provides many commonly used plotting functions that are very convenient, and the plots can be drawn with various groupings. However, sometimes I have some special requirements that ggplot cannot satisfy, so I need to perform some operations manually. For example, today I needed to draw a grouped boxplot where each group has its own outliers. These outliers would make the plot crowded on one side, making it difficult to observe, so they need to be removed. Although geom_boxplot has parameters for outlier handling, unfortunately these parameters don’t actually remove the outlier points from the data - they just don’t display them on the plot. This means the plot remains crowded in the same way, failing to achieve the desired effect. The most common solution I found online is to use the $out content from the built-in boxplot function to extract outlier values and remove them. Unfortunately, this method doesn’t seem to provide grouped outlier results - it gives overall outliers instead. If you need to remove outliers by group, you would have to manually split the dataset and process each group individually. Is there a lazier way? I actually discovered one… Just like built-in functions, ggplot’s plots can extract the actual plotting data: 123plot &lt;- ggplot(data=data,mapping=aes(x=data$label, y=data[,tag])) + geom_boxplot()plot_data &lt;- layer_data(plot) The layer_data function is one way to extract the data (using build_plot(plot)$data has the same effect). plot_data stores all the data that ggplot uses for plotting. The part we need is stored in the outliers column, and it’s arranged in the same order as shown on the plot. Therefore, as long as we manually specify the factor levels for grouping during the initial plotting, we can get the outliers for specific groups directly from the plot. Then, utilizing ggplot’s automatic handling of NA values, we can replace the outlier values in the corresponding column with NA, creating a dataset without outliers. Then, we can redraw the plot to solve the problem! Complete plotting code and plot comparison: 12345678910111213141516171819202122232425library(&#x27;ggplot2&#x27;)data &lt;- read.csv(&quot;plot_data.tsv&quot;, sep=&quot;\\t&quot;, stringsAsFactors=F, na.strings = &#x27;.&#x27;)# Custom levels for grouped data; this must be done subsequentlydata$grp &lt;- factor(data$grp, levels=c(&quot;Grp1&quot;,&quot;Grp2&quot;))# Prepare a vector for mappingclassV &lt;- c(1,2)names(classV) &lt;- c(&quot;Grp1&quot;,&quot;Grp2&quot;)# First round: use ggplot to get outliersplot &lt;- ggplot(data=data,mapping=aes(x=data$grp, y=data[,&#x27;value&#x27;])) + geom_boxplot()print(layer_data(plot))outlier_data &lt;- layer_data(plot)[&#x27;outliers&#x27;]# Use the obtained values to change outlier values in the original dataset to NApdata&lt;-datafor (grp in names(classV)) &#123; pdata[pdata$grp==grp, &#x27;value&#x27;] &lt;- replace(pdata[pdata$grp==grp, &#x27;value&#x27;], pdata[pdata$grp==grp, &#x27;value&#x27;] %in% outlier_data[classV[grp], ][[1]], NA)&#125;# Prepare a comparison vector; use the changed data to plotcomb_list &lt;- list()comb &lt;- combn(c(&quot;Negative&quot;, &quot;Unkown&quot;,&quot;Weak&quot;, &quot;Medium&quot;,&quot;Strong&quot;),2)for (id in seq(dim(comb)[2])) &#123; comb_list[[id]] &lt;- comb[, id]&#125;pplot &lt;- ggplot(data=pdata,mapping=aes(x=pdata$grp, y=pdata[,&#x27;value&#x27;])) +geom_boxplot()","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"},{"name":"ggplot","slug":"ggplot","permalink":"https://silenwang.github.io/en/tags/ggplot/"}]},{"title":"Web Crawler Learning Notes","slug":"爬虫内容学习","date":"2019-12-18T17:44:21.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2019/12/18/爬虫内容学习/","permalink":"https://silenwang.github.io/en/2019/12/18/%E7%88%AC%E8%99%AB%E5%86%85%E5%AE%B9%E5%AD%A6%E4%B9%A0/","excerpt":"I have not systematically studied web scraping before, so I am making notes while watching this video.","text":"I have not systematically studied web scraping before, so I am making notes while watching this video. Request &#x2F; ResponseA request is information sent by a browser or program to the server to request content for display. The response is the server’s reply to the received request. Request content Request method: GET &#x2F; POST (most commonly used) POST has additional form data compared to GET Parameters in GET are directly included in the URL, while parameters in POST requests are included in the form Request URL: A Uniform Resource Locator (URL) is a link to a file or object Request headers: Important configuration information stored as key-value pairs Request body: Generally no information for GET, but required for POST Response content Response status code: A numeric code used to indicate the status of the request Response headers Response body: The result of the request Python requests module usagerequests is based on urllib3 and provides a more convenient and feature-rich interface GET Request RelatedAdding request parameters The params parameter can conveniently add parameters to the request, avoiding manual URL construction 12345data = &#123; &#x27;arg1&#x27;: &#x27;1&#x27;, &#x27;arg2&#x27;: &#x27;2&#x27;&#125;response = requests.get(&#x27;url&#x27;, params=data) JSON parsing Provides a json method that directly converts the returned JSON string into a JSON object 12response = requests.get(&#x27;url&#x27;, params=data)response.json() Binary data retrieval Use GET to request images directly, then write them to a file in ‘wb’ mode Adding headers Mainly for successful retrieval; some websites identify User-Agent to prevent machine scraping 1234headers = &#123; &#x27;User-Agent&#x27;: &#x27;XXXX&#x27;&#125;response = requests.get(&#x27;url&#x27;, headers=headers) POST Request RelatedAdding request parameters &#x2F; headers Similar to GET (see above) Response attributes Common attributes include: status_code headers cookies url history Status code analysis The requests library itself categorizes status codes, so you can quickly determine if the request was successful by calling built-in information. For example, response.status_code == requests.codes.ok is equivalent to 200 12response = requests.get(&#x27;url&#x27;)exit() if response.status_code != requests.codes.ok else print(&#x27;All Right&#x27;) Proxy settings123456proxy = &#123; &#x27;http&#x27;: &#x27;http://127.0.0.1:9743&#x27;, &#x27;https&#x27;: &#x27;https://user:passwd@127.0.0.1:9743&#x27;&#125;response = requests.get(&#x27;url&#x27;, proxies=proxy) If using ss, you need to install additional plugins. 1pip install &#x27;requests[socks]&#x27; 12345proxy = &#123; &#x27;socks5&#x27;: &#x27;http://127.0.0.1:9743&#x27;&#125;response = requests.get(&#x27;url&#x27;, proxies=proxy) Timeout settingsCan be combined with try for exception handling. 1response = requests.get(&#x27;url&#x27;, timeout=1) Selenium SectionFor content obtained through JavaScript and rendered on the page, some elements may not be found when analyzing page requests. At this time, Selenium can control the browser to perform operations. Although this is less efficient, it is very suitable for someone like me who is not familiar with web-related things. Element LocationTo simulate webpage operations, you first need to find the location of the element to be operated on. Selenium provides multiple ways to locate elements; using find_element will return the first matching element, while using find_elements will return a list of all objects. 12345678910browser = webdriver.Firefox()browser.get(&#x27;url&#x27;)browser.find_element_by_id(&#x27;123&#x27;)browser.find_element_by_name(&#x27;123&#x27;)browser.find_element_by_xpath(&#x27;123&#x27;)browser.find_element_by_link_text(&#x27;123&#x27;)browser.find_element_by_partial_link_text(&#x27;123&#x27;)browser.find_element_by_tag_name(&#x27;123&#x27;)browser.find_element_by_class_name(&#x27;123&#x27;)browser.find_element_by_css_selector(&#x27;123&#x27;) iframe LocationA webpage may be divided into several parts, with a large frame containing one or more iframes. When located within the main frame, it is impossible to search for or locate content within the small frames, and thus cannot perform corresponding operations. Therefore, you need to switch frames before performing operations. 1234567browser = webdriver.Firefox()browser.get(&#x27;url&#x27;)# Switch to iframe by namebrowser.switch_to.frame(&#x27;analyzeFrame&#x27;)# Switch back to main framebrowser.switch_to.default_content()# Untested if switching between iframes within an iframe is possible Pop-up Window OperationSometimes certain operations will pop up a warning box, requiring you to switch to this window for corresponding operations before continuing subsequent steps. 1234# Switch to the alert windowal = browser.switch_to_alert()# Click on the accept button in the windowal.accept() Browser Settings ChangeSometimes special settings need to be made to the browser to complete the required operation. For example, I once needed to obtain data by clicking a download button, but normally when clicking the download button, the browser would pop up a download window. 12345678options = Options()# The first item is the setting name, and the second is the corresponding value; available settings can be viewed by entering &#x27;about:config&#x27; in the browser.options.set_preference(&quot;browser.download.folderList&quot;, 2)browser = webdriver.Firefox(firefox_options=options)# The following sets up socks proxychrome_options = webdriver.ChromeOptions()chrome_options.add_argument(&#x27;--proxy-server=socks5://localhost:1080&#x27;)browser = webdriver.Chrome(chrome_options=chrome_options) Waiting SettingsAll Selenium operations are performed after the page has finished loading, but due to network issues, if you wait until it is fully loaded before executing, it significantly affects efficiency (Selenium is already less efficient than purely sending requests via code). Therefore, some settings can be made to execute related operations immediately when a specific condition is met. For example, below waits for a specific search button to become clickable before entering the search content and clicking the search. 123button = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, &#x27;search-bar-btn&#x27;)))browser.find_element_by_class_name(&#x27;search-input&#x27;).send_keys(tar)button.click() Download File CheckingSelenium itself cannot manage download files, so it can only be done through other means. For example: If the download link can be parsed out from the webpage code, you can call other tools to download. If the download link cannot be parsed out, you can use os module related content to monitor the downloaded file and rename it after downloading. I used the following two parts of code together at that time: 123456789101112131415161718import os# Copied code; this function checks the file size every 5 seconds. If the size has not changed since the last check, it is determined that the download is complete, and the browser object will be closed.def wait_download(file_path, browser, rep_name): current_size = getSize(file_path) print(&quot;&#123;&#125; Downloading&quot;.format(str(current_size))) while current_size != getSize(file_path) or getSize(file_path)==0: current_size =getSize(file_path) print(&quot;current_size:&quot;+str(current_size)) time.sleep(5)# wait download print(&quot;Downloaded&quot;) os.rename(file_path, rep_name) browser.close()# Because the download starts itself takes time, the target file appears after starting the above function.while not os.path.exists(file_path): print(&#x27;waiting for download to start...&#x27;) time.sleep(5)wait_download(file_path, browser, rep_name) About the Pitfall of Simulating ClicksSelenium can simulate clicks on buttons, links, checkboxes, etc., but the click must be performed when these elements are within the visible range; otherwise, the click will be ineffective or an exception will be thrown. Therefore, in actual operation, you need to test whether scrolling is required and how much to scroll to ensure that the element can be clicked. 123456# Switch out of iframebrowser.switch_to.default_content()# Scroll browser to bottombrowser.execute_script(&quot;window.scrollTo(0,document.body.scrollHeight)&quot;)# Switch back to iframebrowser.switch_to.frame(&#x27;analyzeFrame&#x27;) Miscellaneous Generally, the first GET request gets the framework of the webpage, and then new requests are sent to fill in the content needed. If you need to obtain corresponding information, you need to analyze AJAX requests. Selenium&#x2F;WebDriver Splash","categories":[{"name":"Script","slug":"Script","permalink":"https://silenwang.github.io/en/categories/Script/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"Web Crawler","slug":"Web-Crawler","permalink":"https://silenwang.github.io/en/tags/Web-Crawler/"},{"name":"requests","slug":"requests","permalink":"https://silenwang.github.io/en/tags/requests/"},{"name":"selenium","slug":"selenium","permalink":"https://silenwang.github.io/en/tags/selenium/"}]},{"title":"Some Magic Methods in Python","slug":"Python中的一些魔术方法","date":"2019-11-25T23:34:06.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2019/11/25/Python中的一些魔术方法/","permalink":"https://silenwang.github.io/en/2019/11/25/Python%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E9%AD%94%E6%9C%AF%E6%96%B9%E6%B3%95/","excerpt":"","text":"I previously used __enter__ and __exit__, which I had heard of before. This time, my program development is mainly object-oriented, so I needed to use more methods to achieve some useful features in Python. Here’s a small note. __len__After defining it, you can use the len() function on an instance. 12345def __len__(self): &#x27;&#x27;&#x27; Define length &#x27;&#x27;&#x27; return len(self.Seq) __getitem__After defining it, you can perform indexing and slicing operations on an instance using []. 12345def __getitem__(self, key): &#x27;&#x27;&#x27; Add slice index &#x27;&#x27;&#x27; return self.d[key] __str__After defining it, when you print an instance using print(), it will print the content returned by this method. 12def __str__(self): return str(self.__dict__) __iter__After defining it, you can iterate over an object, such as for i in object. Note that this method must return an iterator (not just an iterable). 12def __iter__(self): return (mut for mut in self.muts) __eq__ and __hash__These two are together because if you want to automatically remove duplicates when placing instances in a set(), both methods need to be defined. Otherwise, the purpose will not be achieved. The __eq__ method makes objects comparable (== and !=, is cannot be used). Note that the other parameter is fixed here; do not change it. 12345def __eq__(self, other): &#x27;&#x27;&#x27; Ensure automatic removal of duplicates using set characteristics &#x27;&#x27;&#x27; return self.muts == other.muts __hash__ then essentially makes objects hashable. The hash value used for hashing will be the one provided by this method. 12345def __hash__(self): &#x27;&#x27;&#x27; Ensure automatic removal of duplicates using set characteristics &#x27;&#x27;&#x27; return hash(self.muts) That’s it for now. I’ll continue to add more as needed.&#96;&#96;&#96;","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"}]},{"title":"Snakemake's Dynamic Parsing Feature","slug":"Snakemake的动态解析特性","date":"2019-11-24T23:00:31.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2019/11/24/Snakemake的动态解析特性/","permalink":"https://silenwang.github.io/en/2019/11/24/Snakemake%E7%9A%84%E5%8A%A8%E6%80%81%E8%A7%A3%E6%9E%90%E7%89%B9%E6%80%A7/","excerpt":"Damn, I finally successfully used this feature… I’m really stupid…","text":"Damn, I finally successfully used this feature… I’m really stupid… In actual workflows, our processes may have some “dynamic” requirements. For example, aligning fastq files or performing variant detection, the corresponding alignment&#x2F;detection software may not have a well-developed multi-threading mechanism. Additionally, original data files are too large, making direct computation very time-consuming. In such cases, using source file splitting -&gt; parallel processing across multiple processes -&gt; result file merging is a very practical strategy. How do you split them? You can split the files in a fixed way, such as splitting bam files by chromosome during variant detection. However, this fixed splitting pattern may not always be efficient. For example, different chromosomes have vastly different data volumes, and parallel processing by chromosome will inevitably be limited by the largest chromosome’s data volume. But if you don’t follow a fixed strategy for splitting, the number of generated split files may be unpredictable, which conflicts with snakemake‘s logic of parsing what to do before allocating resources. Therefore, developers introduced a dynamic parsing mechanism (Data-dependent conditional execution) in newer versions. The core of this dynamic mechanism is checkpoint, which is a special type of rule. While running, it behaves similarly to a regular rule, but when the workflow reaches a checkpoint, the entire dependency graph (DAG) will be re-parsed. The designer’s intention was clear: set rules that produce unpredictable results as checkpoints, and then re-decide the necessary steps based on the actually generated files. Due to this special dynamic mechanism, using checkpoints is also more troublesome than regular rules. Checkpoints must be used in conjunction with function input, because although a new mechanism has been added, the program’s execution logic still relies on deciding what to do first. And deciding what to do cannot be determined by the program itself; it requires manual design, so we need to use a function to get the files generated by the checkpoint and then pass these files to downstream rules for parsing. I have an example that is slightly easier to understand than the official one: 123456789101112131415161718192021222324252627282930313233343536373839404142from os import pathfrom glob import globrule all: input: &quot;&#123;sample&#125;.all.stat&quot;.format(sample=&quot;NL180614-2_S12_L001_R1_001&quot;)checkpoint split: input: &quot;&#123;sample&#125;.fastq.gz&quot; output: touch(&quot;&#123;sample&#125;.splt.done&quot;) shell: &quot;split -l 1000000 &#123;input&#125; &#123;wildcards.sample&#125;.split. &quot;rule wc: input: &quot;&#123;sample&#125;.split.&#123;split&#125;&quot; output: &quot;&#123;sample&#125;.split.&#123;split&#125;.stat&quot; shell: &quot;wc -l &#123;input&#125; &gt; &#123;output&#125;&quot;def get_split_files(wildcards): split_output = checkpoints.split.get(**wildcards).output[0] split_dir = path.split(split_output)[0] split_files = glob(f&quot;*.split.*&quot;) tar_files = [f&quot;&#123;f&#125;.stat&quot; for f in split_files] return tar_filesrule cat: input: get_split_files output: &quot;&#123;sample&#125;.all.stat&quot; shell: &#x27;&#x27;&#x27; cat &#123;input&#125; &gt; &#123;output&#125; &#x27;&#x27;&#x27; This example implements splitting fastq files into multiple sub-files, counting the number of lines for each split file, and then merging the results into a single file. The checkpoint part is responsible for splitting, while this rule generates an output that does not depend on downstream rules. The truly useful part is the checkpoint itself. The get_split_files() function uses the checkpoints object and accesses the content of the split checkpoint, so it uses this function as input for the cat rule, which has a dependency on the checkpoint split. At the same time, it does not perform actual dependency parsing but waits for its checkpoint split to complete before parsing (i.e., dynamic parsing). The official documentation example is basically the same principle, although its checkpoint generates directory results to facilitate re-parsing of dynamically generated files. Finally, I’ll add another BWA example: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from os import pathfrom glob import globwildcard_constraints: sample=r&quot;[A-Z,a-z,0-9,-]+&quot;rule all: input: &quot;&#123;sample&#125;.srt.bam&quot;.format(sample=&quot;NL180614-2&quot;)checkpoint split: input: fq1=&quot;&#123;sample&#125;_S12_L001_R1_001.fastq.gz&quot;, fq2=&quot;&#123;sample&#125;_S12_L001_R2_001.fastq.gz&quot;, output: directory(&quot;&#123;sample&#125;_tmp&quot;) shell: &#x27;&#x27;&#x27; mkdir -p &#123;output&#125;/R1 mkdir -p &#123;output&#125;/R2 zcat &#123;input.fq1&#125; | split -l 1000000 /dev/stdin &#123;output&#125;/R1/split. zcat &#123;input.fq2&#125; | split -l 1000000 /dev/stdin &#123;output&#125;/R2/split. &#x27;&#x27;&#x27;rule bwa: input: r1=&quot;&#123;sample&#125;_tmp/R1/split.&#123;split&#125;&quot;, r2=&quot;&#123;sample&#125;_tmp/R2/split.&#123;split&#125;&quot;, output: &quot;&#123;sample&#125;_tmp/split.&#123;split&#125;.srt.bam&quot; shell: &#x27;&#x27;&#x27; bwa mem -t 4 -M \\\\ -R &quot;@RG\\\\tID:&#123;wildcards.sample&#125;\\\\tPL:Illumina\\\\tPU:&#123;wildcards.sample&#125;\\\\tSM:&#123;wildcards.sample&#125;&quot; \\\\ /home/silen/git/Bio_Test/hg38-bwa/hg38.fa \\\\ &#123;input.r1&#125; \\\\ &#123;input.r2&#125; \\\\ | samtools view -Sb -@ 4 \\\\ | samtools sort -@ 4 \\\\ &gt; &#123;output&#125; &#x27;&#x27;&#x27;def get_split_files(wildcards): split_dir = checkpoints.split.get(**wildcards).output[0] split_files_r1 = glob(f&quot;&#123;split_dir&#125;/R1/split.*&quot;) split_tag = [s.split(&quot;.&quot;)[-1] for s in split_files_r1] split_bams = [path.join(split_dir,f&quot;split.&#123;tag&#125;.srt.bam&quot;) for tag in split_tag] return split_bamsrule cat: input: get_split_files output: &quot;&#123;sample&#125;.srt.bam&quot; shell: &#x27;&#x27;&#x27; samtools merge -f &#123;output&#125; &#123;input&#125; &#x27;&#x27;&#x27; I have been using this feature in some of my projects, and the actual experience is that it seems simple and general-purpose, but there are still different issues to consider depending on the software situation… For example, in the above BWA example, there is still one problem to solve: here, the generated directories are actually temporary directories, and I hope to delete them after use. However, temp and directory keywords conflict, and the files in the temporary folder are dynamic and cannot be specified directly, so it’s currently not possible to utilize snakemake’s own features to automatically delete them.&#96;&#96;&#96;","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"snakemake","slug":"snakemake","permalink":"https://silenwang.github.io/en/tags/snakemake/"}]},{"title":"Let Custom Objects Use the 'with' Statement","slug":"让自写的对象可以用上with","date":"2019-11-14T18:56:24.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2019/11/14/让自写的对象可以用上with/","permalink":"https://silenwang.github.io/en/2019/11/14/%E8%AE%A9%E8%87%AA%E5%86%99%E7%9A%84%E5%AF%B9%E8%B1%A1%E5%8F%AF%E4%BB%A5%E7%94%A8%E4%B8%8Awith/","excerpt":"The with open statement is a particularly convenient feature in Python, allowing for automatic closure of objects when operations are completed. For custom objects, it’s also possible to support the with syntax.","text":"The with open statement is a particularly convenient feature in Python, allowing for automatic closure of objects when operations are completed. For custom objects, it’s also possible to support the with syntax. To achieve this, define the __enter__ and __exit__ methods in your class: 1234567class MyClass(object): def __init__(self): pass def __enter__(self): print(&quot;Start&quot;) def __exit__(self, exc_type, exc_val, exc_tb): print(&quot;End&quot;) With this setup, when using the with statement, it will first execute the content within __enter__, and automatically execute the content within __exit__ at the end of the code block. 123456&gt;&gt;&gt; with MyClass() as tClass:... print(&quot;test&quot;)...StarttestEnd","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"}]},{"title":"The Chronicles of Monster Hunter - Tempered Bazelgeuse","slug":"猛汉王狩猎日记-历战爆鳞龙","date":"2019-10-30T22:47:37.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2019/10/30/猛汉王狩猎日记-历战爆鳞龙/","permalink":"https://silenwang.github.io/en/2019/10/30/%E7%8C%9B%E6%B1%89%E7%8E%8B%E7%8B%A9%E7%8C%8E%E6%97%A5%E8%AE%B0-%E5%8E%86%E6%88%98%E7%88%86%E9%B3%9E%E9%BE%99/","excerpt":"The romance of a hunter is not just about explosions… but also about damage output! Obsessed with hunting… neglecting my career…","text":"The romance of a hunter is not just about explosions… but also about damage output! Obsessed with hunting… neglecting my career…","categories":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/categories/Gaming/"}],"tags":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/tags/Gaming/"},{"name":"Monster Hunter","slug":"Monster-Hunter","permalink":"https://silenwang.github.io/en/tags/Monster-Hunter/"}]},{"title":"Monster Hunter Diary - Cherry Fire Dragon","slug":"猛汉王狩猎日记-樱火龙","date":"2019-10-06T12:11:48.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2019/10/06/猛汉王狩猎日记-樱火龙/","permalink":"https://silenwang.github.io/en/2019/10/06/%E7%8C%9B%E6%B1%89%E7%8E%8B%E7%8B%A9%E7%8C%8E%E6%97%A5%E8%AE%B0-%E6%A8%B1%E7%81%AB%E9%BE%99/","excerpt":"As expected, just like I bought a tablet and spent two days reading books before starting to play PSP emulators… this time after buying a computer, I didn’t use it for software testing for long either… I immediately started playing Monster Hunter. I even fiddled with how to record video from the client machine on the host… After struggling all day, I finally chose dual-streaming of video and audio… separately through the Looking-Glass and Scream projects to stream video and audio back to the host, and then recorded using OBS on the host. As for why I didn’t record directly on the client machine… that wouldn’t reflect the essence of being a masochist, right! Here’s a test video of me playing Cherry Fire Dragon without any technical skill:","text":"As expected, just like I bought a tablet and spent two days reading books before starting to play PSP emulators… this time after buying a computer, I didn’t use it for software testing for long either… I immediately started playing Monster Hunter. I even fiddled with how to record video from the client machine on the host… After struggling all day, I finally chose dual-streaming of video and audio… separately through the Looking-Glass and Scream projects to stream video and audio back to the host, and then recorded using OBS on the host. As for why I didn’t record directly on the client machine… that wouldn’t reflect the essence of being a masochist, right! Here’s a test video of me playing Cherry Fire Dragon without any technical skill:","categories":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/categories/Gaming/"}],"tags":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/tags/Gaming/"},{"name":"Monster Hunter","slug":"Monster-Hunter","permalink":"https://silenwang.github.io/en/tags/Monster-Hunter/"}]},{"title":"The Great Ace Attorney, Clearing the Case \\(￣▽￣)/","slug":"大逆转裁判","date":"2019-10-05T18:49:39.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2019/10/05/大逆转裁判/","permalink":"https://silenwang.github.io/en/2019/10/05/%E5%A4%A7%E9%80%86%E8%BD%AC%E8%A3%81%E5%88%A4/","excerpt":"I have cleared the case in “The Great Ace Attorney”. Personally, I feel that this game is even better than the Phoenix Wright trilogy. Compared to them, this game does not have any supernatural elements, and the part about evidence in the story seems more reasonable (not just moving an item&#x2F;person out can be used as evidence). Of course, in a game centered on reasoning… I might be overthinking something~","text":"I have cleared the case in “The Great Ace Attorney”. Personally, I feel that this game is even better than the Phoenix Wright trilogy. Compared to them, this game does not have any supernatural elements, and the part about evidence in the story seems more reasonable (not just moving an item&#x2F;person out can be used as evidence). Of course, in a game centered on reasoning… I might be overthinking something~","categories":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/categories/Gaming/"}],"tags":[{"name":"Playing Games","slug":"Playing-Games","permalink":"https://silenwang.github.io/en/tags/Playing-Games/"}]},{"title":"Writing Dockerfile to Build Docker Image","slug":"编写Dockerfile构建docker镜像","date":"2019-10-05T16:58:54.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2019/10/05/编写Dockerfile构建docker镜像/","permalink":"https://silenwang.github.io/en/2019/10/05/%E7%BC%96%E5%86%99Dockerfile%E6%9E%84%E5%BB%BAdocker%E9%95%9C%E5%83%8F/","excerpt":"From past experiences… I have gained many skills as a byproduct… Today, this is also the case. My original intention was to use the source code provided by NetEase to compile an improved version of VirtualBox, but in the end… I learned several Docker-related skills…","text":"From past experiences… I have gained many skills as a byproduct… Today, this is also the case. My original intention was to use the source code provided by NetEase to compile an improved version of VirtualBox, but in the end… I learned several Docker-related skills… The construction of images is basic to Docker. Although it is possible to manually build images after creating the base image each time, or directly pull them from platforms like Docker Hub, the former is time-consuming and lacks repeatability, while the latter has certain network requirements and limited customizability. Therefore, it is still essential to master the use of Dockerfile, which serves as the blueprint for building images. In fact… this thing is quite simple. The keywords I might use most frequently in a Dockerfile are: FROM: Specifies the base image for building. Before starting the build, it will pull the specified image as the base. COPY: Copies files from the external (host) to the image’s internal path. The first parameter is the external path, and the second parameter is the internal path. The usage is similar to cp, but I haven’t tried using wildcards (not sure if they work). ADD: Similar to COPY, but ADD can fetch files from the network, which is very useful when deploying containers elsewhere. RUN: Commands that need to be executed. WORKDIR: The directory where the built image will start when it runs. CMD: The command that the built image will run by default when it starts. Here are some special notes about the COPY and RUN commands: When using COPY, you can only fetch files from the build path specified by docker build and its subdirectories. This is due to the context restriction during Docker runtime. Each time the RUN command runs, it generates a new layer in the image. If not necessary, try to minimize the number of commands and combine them with &amp;&amp; in a single RUN command to prevent the image from becoming large and slow. Here is an example of a Dockerfile I used for deploying an R package sscClust: 1234567891011121314151617181920212223242526272829303132## sscClust Dockerfile## https://github.com/dockerfile/fpm## Pull base image.FROM centos:7# Install dep-packagesRUN yum install -y epel-release wget \\ &amp;&amp; mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak \\ &amp;&amp; mv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.bak \\ &amp;&amp; wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo \\ &amp;&amp; wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo \\ &amp;&amp; yum clean all \\ &amp;&amp; yum makecache -y \\ &amp;&amp; yum install -y R openssl-devel libcurl-devel libxml2-devel gsl-devel \\ &amp;&amp; mkdir -p /install_script# Install R packagesCOPY install.r /install_script/install.rRUN Rscript /install_script/install.r# Define working directory.WORKDIR /# Define default command.CMD /bin/bash The corresponding R script is: 12345options(&quot;repos&quot; = c(CRAN=&quot;https://mirrors.ustc.edu.cn/CRAN/&quot;))options(BioC_mirror=&quot;http://mirrors.ustc.edu.cn/bioc/&quot;)install.packages(c(&quot;BiocManager&quot;, &quot;devtools&quot;))BiocManager::install(c(&#x27;SingleCellExperiment&#x27;, &#x27;scran&#x27;, &#x27;SC3&#x27;, &#x27;zinbwave&#x27;, &#x27;BiocParallel&#x27;))devtools::install_github(&quot;Japrin/sscClust&quot;) Execute docker build -t=test_docker . to build the image using the above file. Of course, this example may no longer be able to build successfully because software is constantly updating, and my build process does not specify any specific version of software. As time progresses, various dependency issues may arise… However, as a usage record, it’s still quite useful.","categories":[{"name":"Script","slug":"Script","permalink":"https://silenwang.github.io/en/categories/Script/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://silenwang.github.io/en/tags/docker/"},{"name":"dockerfile","slug":"dockerfile","permalink":"https://silenwang.github.io/en/tags/dockerfile/"}]},{"title":"Adding Custom Services in Linux Distributions","slug":"在Linux系统中添加自定义服务","date":"2019-10-05T16:00:31.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2019/10/05/在Linux系统中添加自定义服务/","permalink":"https://silenwang.github.io/en/2019/10/05/%E5%9C%A8Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9C%8D%E5%8A%A1/","excerpt":"When using a Linux system, it’s often necessary to set up things that start automatically at boot. For distributions that use systemd, writing your own service files and enabling them is a great choice.","text":"When using a Linux system, it’s often necessary to set up things that start automatically at boot. For distributions that use systemd, writing your own service files and enabling them is a great choice. Service files are written in a configuration file format similar to ini. A basic service configuration file includes the following content: Unit: The defined section, such as what the service does Service: The actual execution content, including what type of service it is, what to do when starting, what to do when stopping, and what to do when restarting Install: It’s mostly about dependencies, such as when the service should start or before or after XX starts Here is an example of a v2ray service: 12345678910[Unit]Description=Daemon to start V2ray[Service]Type=simpleExecStart=/usr/bin/v2ray -config /home/silen/script/v2ray/v2ray.json[Install]Alias=v2rayWantedBy=default.target If this service is enabled, it will start automatically by default(WantedBy=default.target), executing the command/usr/bin/v2ray -config /home/silen/script/v2ray/v2ray.json, when booting, which means activating the v2ray proxy. Other actions for the service, such as restarting and stopping, are not defined. After writing the above file, you can name the filev2ray.serviceand place it in the specified location (/etc/systemd/system or /usr/lib/systemd/system), then use systemctl enable v2ray.servicewith root privileges to activate the service (it will start according to the configuration after boot), and use systemctl start v2ray.service to start the service immediately. It should be noted that when calling root privileges to start the service, the executing user is naturally root, which is not always a good choice. Therefore, my previous synchronization service and this v2ray service are actually both started with personal user permissions. In other words, put the above service file in /home/&lt;user&gt;/.config/systemd/user, and then directly systemctl start --uesr v2ray.service to start the service. In this case, the service will only start after my current user logs in, and there is no need for root privileges (whether the user needs to be in the root group or not, I am not sure). The v2ray program used in the example belongs to the type that occupies the foreground, so it can be directly used with the sample and oneshot type. In addition to this type, there are: simple, forking, oneshot, notify, dbus, idle. Of course, my custom services are generally quite simple, simple and oneshot should be enough. As for ExecStart, ExecStop, and ExecReload, the former is required, while the latter two can be written or not depending on the start command situation. Finally, WantedBy=default.target in Install is a must according to my test, because the service will not start automatically without it… It’s not clear whether this is only for services at the user level.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"systemd","slug":"systemd","permalink":"https://silenwang.github.io/en/tags/systemd/"}]},{"title":"Desktop Setup Plan (Part 2)","slug":"台式机置备计划-2","date":"2019-09-08T12:35:13.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2019/09/08/台式机置备计划-2/","permalink":"https://silenwang.github.io/en/2019/09/08/%E5%8F%B0%E5%BC%8F%E6%9C%BA%E7%BD%AE%E5%A4%87%E8%AE%A1%E5%88%92-2/","excerpt":"After putting in a lot of effort to get the hardware sorted out, it’s time for the software.","text":"After putting in a lot of effort to get the hardware sorted out, it’s time for the software. Following my previous plan, I wanted my machine to be multi-functional: the host Linux system for computation and a virtual Windows system for gaming. Anyone who has used virtual machines knows… the performance is always an issue. Even with top-tier products like VMware and Mac-specific Parallel, their graphical performance still falls short of native hardware. That’s why PCI Passthrough technology came into play. It allows PCI slot devices to be isolated from the host and directly allocated to the virtual machine, achieving almost-native performance without any middlemen. This is also the reason I bought a motherboard with two PCI slots; if I ever wanted to run two gaming PCs in virtual machines, each would need its own dedicated graphics card, and the host needs an integrated graphics card as well. This virtualization setup uses Manjaro as the host system. References for this configuration include: ArchWiki GPU Passthrough Tutorial Linus Tech Tips Video on Virtual macOS INVALID POST SLUG PROVIDED Since this configuration is based on an Arch-based distribution, the specific commands and components used will differ from materials found in Chinese websites (which are mostly based on Ubuntu). Steps1. Prepare the System Disk Host System: manjaro-architect-18.0.4-stable, a derivative of Arch Linux known for its simplicity and ease of use. Despite being considered heretical by the Arch community, it has gained significant popularity (even I saw Manjaro’s community open a company similar to Ubuntu). Using Architect instead of a pre-installed desktop live CD is mainly because the image is small, with all software packages downloaded over the network. Subsystem: Different from the host system, the subsystem needs to prepare an installation disk for Windows and possibly an EFI-bootable ISO image. The ArchWiki tutorial uses OVMF (which simplifies PCI passthrough by reducing the need for configuring virtual BIOS), which means the virtual machine will use EFI (only) instead of BIOS. This requires the installation system’s ISO image to support EFI boot. However, I downloaded many installation images that don’t support it… so I had to find a PE image that supports UEFI and used this image to boot and install another installation image on top. 2. Install the Host SystemInstall Manjaro using the host system disk on your computer. Detailed steps can be found elsewhere, so I’ll skip them for now (I’ll add screenshots later when I have time). 3. Set Up GPU IsolationTo achieve PCI passthrough, we need to prevent the host from accessing the hardware. The specific settings are as follows: Enable IOMMU Edit the configuration file (/etc/default/grub) to change kernel parameters and enable IOMMU: 1234...GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet udev.log_priority=3 audit=0 amd_iommu=on iommu=pt&quot;GRUB_CMDLINE_LINUX=&quot;&quot;... After making the above changes, regenerate grub.cfg: 1sudo grub-mkconfig -o /boot/grub/grub.cfg Edit the configuration file to use vcfio to block the PCI devices to be isolated Use the lspci -nnk command to view all PCI hardware information, find the device IDs of the hardware you want to passthrough, and record them: 1234567801:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Baffin [Radeon RX 460/560D / Pro 450/455/460/555/555X/560/560X] [1002:67ef] (rev e5) Subsystem: Tul Corporation / PowerColor Baffin [Radeon RX 460/560D / Pro 450/455/460/555/555X/560/560X] [148c:2385] Kernel driver in use: vfio-pci Kernel modules: amdgpu01:00.1 Audio device [0403]: Advanced Micro Devices, Inc. [AMD/ATI] Baffin HDMI/DP Audio [Radeon RX 550 640SP / RX 560/560X] [1002:aae0] Subsystem: Tul Corporation / PowerColor Baffin HDMI/DP Audio [Radeon RX 550 640SP / RX 560/560X] [148c:aae0] Kernel driver in use: vfio-pci Kernel modules: snd_hda_intel The above information shows that the drivers for both devices are now vfio-pci, indicating successful isolation. Record the IDs in brackets, which are [148c:2385] and [1002:aae0] for the display device and audio device, respectively. Create /etc/modprobe.d/vfio.conf and add options vfio-pci ids=&lt;id to block&gt; in it. Edit /etc/mkinitcpio.conf, find the relevant project and add the corresponding content: 12345...MODULES=(... vfio_pci vfio vfio_iommu_type1 vfio_virqfd ...)...HOOKS=(... modconf ...)... After editing, changes won’t take effect until you regenerate initramfs: Note: I don’t fully understand what this is about, and the ArchWiki content is also unclear to me. Here’s the actual operation I performed, which may not be the most correct method. Execute the command to regenerate /boot‘s boot image: sudo mkinitcpio -g /boot/initramfs-4.19-x86_64.img(it’s recommended to back up the original) After overwriting, restart the host system. If the settings are successful, the corresponding device drivers should be vfio-pci. After overwriting, the host system cannot access the isolated hardware; if you need access, select initramfs-4.19-x86_64-fallback.img to start from the unmodified version. 4. Install and Configure Virtual MachineAfter setting up the hardware, it’s time for software configuration. Install virtual machine management software tip: Here I installed QEMU and VirtManager, which simplify QEMU settings, along with libvirt. KVM is a module in the Linux kernel that can be used as long as virtualization is enabled on the motherboard; no additional installation is required. 1sudo pacman -S qemu libvirt virt-manager ovmf Create a virtual machine Open VirtManager, click “Create a new VM” Select the installation image (not for booting), since the downloaded image is not official, it cannot automatically recognize the system type. Remove automatic detection and manually select Win10 then proceed to the next step CPU and memory allocation Choose to create a new image (skipping detailed steps…) Confirm information, select network as bridge, and further edit Allocate the PCI devices needed for passthrough Since my installation image does not support UEFI boot, I found another one that supports but lacks installation files. I need to make some small adjustments to the boot order to ensure it mounts and starts from the bootable image. Edit CPU information, mainly check “Copy host CPU configuration” (equivalent to host-model) Set the BIOS to OVMF (refer to ArchWiki for details; I forgot to take a screenshot…) Click the top-left “Start installation” and proceed with the normal system installation. 5. Audio Device Bug Fix and Other IssuesAfter completing the above steps and installing the system, a physical GPU virtual machine is set up, and connecting the display interface of the graphics card to any display device allows you to enjoy gaming happily. However, there’s still an issue: although the graphics card has built-in audio devices, theoretically, one HDMI cable should be able to transmit audio to my monitor’s speakers. But in fact, no audio devices from the graphics card are found in the virtual machine. This problem puzzled me for a week until I saw a similar issue on the Proxmox forum, which said this was a bug in Q35 4.0 and could be resolved by downgrading to version 3.1 in the virtual machine configuration file. Therefore, we need to manually edit the virtual machine’s XML configuration file (open it directly for editing), find the relevant entry, and change it to the following configuration: 123...&lt;type arch=&quot;x86_64&quot; machine=&quot;pc-q35-3.1&quot;&gt;hvm&lt;/type&gt;... Finally… I can sit down and enjoy gaming properly~~~~~ Postscript: A 9.8 post that started writing was pigeonholed until almost 10.8…… The charm of猛汉王 is truly irresistible….&#96;&#96;&#96;&#96;","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"GPU Passthrough","slug":"GPU-Passthrough","permalink":"https://silenwang.github.io/en/tags/GPU-Passthrough/"},{"name":"Manjaro","slug":"Manjaro","permalink":"https://silenwang.github.io/en/tags/Manjaro/"},{"name":"kvm","slug":"kvm","permalink":"https://silenwang.github.io/en/tags/kvm/"}]},{"title":"Desktop Setup Plan (Part 1)","slug":"台式机置备计划_1","date":"2019-09-02T22:24:41.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2019/09/02/台式机置备计划_1/","permalink":"https://silenwang.github.io/en/2019/09/02/%E5%8F%B0%E5%BC%8F%E6%9C%BA%E7%BD%AE%E5%A4%87%E8%AE%A1%E5%88%92_1/","excerpt":"At the beginning of the year, I saved 2000 yuan to buy a second-hand server for testing software and playing games. However, after looking around, I found that servers were too troublesome… they consume a lot of electricity and make noise. If you want to add a graphics card, you need to modify the case… since I have no experience with hardware… so I decided to buy a desktop computer that is both affordable and sufficient for my needs. Originally, I was going to buy something on 618… but due to work changes, I didn’t have time or energy to mess around, so I gave up… recently, I saw a motherboard on JD at a discount, so I couldn’t resist buying it… but when I checked, I found that the CPU I wanted was hard to find new… (probably sold out during 618)… after several weeks of effort, I finally got everything and assembled my first self-built desktop computer!","text":"At the beginning of the year, I saved 2000 yuan to buy a second-hand server for testing software and playing games. However, after looking around, I found that servers were too troublesome… they consume a lot of electricity and make noise. If you want to add a graphics card, you need to modify the case… since I have no experience with hardware… so I decided to buy a desktop computer that is both affordable and sufficient for my needs. Originally, I was going to buy something on 618… but due to work changes, I didn’t have time or energy to mess around, so I gave up… recently, I saw a motherboard on JD at a discount, so I couldn’t resist buying it… but when I checked, I found that the CPU I wanted was hard to find new… (probably sold out during 618)… after several weeks of effort, I finally got everything and assembled my first self-built desktop computer! Component ListMy configuration idea was a bit special because I had specific expectations for this machine. I hoped it could meet all the following requirements: Personal bioinformatics computation testing (multi-core + large memory + Linux) Basic gaming needs on Windows Virtualize into multiple machines with my girlfriend playing games together (two entry-level graphics cards + Linux + GPU passthrough + Windows virtual machines) A small and beautiful case Based on these requirements, I chose a motherboard with 4 memory slots and 2 graphics card slots for M-ATX to support future upgrades to 32G or even 64G memory (some research institutions’ prototype software has huge memory consumption), and also to support dual GPUs for virtualizing two Windows machines. The CPU was difficult because the host needed an integrated graphics card, and I wanted AMD… so there wasn’t much choice, and the only model available at the moment was the 2400G… Complete component list as follows: Component Model Platform Price Notes Processor AMD Ryzen 2400G Taobao 760 Second-hand散片 Graphics Card Dianlan Hengjin RX560d 4G Taobao 245 Miner card Motherboard Gigabyte B450M DS3H JD 499 Bought at a discount of 50… Memory Kingston Firebird DDR4 3000 8GB x 2 JD 498 Case Thermaltake Challenger S3 JD 129 Power Supply Thermaltake Smart RGB 600W JD 278 Didn’t see… bought a non-modular… Cooler Thermaltake Rainbow D400P JD 85 Didn’t see… 65W CPU with a 150W cooler… Total 2494 Pretty cheap… Buying desktop components is really more complicated than buying laptops… I encountered: The CPU was out of stock, so I had to buy second-hand散片, but the second-hand散片 were also hard to find, and after looking at three places, I finally bought it, and even got 30 yuan in compensation for the appeal (out of stock but not taken off the shelf)… Although I knew it was a miner card… but I didn’t notice that the one I bought had only DVI interfaces… so I had to buy an adapter later Although the motherboard has two graphics card slots, you can’t directly plug in two GPUs because the lower graphics card slot would block the PIN lines and USB3.0 cables, needing additional adapters The power supply I wanted was out of stock, so when buying a replacement, I didn’t notice that it was a non-modular power supply… subsequent cable management was a headache The cooler size exceeded my expectations… fortunately, the case I bought was relatively large… The case turned out to be much larger than expected… and ended up being a hit with the cooler… Buying components is already complicated enough… subsequent installation issues were one after another: Installed the cooler fan backwards, which made it difficult to take off later… The non-modular power supply cable management was a headache Installed the hard drive bracket backwards, almost couldn’t pull out Didn’t notice the instructions in the wind cooler manual, spent two and a half hours installing After nearly two weeks of fiddling around, I finally got everything set up… as a buffer, I first installed Windows 10 to test playing Madoka Magica. It was quite surprising that the 2400G could run at 720p low quality smoothly… and since I was using my landlord’s TV… there wasn’t much difference between 720p and 1080p on this TV… which means if I just bought a machine to play Madoka Magica, cutting down 8GB of memory (250), buying a 2200G plus A320M set (1000元左右), using a 100 yuan power supply that came with the case, or even 1500 yuan could play Madoka Magica… so dependent… Madoka Magica is really affordable… This is the hardware part of my desktop setup plan. I’ll update the software part later~","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"Daily","slug":"Daily","permalink":"https://silenwang.github.io/en/tags/Daily/"}]},{"title":"Using Syncthing for File Synchronization","slug":"使用Syncthing进行文件同步","date":"2019-08-18T00:03:33.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2019/08/18/使用Syncthing进行文件同步/","permalink":"https://silenwang.github.io/en/2019/08/18/%E4%BD%BF%E7%94%A8Syncthing%E8%BF%9B%E8%A1%8C%E6%96%87%E4%BB%B6%E5%90%8C%E6%AD%A5/","excerpt":"As long as you use multiple devices for work, you will inevitably encounter the problem of mutual synchronization between different devices. Now that it’s not the era when cloud services just started to rise and were cheap and generous, there probably isn’t a reliable and user-friendly service provider that won’t suddenly run away or change its terms of service. So, we still have to rely on ourselves to set up our own…","text":"As long as you use multiple devices for work, you will inevitably encounter the problem of mutual synchronization between different devices. Now that it’s not the era when cloud services just started to rise and were cheap and generous, there probably isn’t a reliable and user-friendly service provider that won’t suddenly run away or change its terms of service. So, we still have to rely on ourselves to set up our own… Syncthing is introduced in Little Known Software and Anycard Software, a simple yet open-source synchronization software. Since the official version already provides pre-packaged executable files, Syncthing can be run directly on three major platforms without needing to download any additional packages. Therefore, I won’t repeat that part here. This post mainly records how to set up Syncthing as a service on my computer and VPS. Both CentOS7 and Manjaro use systemd, and the command for managing services is systemctl, so the operations are the same. First, find the Syncthing executable file package. Inside the Syncthing/etc/ directory, there are sample service configuration files. I used the Syncthing/etc/linux-systemd/user/syncthing.service file. Open this file and change the program path in the [Service] section of ExecStart. Since I am setting up this service as a user-level service rather than a system-wide service, I copied this file to ~/.config/systemd/user/, then started the service using the following commands: 12systemctl --user enable syncthing.servicesystemctl --user start syncthing.service After starting, you can use systemctl like other services to check the status of the Syncthing service. &#96;&#96;&#96;","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"Syncthing","slug":"Syncthing","permalink":"https://silenwang.github.io/en/tags/Syncthing/"}]},{"title":"Git Hooks Usage","slug":"Git钩子使用","date":"2019-08-17T19:33:27.000Z","updated":"2026-02-10T14:03:13.683Z","comments":true,"path":"2019/08/17/Git钩子使用/","permalink":"https://silenwang.github.io/en/2019/08/17/Git%E9%92%A9%E5%AD%90%E4%BD%BF%E7%94%A8/","excerpt":"Although I also use Git for some project code management, I actually don’t know many of Git’s features and haven’t used them. Today, I tried using Git hooks.","text":"Although I also use Git for some project code management, I actually don’t know many of Git’s features and haven’t used them. Today, I tried using Git hooks. Hook, directly translated as a hook, refers to a series of predefined scripts. These scripts are automatically executed when we use specific Git commands to perform some code management operations, with the purpose of reducing repetitive operations and improving efficiency. For example, my blog source code is managed by Git. The steps I take to write a blog post and publish it are: hexo new post creates a markdown draft for a new blog post. Open the newly created markdown file and write content. After writing, run hexo g and hexo d to generate and push the blog content to the Gitpages source. Run git commit to submit changes to the blog source code, then git push to the source repository. Among these steps, there may be some variable operations during the writing process, but the last two steps are always repeated and not very changeable each time I write a blog post. These steps can be executed using hooks. First, we need to determine when these things should be executed: My blog source code is checked before commit. Therefore, theoretically, any hook triggered by git commit can be used. The actual selected hook is post-commit, which executes after commit, so I wrote the following content into the .git/hooks/post-commit file of the project (the name should not be changed; specific names are required to be executed). 1234#!/bin/bashcd /home/silen/git_proj/silen_blog/hexo ghexo d Then, give this file executable permissions. This way, every time I complete a commit for this project, the script will automatically generate and push the blog. Of course, the example here may not be very attractive because these two steps are actually not many… saving a lot of effort. But now, in addition to my main site, I have set up a personal quick reference manual on a sub-site using mkdocs, which is another independent project. If I want to achieve that one update synchronizes the other, hooks would be very useful. Finally, it’s worth noting that Git hooks are divided into client-side and server-side. I am currently using a client-side hook. One drawback of client-side hooks is that they cannot synchronize across different copies of the project (different copies are different clients), which is a problem I will need to solve later… That’s all.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"git","slug":"git","permalink":"https://silenwang.github.io/en/tags/git/"},{"name":"githook","slug":"githook","permalink":"https://silenwang.github.io/en/tags/githook/"}]},{"title":"Introduction to Decorators","slug":"装饰器入门","date":"2019-08-15T22:48:43.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2019/08/15/装饰器入门/","permalink":"https://silenwang.github.io/en/2019/08/15/%E8%A3%85%E9%A5%B0%E5%99%A8%E5%85%A5%E9%97%A8/","excerpt":"","text":"A long time ago, a Python expert from my previous company taught us about decorators. However, due to my愚钝ness, I never fully understood this concept and didn’t use it in actual coding. Therefore, I’m recording this example that comes up at work. Firstly, the role of a decorator in Python is to attach new functionality to an existing function without modifying its code. This requires defining a function that takes another function as an argument and returns a function. This week, I encountered a problem where I wrote a function to calculate some metrics for UMI sequences in a Bam file. However, the Bam files have two types of UMIs: Double-end with N bp UMIs Single-end with N bp UMIs The way to read UMIs from these files is different: single-end requires reading the full length of the UMI, while double-end only reads half the length. The original single-end code looks like this: 1234567891011121314151617181920212223242526def single_count(file, tag, mode): umi_count = &#123; &#x27;R1&#x27;: [], &#125; infos = load_single(file) for info in infos: if info[&#x27;strand&#x27;] == &#x27;R1&#x27;: umi_count[&#x27;R1&#x27;].append(info[&#x27;umi&#x27;]) umi_count[&#x27;R1&#x27;] = Counter(umi_count[&#x27;R1&#x27;]) total_umi = 0 read_with_umi = 0 max_grp_size = 0 max_umi = &#x27;&#x27; size_stat = Counter([1, 5, 10, 100, 1000]) for umi in umi_count[&#x27;R1&#x27;]: if umi_count[&#x27;R1&#x27;][umi] &gt; max_grp_size: max_grp_size = umi_count[&#x27;R1&#x27;][umi] max_umi = umi for size in size_stat: # 注意初始化的时候, 全都是1, 不是0, 所以最后要处理(-1) if umi_count[&#x27;R1&#x27;][umi] &lt;= size: size_stat[size] += 1 total_umi += 1 read_with_umi += umi_count[&#x27;R1&#x27;][umi] size_stat = OrderedDict(list([(&quot;%%Group_Size_below_%s(%s)&quot; % (size, tag), str((size_stat[size] - 1) / total_umi )) \\ for size in size_stat])) # python所以要先list化 return total_umi, read_with_umi, max_grp_size, max_umi, size_stat The only difference between the double-end and single-end code is that infos = load_single(file) needs to be replaced with infos = load_duplex(file). To achieve this, there is a straightforward way: add a parameter to the function and decide whether to call load_duplex or load_single based on the parameter. However, this would require modifying the function code as well as the code that calls this function. If there are many such places, it’s easy to introduce new bugs. Therefore, I tried using decorators to solve this problem: 1234567def load_mod(func): load_single = load_duplex return func@load_moddef duplex_count(file, tag, mode): return single_count(file, tag, mode) Here, I defined two functions: duplex_count aims to repeat all the steps in single_count except for infos = load_single(file), and only change the function called to load_duplex. Therefore, you can see that this function’s content is essentially returning the result of single_count. However, this alone doesn’t complete the replacement of the two load functions. That’s where load_mod comes into play: it replaces the content in the local namespace with load_duplex. By decorating duplex_count with load_mod, it’s equivalent to replacing the function call before executing it, so that the function executes differently. Of course, I use it more like the ‘closure’ usage in common decorator tutorials rather than ‘adding new functionality to a function’. Other applications will be discussed later.&#96;&#96;&#96;","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"Decorators","slug":"Decorators","permalink":"https://silenwang.github.io/en/tags/Decorators/"}]},{"title":"Dark History Project, Start!","slug":"黑历史计划-启动","date":"2019-07-27T21:40:03.000Z","updated":"2026-02-10T14:03:13.692Z","comments":true,"path":"2019/07/27/黑历史计划-启动/","permalink":"https://silenwang.github.io/en/2019/07/27/%E9%BB%91%E5%8E%86%E5%8F%B2%E8%AE%A1%E5%88%92-%E5%90%AF%E5%8A%A8/","excerpt":"As a former and still chuunibyou uncle teenager, how could I not have any dark history?","text":"As a former and still chuunibyou uncle teenager, how could I not have any dark history? The other day when I was searching for my Google ID, I suddenly remembered that I used to post blogs on NetEase LOFTER and Ant Notes… I’ll let the love-hate stories with my ex-girlfriend stay on NetEase Blog. I took a quick look and found quite a bit of content. It turns out that I wasn’t as asocial back then hhhhhhhh. Since it’s just my personal gitpage, why not follow the trend and repurpose the old content? Let’s see how past me was… so idle… With hundreds of posts on LOFTER, my mentor is truly generous. Since most of the old content is about daily life, I’ll put them all under Daily. Because it’s a repost, I’ll keep the original dates, so these posts won’t appear on the homepage. Also, since some short posts don’t have much value in republishing, I’ll skip those. That’s it.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[]},{"title":"MultiQC Simple Usage","slug":"MultiQC使用","date":"2019-07-27T21:02:16.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2019/07/27/MultiQC使用/","permalink":"https://silenwang.github.io/en/2019/07/27/MultiQC%E4%BD%BF%E7%94%A8/","excerpt":"MultiQC is a tool for NGS data quality control. Unlike many other tools, it does not directly obtain and calculate metrics; instead, it reads results files from various common quality control tools and provides comprehensive reporting.","text":"MultiQC is a tool for NGS data quality control. Unlike many other tools, it does not directly obtain and calculate metrics; instead, it reads results files from various common quality control tools and provides comprehensive reporting. Currently, MultiQC supports 75 types of tools, with the ability to add more information (although I haven’t tried it yet). It covers the entire process from base calling to mutation detection, and the reports generated are very aesthetically pleasing, making them ideal for final data quality summaries. MultiQC’s usage is quite simple. First, place the result files from other tools in a unified directory. Then run MultiQC /path/to/file, and MultiQC will search for various quality control results in the folder and generate the final summary report. For example, I have a report generated by MultiQC using FastQC results: You can see that the entire report is very beautiful. At the top of the report, there is a table summarizing various metrics. On the left side is a navigation bar showing calculated metrics for different software. Depending on the software used, specific content may be displayed as charts or tables. In the top summary table, by default, only some metrics are shown. If you want to change the selected metrics, click configure columns to adjust which metrics need to be displayed. Another convenient feature is that you can click the plot button to select two metrics from the summary table for a scatter plot. This feature will be particularly useful when checking for anomalies later. That’s all for a simple introduction. For more advanced usage, such as customizing new metrics and changing sample name matching rules to consolidate results from the same samples, I will continue to update this post with new findings.","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"qc","slug":"qc","permalink":"https://silenwang.github.io/en/tags/qc/"},{"name":"multiqc","slug":"multiqc","permalink":"https://silenwang.github.io/en/tags/multiqc/"}]},{"title":"Common Machine Learning Methods","slug":"常见机器学习方法","date":"2019-07-09T00:33:42.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2019/07/09/常见机器学习方法/","permalink":"https://silenwang.github.io/en/2019/07/09/%E5%B8%B8%E8%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/","excerpt":"Recently, my work requires me to understand some common machine learning methods. I’ll just write a post about it…","text":"Recently, my work requires me to understand some common machine learning methods. I’ll just write a post about it… Decision Trees: Build multi-level recursive binary classification paths based on attributes, and finally arrange the input object in an appropriate position according to this series of binary classification results. Pro: Easy to understand and explain Con: Sensitive to missing data; prone to overfitting; does not consider the correlation between attributes (treats them as independent?) Random Forest Algorithm: Based on decision trees. First, split the training data into multiple subsets, generate a decision tree for each subset, then input the classification information to be classified into each decision tree, and finally decide the final result by minority obeys the majority. Pro: Not prone to overfitting, has good noise resistance, insensitive to outliers Con: Poor interpretability, easily affected by multi-class attributes Lasso regression: An improvement on general linear regression, it reduces model complexity by penalizing coefficients. Pro: Strong interpretability, solves overfitting Con: Penalization can cause underfitting Logistic Regression: Uses logistic regression to get the relationship between the probability ratio of the dependent variable classification result and the input independent variables. Pro: Highly interpretable Con: Essentially a generalized linear model, not suitable for some complex situations Support Vector Machine (SVM): Separates points on a plane with a straight line. The distance between the two classes of points closest to the line is maximized. Extending this problem to multidimensional space results in support vector machines. Pro: Suitable for small sample sizes, has good universality Con: Poor interpretability, sensitive to missing data K-Nearest Neighbors (KNN): Places new data in a multidimensional space with known classified points. If the majority of the K nearest points belong to a certain class, this point belongs to that class. Pro: Simple theory and easy implementation Con: Large computational load, recalculates every time for classification, significantly affected by imbalanced classification Naive Bayes: Uses Bayes’ formula to calculate the probability of being in a certain category. Pro: Simple implementation and only requires a small amount of training data. Fast during large-scale calculations. Con: When applying the Naive Bayes classifier, it must satisfy the condition that all attributes are conditionally independent. K-Means Algorithm: A clustering algorithm in unsupervised learning; first select K initial points from a set of data, calculate the distance between all points and these K points, then assign each point to the nearest one among the K. After the first round of classification is completed, calculate an average center point (K) for each category of points, then recalculate the distance from all points to these K center points, and assign them to the nearest category. Repeat this process. When repeated a certain number of times, the grouping will stabilize, at which point it stops. Pro: Not sensitive to outliers Con: Large computational load when dealing with large amounts of data, poor effect with small amounts of data; easily affected by imbalanced classification Adaboost Algorithm: Combines multiple classifiers (multiple models) to achieve better classification results. Pro: Combines multiple basic algorithms and considers the weights of different classifiers Con: Training time-consuming Neural Network (NN): A relatively complex deep learning (DL) method. Personally, I understand that the neurons in a neural network are similar to decision trees, which will get a classification result based on input data, then pass this result to downstream neurons. The downstream neuron accepts upstream information and derives results before passing them on, finally summarizing the last layer’s results to give a classification result. Pro: High classification accuracy, strong robustness and fault tolerance Con: Deep learning, large computational load, poor interpretability Markov Chain (Markov Model): I didn’t understand… Main content excerpted from:轻松看懂机器学习十大常用算法机器学习算法优缺点及其应用领域 Of course… my explanations are not as simple and clear as those in the video. It seems that now learning requires Bilibili… Video Source, if you like it, please go follow the up and give it a triple like!","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"https://silenwang.github.io/en/tags/machine-learning/"}]},{"title":"Using Gitbook and mkdocs for Documentation","slug":"使用Gitbook和mkdocs写文档","date":"2019-07-07T22:13:31.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2019/07/07/使用Gitbook和mkdocs写文档/","permalink":"https://silenwang.github.io/en/2019/07/07/%E4%BD%BF%E7%94%A8Gitbook%E5%92%8Cmkdocs%E5%86%99%E6%96%87%E6%A1%A3/","excerpt":"My blog is generated using Hexo, a static blog framework based on Node.js. It helps people like me who know nothing about frontend development to quickly set up a decent-looking blog, while I only need to know how to write posts in Markdown. Similarly, during development there’s always a need to write technical documentation, hence tools like Gitbook and mkdocs exist.","text":"My blog is generated using Hexo, a static blog framework based on Node.js. It helps people like me who know nothing about frontend development to quickly set up a decent-looking blog, while I only need to know how to write posts in Markdown. Similarly, during development there’s always a need to write technical documentation, hence tools like Gitbook and mkdocs exist. Gitbook and mkdocs actually have different positioning. Gitbook was originally designed for quickly writing books (including documentation) using Markdown, while mkdocs helps technical personnel quickly write and maintain easily modifiable documentation. But in the hands of beginners like me…their basic functionalities are quite similar, so there’s not much difference… Gitbookgitbook-cli is a Node.js-based tool that can be installed via npm: npm install gitbook-cli. Note that due to changes in the Gitbook project’s direction, this CLI tool is no longer being updated. However, the basic features are sufficient and it still works fine… After installation, you can use the gitbook command to create and write documentation. The creation command is gitbook init, which generates README.md and SUMMARY.md files in the directory. SUMMARY.md serves as both the table of contents and structure file - gitbook-cli uses this file to generate the final documentation. SUMMARY.md itself is also a Markdown file: 1234567# Chapter 1* [Documentation](README.md)* [Part 1](Part1/README.md) * [Content 1](Part1/content1.md) * [Sub-content](Part1/content1.md)# Chapter 2* [XXXX](NNNN.md) Book chapters are controlled by first-level heading syntax (#). Different chapters are separated by dividers in the table of contents tree. If auto-numbering is enabled, the numbering will differ between chapters. Chapter contents are stored as unordered lists, with each list item using link syntax ([]()) to point to specific Markdown files. Within these Markdown files, first-level headings serve as anchors. Currently only first-level headings work as anchors - if you need to anchor to second-level headings or other content, you’ll need to manually add HTML tags as anchors. Gitbook supports all basic Markdown syntax as well as HTML, so if you need to include complex tables, you can write them in HTML format and paste them directly. With this knowledge, you can basically start writing books. If you need to customize the appearance or use plugins, you can create a book.json configuration file and fill in the settings accordingly. After writing, you can use gitbook serve to preview the book locally (default port 8000). The preview service can remain running, and modifications to Markdown source files will be reflected in real-time (may require refresh). When you’re satisfied with the book, use gitbook build to generate HTML files in the _book/ directory. mkdocsmkdocs is a Python-based tool that can be easily installed via pip or conda. After installation, the mkdocs command becomes available. To create documentation, use mkdocs new proj_name, which creates an mkdocs.yml file and docs/ folder in the proj_name/ directory. mkdocs.yml serves as both configuration and table of contents file, using YAML format: 12345678site_name: &quot;My Documentation&quot;pages:- Table of Contents: index.md- Chapter 1: chapter1.md- Chapter 2: chapter2.mdtheme: readthedocs mkdocs generates a table of contents similar to common Markdown TOCs, without needing additional anchor settings like Gitbook. Since my use case was relatively simple, I didn’t explore more advanced features. Completed documentation can be previewed using mkdocs serve, which also updates in real-time. Alternatively, use mkdocs build to generate static files in the site/ directory. Comparison of the Two ToolsI’ve used both tools while preparing handover documentation and have some comparative observations. In terms of basic functionality, there’s almost no difference for my needs - both use Markdown for writing documentation, have simple commands, and support local preview. However, since gitbook-cli is no longer updated, there might be future compatibility issues. Regarding extensibility…they’re also quite similar, with many plugins available for both. One notable difference is that Gitbook seems to lack third-party themes? I searched many times but couldn’t find ready-to-use themes, while mkdocs comes with several built-in ones. Also, since this was handover documentation for work, my colleagues weren’t accustomed to reading HTML, so I tried PDF export plugins for both tools…they work…but not very elegantly…compared to the HTML output, both PDFs look rather crude…though if I had to choose, mkdocs’ PDF output is slightly better.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"documentation","slug":"documentation","permalink":"https://silenwang.github.io/en/tags/documentation/"},{"name":"gitbook","slug":"gitbook","permalink":"https://silenwang.github.io/en/tags/gitbook/"},{"name":"mkdocs","slug":"mkdocs","permalink":"https://silenwang.github.io/en/tags/mkdocs/"}]},{"title":"Two Bash Tricks","slug":"Bash小技巧两则","date":"2019-06-05T21:17:03.000Z","updated":"2026-02-10T14:03:13.683Z","comments":true,"path":"2019/06/05/Bash小技巧两则/","permalink":"https://silenwang.github.io/en/2019/06/05/Bash%E5%B0%8F%E6%8A%80%E5%B7%A7%E4%B8%A4%E5%88%99/","excerpt":"While chaining various programs using Bash, you often discover some amazing and useful tricks. Here are two I recently found.","text":"While chaining various programs using Bash, you often discover some amazing and useful tricks. Here are two I recently found. The first trick is something I discovered when trying to replace samtools with sambamba. sambamba is a tool designed to replace samtools for handling BAM files, and many of its commands are similar to those in samtools. However, one subcommand did not support standard output, which meant that you had to generate an intermediate file before reading it for the next step. This was very wasteful of I&#x2F;O resources. After searching online, I found out that there are three special files in Linux: /dev/stdin, /dev/stdout, and /dev/null. These correspond to standard input, standard output, and discarding, respectively. For programs that do not explicitly support standard input&#x2F;output, you can try replacing the input&#x2F;output files with these special files to achieve reading content from standard input or writing output to standard output: 1234sambamba merge -t 4 /dev/stdout input1.bam input2.bam \\| sambamba sort -o /dev/stdout /dev/stdin \\| samtools view -Sb -f 30 \\&gt; output.srt.flt.bam /dev/null is also very useful. For example, samblaster only supports the -o option by default, which outputs the main result to standard output. However, I actually need the -s or -u options and want to process these results further. So, I can use /dev/null to discard the main result and then output the needed results to /dev/stdout for subsequent operations: 12345678910111213samtools view -h \\| samblaster \\ --ignoreUnmated \\ -o /dev/null \\ -d /dev/null \\ -s /dev/null \\ -u /dev/stdout \\| sed &quot;s/_[0-9]//g&quot; \\| /soft/bwa-0.7.15/bin/bwa mem \\ -t 30 -k 32 -M -R &quot;@RG\\tID:$&#123;sam_id&#125;\\tLB:$&#123;sam_id&#125;\\tSM:$&#123;sam_id&#125;\\tPL:ILLUMINA&quot; \\ /resource/GV/ref_genome/hg38/bwa-0.7.15/hg38.fa /dev/stdin \\| samtools view -Sb -f 256 \\| samtools merge -f output.bam input.bam /dev/stdin Another trick is to mask command results as files for input. For example, if I have multiple two-column files where one column is a data title and the other is a data value, and I want to merge them into one while retaining the first column’s titles, I can do: 12345paste file1 \\ &lt;(cut -f 2 file2) \\ &lt;(cut -f 2 file3) \\ &lt;(cut -f 2 file4) \\ &gt; pasted_file In the above operation, the data from file&#123;2..4&#125; is intercepted using cut -f 2 and then masked as files to be input into the paste command. This avoids using intermediate files and makes the process more efficient.&#96;&#96;&#96;","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://silenwang.github.io/en/tags/linux/"},{"name":"bash","slug":"bash","permalink":"https://silenwang.github.io/en/tags/bash/"}]},{"title":"Rewriting My Project in Go Language","slug":"用Go语言重写我的项目","date":"2019-06-02T14:46:49.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2019/06/02/用Go语言重写我的项目/","permalink":"https://silenwang.github.io/en/2019/06/02/%E7%94%A8Go%E8%AF%AD%E8%A8%80%E9%87%8D%E5%86%99%E6%88%91%E7%9A%84%E9%A1%B9%E7%9B%AE/","excerpt":"To learn Go language, I rewrote a script that was used frequently in my previous work. Compared to Python, which is simple, quick, easy to understand and has many useful third-party modules with abundant learning materials in both Chinese and English, if it wasn’t for performance reasons, I really wouldn’t want to switch…","text":"To learn Go language, I rewrote a script that was used frequently in my previous work. Compared to Python, which is simple, quick, easy to understand and has many useful third-party modules with abundant learning materials in both Chinese and English, if it wasn’t for performance reasons, I really wouldn’t want to switch… A Go Language Beginner’s ViewSince most of my larger programs were written in Python and R, which are not strong-typed languages, the difficulty mainly lies in using third-party modules and handling logical bugs designed by myself. Go is different… it is a strongly typed language. Variables must be declared before use, and once declared, their type cannot be changed later. This made me feel uncomfortable for a long time when I first started learning Go… Although Go provides interfaces to some extent to increase usability, as a beginner who didn’t have similar concepts in Python, R, or Bash, it took me quite some time to understand what an interface actually is. After solving the basic variable usage issues, there were differences in error handling methods, fewer third-party modules, and lack of Magic writing techniques, among other things. These problems had to be solved one by one… ultimately leading to this rewrite taking two whole weeks to initially complete… Although it was difficult, once you experience its speed, many people would be as excited as me (although during this rewrite, I also optimized the algorithm, but that optimization is absolutely not enough to bring a 200-fold difference). Well, the world changes so fast, learning something new is never wrong~ Here’s the address of the rewritten project: FUEX Quickly Mastering Go with Python as ReferenceJust like how I always found it hard to think in English when learning it from Chinese, writing this practice project was inevitably influenced by my way of thinking in Python. However, Python and Go are two very different languages, some things that are used in Python have similar or even no equivalents in Go, so here’s a summary of the alternative implementation schemes I encountered during this development. Variables&#x2F;Data TypesVariable Declaration and InitializationIn Python, there is no separate step for variable declaration and initialization. You can directly use var=value to use it, and the type of the variable can be changed later when processing. For example: 123var1 = Truevar2 = 1var1 = var2 if var2 else True In the above code, var1 and var2 are automatically assigned types (boolean, int), and although their types are different, you can pass the value of var2 to var1, and after passing, var1 will change from boolean to int. While all variables need to be declared like this, Go provides a simple way: var1 := 1. This allows you to declare and assign a variable at the same time, with the type of the variable automatically specified. However, since it’s automatically specified, if there are special requirements for the type, it might still be better to declare it manually. Go is a Strongly Typed LanguageThe term “strongly typed” refers to the detailed classification of variables in the language and the strict restrictions on passing and using variables of different types during program execution (personal understanding). In Python, the variable types are numbers, strings, logic, and empty (None), with complex ones being numbers further divided into floating-point and integers. Even Python has gotten rid of constants, only having variables, but it recommends using all uppercase letters to indicate constants in language writing suggestions (actually, the value can still be changed). In Go, there are constants (Const) and variables (Var). While variables have integer, float, logic types among others, integers and floats are further divided into different lengths such as int16, int32, int64. Even the uint I’m not quite clear about… These variable types cannot be automatically converted during use. For example, if a function gives you an int16, and the next function needs an int64, you still need to manually convert it once… These type conversions are really very uncomfortable for me who is used to Python. Another thing to note is that in Go, byte and string are two different types. Characters use single quotes (‘), while strings use double quotes (“). Complex Data Types—-Arrays&#x2F;Slices&#x2F;Collections&#x2F;StructuresJust like Python has Tuple, List, Dict, Set for storing multiple elements or mapping data, Go also has data types that meet the needs of data storage and processing: Array: Arrays are similar to Lists or Tuples. First, the length of an array in Go is fixed and cannot be changed once declared. At the same time, although the contents stored in an array can be mutable, the type of the stored elements must be fixed. Slice: Similar to arrays, but its length is variable. Map: Equivalent to Dict, similarly, both the Key and Value types are fixed and unchangeable. The Keys used by Map can be any type that supports logical comparison ==. Finally, what I needed most in my project was structures. It’s similar to R’s list or Python’s nested Dict, where it can store various kinds of information. Strong Typing with Flexible Handling—-Interface(interface)Interfaces are a new concept I encountered when接触Go. It is quite different from the API I’ve heard before. I spent quite some time trying to understand it… here’s a record. First, according to the previous section, we know that Go is strongly typed. This strong typing runs through the entire language system and can sometimes cause unnecessary troubles. For example, when writing functions, you need to define the types of the receiving and returning variables, input and return variables must be given types. Once the type is determined, the function can only receive&#x2F;this type of variable, it cannot change. This is very annoying. We write programs, there are many times when we have some functions that need to receive some objects with unclear situations, then according to the situation of the object for further processing. But under the rules of strong typing, receiving things must be a determined type. Then you have to write two duplicate functions to receive different types of objects and do similar processing. Take an actual example from vcfgo‘s Info().Get() function. This function’s purpose is to take out corresponding information in the INFO column of a vcf file and return it. Each INFO item may have multiple values, if there is only one value, then return the corresponding string, if there are multiple, then put them into a list with strings as elements and return it. This looks very simple, but under the rules of strong typing, it cannot be directly implemented. Because when defining functions, you must define the type of the returned variable. A variable’s type defined as a string cannot store a list of strings, vice versa. Then to achieve the target function, do we have to let the function have multiple return values? This is when interface comes into play. The definition of an interface is that as long as an object implements the methods defined in the interface, it has implemented this interface and can be passed in the form of this type. Returning back to the previous problem, just define an interface, then let the things you want to return implement this interface, so they can be returned in the form of interfaces. Finally, note that since the return is an interface, their types are all interface. If you need to pass them further, you may still need to use assertion to take them out from the interface as the original type. For example, there’s a line in my project: 123if annObj, ok := anns.(string); ok &#123; // Single annotation, directly parse annTarget = annObj&#125; This line of code where ann is the return value of the Info().Get() function. This interface may be a string or a list of strings, and I use anns.(string) to try to assert that it’s a string inside. If the assertion is correct, then directly assign the parsed string, if incorrect, then perform another processing. File OperationsPython’s file reading and writing are very simple. Just one line of code opens a file, and the obtained object is iterable. You can directly use for loop to process file content line by line. Writing files is similar, it can be done with just a few lines of code too. At the same time, Python has a with statement that automatically closes the file handle after operations are completed, which is very convenient. 123with open(&quot;file&quot;, &quot;r&quot;) as f: for line in f: print(line) Compared to Go, file reading and writing are much more troublesome. For example, if I want to read file content line by line and print it: 12345678910111213file, err := os.Open(vcfFile)defer file.Close() // Automatically close the file handle, similar to using &#x27;with&#x27; in Pythonif err != nil &#123; panic(err)&#125;reader := bufio.NewReader(file)for &#123; str, err := reader.ReadString(&#x27;\\n&#x27;) if err == io.EOF &#123; break &#125; fmt.Println(str)&#125; The main steps are first opening a file, then specifying with another function to read one line of content using &#39;\\n&#39; as the delimiter. Then print the read content to the screen. Actually, the core part is similar to Python, but some things have become self-handled, so the code naturally becomes longer. For example, after opening the file, error handling in Go has to be done manually. If Python can’t find a file when reading, it will automatically throw an exception and tell you that the file doesn’t exist, then exit. In contrast, writing Go requires manual panic and displaying the error message from err. In Python, using for to iterate over the opened file object allows you to read file content line by line, and it will automatically terminate when reaching the end of the file. In Go, this process has to be written manually, even specifying that files are separated by &#39;\\n&#39;. Writing files is similar to reading, but slightly more complex in Go. Here’s a pit I’ll fill later. Nested Data StructuresPython commonly used data structures can be nested to form complex data structures to meet actual data processing needs. This kind of nested data structure is very flexible because dict&#x2F;list&#x2F;tuple&#x2F;set have no special restrictions on what they store, so we can easily compose a series of different things into one organized structure. This is one reason why I prefer Python over R for certain data processing tasks (R has some internal structures that only store similar types of elements). For example, in the refGene file information stored in my project, my program needs to read out a gene’s transcript number, exon start and end positions, transcription direction, gene name, etc. series of information. Among them, the transcript number is a string, the exon start and end positions store a list of integers, the transcription direction is a single character, and the gene name is also a string. This series of information can be stored in a large dictionary with Key: Value form, and you can get the corresponding value according to the Key when needed. This is impossible in Go, because list corresponds to arrays, dict corresponds to maps, in Go they can only store things declared beforehand: 12outStrList := []string&#123;&quot;out1&quot;, &quot;out2&quot;&#125; // Can only store stringsgene2tran := make(map[string][]string) // Key and Value must be strings Therefore, to achieve similar nested data structures as Python, you have to use the structure struct: 1234567891011// Structure I defined in my project to store transcript informationtype geneRecord struct &#123; exon [][]int64 intron [][]int64 transRegion [2]int64 cdsRegion [2]int64 chr string strand string geneSymbol string&#125;tran2info := make(map[string]geneRecord) // map of structures Control StructuresGo’s control structures are even simpler than Python… judgment uses if/else, loop uses for, multi-branch structure uses switch——only these. 123456789101112131415161718192021222324252627282930// if else and for examplesif cols[3] == &quot;+&quot; &#123; for idx, _ := range exonStarts &#123; exons = append(exons, []int64&#123;exonStarts[idx], exonEnds[idx], exonFrames[idx]&#125;) if idx != len(exonStarts)-1 &#123; introns = append(introns, []int64&#123;exonEnds[idx], exonStarts[idx+1]&#125;) &#125; &#125;&#125; else &#123; for idx, _ := range exonStarts &#123; exons = reverSlice(append(exons, []int64&#123;exonStarts[idx], exonEnds[idx], exonFrames[idx]&#125;)) if idx != len(exonStarts)-1 &#123; introns = reverSlice(append(introns, []int64&#123;exonEnds[idx], exonStarts[idx+1]&#125;)) &#125; &#125;&#125;// switch exampleswitch &#123;case AinA &amp;&amp; BinB: // Target fusion, corresponding correct locates[&quot;breakA&quot;] = locAinA locates[&quot;breakB&quot;] = locBinB return locates, &quot;right&quot;, nilcase AinB &amp;&amp; BinA: // Target fusion, corresponding reversed locates[&quot;breakA&quot;] = locAinB locates[&quot;breakB&quot;] = locBinA return locates, &quot;wrong&quot;, nildefault: err := errors.New(&quot;Not Fusion&quot;)&#125; Although there are fewer things, they are enough… Another thing to note is that Go provides goto as a jumping control statement besides break and continue. However, I haven’t used it yet… Building FunctionsCreating functions in Go is not special… just pay attention to declaring the types of input parameters and return values first (in the example below, reverSlice follows with the passed-in parameter, outside the parentheses are the declarations of the returned value). Once declared, they cannot be changed. If you need flexible inputs and outputs, you can rely on interface. 1234567func reverSlice(a [][]int64) [][]int64 &#123; for i := len(a)/2 - 1; i &gt;= 0; i-- &#123; opp := len(a) - 1 - i a[i], a[opp] = a[opp], a[i] &#125; return a&#125; Error HandlingEssentially, Go does not have Python’s specialized error handling or exception throwing operation. However, when using Go, you will find that many functions return two values, for example, the file opening function: 1file, err := os.Open(yourFile) The above file opening function returns not only the opened file object but also another variable. Personally, I understand this as something similar to the status code returned by executing a Shell command. Generally speaking, if the program execution within the function is normal, this variable will be nil. If there is an error during the function’s part of execution, it will return an error class object containing the error description string. Processing this variable is equivalent to using Try: Except: in Python. String HandlingGo’s string handling mainly relies on the strings package, which includes basic operations such as connection, splitting, replacement, and trimming of whitespace… the actual usage experience is similar to R’s stringr… there are many things… but it feels not so convenient. Another thing to note is that Python’s formatted strings in Go do not have an equivalent. So you cannot create a large text template in Go and fill in the content. JSON HandlingGo also has a specialized json package to read JSON file contents into Go data structures, and convert Go objects into JSON strings. However, due to type restrictions, it may be slightly more complex than Python… 12345678tarTranFile := &quot;RXA_gene_trans.json&quot;data, err := ioutil.ReadFile(tarTranFile)if err != nil &#123; return&#125;var targetTransFile interface&#123;&#125;err = json.Unmarshal(data, &amp;targetTransFile)targetTrans := targetTransFile.(map[string]interface&#123;&#125;) You can see that to read the written JSON file, I executed opening the file, loading the content, using a specific function to process it, and then because the return is interface, so finally did type assertion. Similar operations… in Python, 1~2 lines can be completed… Program CompilationUnlike Python which translates and executes code line by line, Go language needs to be compiled before use (although there’s also go run), so you need to study how to compile executable files.(Placeholder for digging a hole) Unique Features of Python and GoThis section records some unique features of Python and Go respectively (digging a hole placeholder). Python Part List&#x2F;Dictionary Comprehensions: In Python, list&#x2F;dictionary comprehensions are very convenient iterable object generation&#x2F;handling methods that can effectively simplify code (whether there’s an efficiency boost isn’t clear), but in Go, you have to write loops obediently. String Formatting: The benefit of Python’s string formatting is that it allows you to fill in a large amount of text one by one. This writing method makes the final appearance of the string very clear and easy to understand. In contrast, Go can only concatenate strings. Go Part Interface (see above) switch control structure Convenient parallelization: Although Python has multi-threading, multi-processing, coroutines… but they are not as efficient or convenient as Go. This is one of the reasons why I decided to learn Go.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://silenwang.github.io/en/tags/golang/"}]},{"title":"Setting up HTTPS using Caddy","slug":"https配置","date":"2019-05-07T21:59:51.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2019/05/07/https配置/","permalink":"https://silenwang.github.io/en/2019/05/07/https%E9%85%8D%E7%BD%AE/","excerpt":"Previously, I set up a Gogs server but didn’t configure HTTPS; it was directly using HTTP. Last time, I tried and found that intercepting passwords was too easy… So I thought about getting a domain name and setting up HTTPS…","text":"Previously, I set up a Gogs server but didn’t configure HTTPS; it was directly using HTTP. Last time, I tried and found that intercepting passwords was too easy… So I thought about getting a domain name and setting up HTTPS… Firstly, to use HTTPS, you need an SSL certificate. To get a certificate, it seems that free SSL certificates basically require a domain name. So I got the cheapest one from INVALID POST SLUG PROVIDED www.namesilo.com/‎. Domain registration and binding IP can be found in this blog post: this article. After getting the domain, the next step is to set up a reverse proxy server. I chose Caddy because I read that it’s simple and easy to use, with automatic application and renewal of SSL certificates. Maybe my luck wasn’t good… Although setting it up was very simple, I couldn’t get the SSL certificate to work. In the end, I found a project on GitHub called acme.sh to apply for SSL certificates. This script is very easy to use, and the author’s usage instructions are detailed; following them should be fine. Of course, there’s also a more user-friendly tutorial: this guide. Using acme.sh will generate four files, among which fullchain.cer, your.domain.key are the certificate and key. Since I deployed Gogs and Caddy using Docker, to make things convenient, I wrote both in a single compose file: 123456789101112131415161718192021222324version: &quot;2&quot;services: gogs: image: 08fa8ee955da container_name: gogs restart: always ports: - &quot;3022:22&quot; volumes: - /var/gogs:/data caddy: container_name: caddy image: abiosoft/caddy environment: - ACME_AGREE=true volumes: - &quot;~/gogs/Caddyfile:/etc/Caddyfile&quot; - &quot;/root/.acme.sh/your.domain/fullchain.cer:/root/fullchain.cer&quot; - &quot;/root/.acme.sh/your.domain/your.domain.key:/root/your.domain.key&quot; ports: - &quot;8080:2015&quot; - &quot;80:80&quot; - &quot;443:443&quot; restart: always In the above file, two containers are configured. Gogs only exposes port 22 for SSH service; HTTP ports are not exposed. Caddy can set access directly in its configuration (although I don’t know how to do it).For the Caddy container, note that setting the ACME_AGREE=true environment variable is necessary; otherwise, acme will prompt you that you cannot use it… Also, specify the Caddy configuration file Caddyfile and the certificates and keys generated above. The port exposure in the Caddy container’s configuration seems to be incorrect; I’ll look into how to fix it later. The specific content of the Caddyfile is as follows: 1234567891011your.domain &#123; proxy / gogs:3000 &#123; header_upstream Host &#123;host&#125; header_upstream X-Real-IP &#123;remote&#125; header_upstream X-Forwarded-For &#123;remote&#125; header_upstream X-Forwarded-Proto &#123;scheme&#125; &#125; log /var/log/caddy.log gzip tls /root/fullchain.cer /root/your.domain.key&#125;","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"caddy","slug":"caddy","permalink":"https://silenwang.github.io/en/tags/caddy/"},{"name":"https","slug":"https","permalink":"https://silenwang.github.io/en/tags/https/"}]},{"title":"Graphviz Usage","slug":"Graphviz的使用","date":"2019-04-25T21:31:53.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2019/04/25/Graphviz的使用/","permalink":"https://silenwang.github.io/en/2019/04/25/Graphviz%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"When developing software, whether for presentation or to organize thoughts, using flowcharts is often necessary. Instead of using a drawing board or Word, which can be time-consuming when the diagram becomes complex and needs frequent updates, a tool like Markdown that allows you to focus on content rather than form is very important.","text":"When developing software, whether for presentation or to organize thoughts, using flowcharts is often necessary. Instead of using a drawing board or Word, which can be time-consuming when the diagram becomes complex and needs frequent updates, a tool like Markdown that allows you to focus on content rather than form is very important. Graphviz is a script-based graph visualization tool that can create many types of graphs. Currently, I only use flowcharts. To create a flowchart, you need to write a .dot formatted file, which looks like this: 12345678910111213141516digraph workflow&#123; // Default definition section for the entire graph label = &quot;Workflow For Mutation Call&quot; node[shape = box] // Definition section for nodes in the graph raw [label=&quot;Raw Bam&quot;]; clean [label=&quot;UMI Dedup Bam&quot;]; align [label=&quot;Split Bam&quot;]; vcf [label=&quot;Disc Bam(Prototype)&quot;]; // Relationship section for the flowchart raw -&gt; clean [label=&quot;fastp&quot;]; clean -&gt; align [label=&quot;bwa&quot;]; align -&gt; vcf [label=&quot;GATK Mutect2&quot;];&#125; I divided the above code into three parts: The first part sets default properties for elements in the graph, such as label which is the title of the graph, and node which defines the nodes in the flowchart. I set the default node shape to a box (the default is a circle). The second part defines the nodes in the graph by specifying what nodes there are and then customizing them in the brackets, such as label to specify the displayed text. The third part defines the order of relationships in the flowchart, such as raw -&gt; clean which draws an arrow from raw to clean. Similarly, you can customize it with brackets. After writing the above code, you can generate images by calling the dot command of Graphviz: 1dot -Tpng workflow.dot &gt; workflow.png There is a corresponding plugin for VSCode, and on Linux, after installing the plugin, you can preview flowcharts as you write, which is very convenient.&#96;&#96;&#96;","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"graphviz","slug":"graphviz","permalink":"https://silenwang.github.io/en/tags/graphviz/"}]},{"title":"The usability of Linux distributions is getting higher and higher","slug":"Linux下的软件越来越丰富了","date":"2019-04-06T17:03:42.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2019/04/06/Linux下的软件越来越丰富了/","permalink":"https://silenwang.github.io/en/2019/04/06/Linux%E4%B8%8B%E7%9A%84%E8%BD%AF%E4%BB%B6%E8%B6%8A%E6%9D%A5%E8%B6%8A%E4%B8%B0%E5%AF%8C%E4%BA%86/","excerpt":"Recently, while helping my senior sister with sequence alignment, I discovered that the software I used to use frequently, MEGA X (10), now has a Linux GUI version. This made me feel that the world is indeed changing.","text":"Recently, while helping my senior sister with sequence alignment, I discovered that the software I used to use frequently, MEGA X (10), now has a Linux GUI version. This made me feel that the world is indeed changing. I remember installing my first Linux distribution for fun in my third year of university. At that time, I chose Ubuntu. The interface and effects were very cool, and it was much smoother on my old computer compared to Windows. However, after a while, the interface became boring, and various problems started popping up: poor Chinese support, lack of commonly used software, frequent system updates (involving kernel or major version changes), and little documentation for beginners. I can’t even remember all the small issues I encountered. At that time, Deepin was based on Ubuntu and focused on optimizing the user experience for Chinese users. So, within half a month, I switched from Ubuntu to Deepin. Although it improved Chinese support and system updates compared to the original Ubuntu, the lack of commonly used software (such as office and communication tools) made it almost impossible to use this system in an environment where everyone uses Windows. After about two months, I gave up because of assignments and gaming (essentially gaming…). Later, when I installed a Linux distribution again during my second semester of graduate school, my initial intention was to reduce gaming time and focus on studying. At that time, smartphones had already become widely available, so basic communication could be done using phones, and there were even Linux versions of WPS for writing documents and presentations (although the playback format still needed adjustment). For academic purposes, I used R for statistics and Ugene for sequence analysis. Writing scripts was much easier on Linux than on Windows (one command in Linux versus multiple installations and configurations in Windows), so I smoothly transitioned to a pure Linux environment. Then, I switched to Manjaro based on Arch due to some incorrect installation methods, but I never gave up. G胖’s efforts to promote the Linux gaming ecosystem made me feel like being dominated… From then on, I could use Linux as my primary operating system for daily life without major issues. Of course, this is partly because of the development of Linux GUIs and other factors that have resolved past problems. For example, communication software still hasn’t been fully resolved (WeChat, QQ, Taobao旺旺 don’t have native clients), but due to the rise of smartphones, we often no longer need to use computers for instant messaging. Another example is Flash and HTML5; although Linux support for Flash was poor, many websites required it, making them unusable without a virtual machine. However, Flash eventually fell out of favor and was replaced by HTML5… I hope one day in my lifetime, the market share of Windows will really decrease, allowing us to be free from being limited to a single system platform.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://silenwang.github.io/en/tags/linux/"}]},{"title":"Using GNU Parallel in Bash for Parallel Execution","slug":"使用GNU-Parallel在Bash中执行并行","date":"2019-03-31T23:07:26.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2019/03/31/使用GNU-Parallel在Bash中执行并行/","permalink":"https://silenwang.github.io/en/2019/03/31/%E4%BD%BF%E7%94%A8GNU-Parallel%E5%9C%A8Bash%E4%B8%AD%E6%89%A7%E8%A1%8C%E5%B9%B6%E8%A1%8C/","excerpt":"I recently discovered GNU Parallel, a simple and useful parallel program implemented in Perl, which can easily execute various commands in Bash in parallel. This is particularly useful for tasks that can be divided into independent subtasks.","text":"I recently discovered GNU Parallel, a simple and useful parallel program implemented in Perl, which can easily execute various commands in Bash in parallel. This is particularly useful for tasks that can be divided into independent subtasks. GNU Parallel allows you to replace loops with parallel execution by specifying the command to be executed in parallel. For example, consider a script that filters VCF files based on chromosomes: 1234for chr in &#123;&#123;1..22&#125;,X,Y&#125;;do bcftools view -f PASS -r chr$&#123;chr&#125; sample.vcf &gt; sample.chr$&#123;chr&#125;.flt.vcfdonebcftools merge sample.chr&#123;&#123;1..22&#125;,X,Y&#125;.vcf &gt; sample.flt.vcf Using GNU Parallel, you can rewrite it as follows: 12echo &#123;&#123;1..22&#125;,X,Y&#125; | tr &quot; &quot; &quot;\\n&quot; | parallel &#x27;bcftools view -f PASS -r chr&#123;&#125; sample.vcf &gt; sample.chr&#123;&#125;.flt.vcf&#x27;bcftools merge sample.chr&#123;&#123;1..22&#125;,X,Y&#125;.vcf &gt; sample.flt.vcf In the first example, the script loops through each chromosome and filters the VCF file. In the second example, the filtering is done in parallel, followed by merging. Of course, this example doesn’t make sense because both methods essentially do the same thing. By default, GNU Parallel uses all available cores, so you can specify a maximum number of cores using -j or set a load threshold with --load 80%. With this tool, any command that supports positional parameter analysis for regions can be quickly converted to parallel execution, significantly speeding up the analysis process. This makes writing Snakemake workflows more convenient… I will continue to explore other uses of this program in future posts…","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"bash","slug":"bash","permalink":"https://silenwang.github.io/en/tags/bash/"},{"name":"parallel computing","slug":"parallel-computing","permalink":"https://silenwang.github.io/en/tags/parallel-computing/"}]},{"title":"Lumpy's Algorithm Interpretation","slug":"Lumpy的算法解读","date":"2019-03-31T22:10:20.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2019/03/31/Lumpy的算法解读/","permalink":"https://silenwang.github.io/en/2019/03/31/Lumpy%E7%9A%84%E7%AE%97%E6%B3%95%E8%A7%A3%E8%AF%BB/","excerpt":"Lumpy’s author positioned this project more as building a general structural variant signal generation scheme rather than creating an excellent tool. It was implemented as a basic version of the scheme. Compared to other tools, Lumpy spent more time in its article explaining the evidence concepts and methods for obtaining information. Interpreting this tool has made me understand better how to obtain structural variant signals from NGS sequencing data.","text":"Lumpy’s author positioned this project more as building a general structural variant signal generation scheme rather than creating an excellent tool. It was implemented as a basic version of the scheme. Compared to other tools, Lumpy spent more time in its article explaining the evidence concepts and methods for obtaining information. Interpreting this tool has made me understand better how to obtain structural variant signals from NGS sequencing data. Tool BasicsLumpy is written in C++ and all its functionalities seem to be implemented independently without referencing any existing modules. The software can be obtained via conda, and the program itself hasn’t been updated for several years, so there’s no need to worry about compiling from source code. The output format of Lumpy results can be selected between VCF and BEDPE. However, it’s worth noting that these two formats contain different information; VCF is more detailed. Additionally, this tool relies entirely on the alignment records in BAM files. Therefore, if the alignment itself is not ideal, it will inevitably affect downstream results. Furthermore, the tool does not contain any random components; with the same input files, the detection results will be identical. Lumpy’s Structural Variant ModelLumpy first created a structural variant breakpoint model: $b &#x3D; (E,l,r,v)$ In this model, breakpoints are defined as pairs of genes that are adjacent in the target genome but not in the reference genome. These pairs of breakpoints are described by four indicators: $E$: A set of evidence supporting the breakpoint $l$: The left breakpoint interval (interval), containing a vector p representing the relative probability of each point being the true breakpoint position $r$: Consistent with $l$, containing all information about the right breakpoint $v$: Information about the complexity of the breakpoint, mainly containing direction information from different evidence. This indicator is included to retain more original information because when results are detected, they will only be labeled as one type of structural variant (e.g., deletion), but the original signal may contain support for other types of evidence. Lumpy uses both SR and RP signals, each parsing out an instance of a breakpoint. After reading all breakpoint instances, it needs to infer whether the same breakpoint exists based on the information contained and then merge breakpoints. The conditions for merging are: Both sides of the breakpoints have overlapping regions (threshold unspecified) Consistent complexity If these conditions are met, the two breakpoints can be merged. $E$ will contain the original signals from both breakpoints, $v$ remains unchanged (since it is consistent), and $l$ and $r$ are combined by taking the arithmetic mean. That is, the start of $l$ is the average of the starts of $l1$ and $l2$, and the end is also the average of the ends of $l1$ and $l2$, $r$ is similar (but when there are multiple, how it executes specifically is unknown; taking averages two by two results in different outcomes): $$merge.start &#x3D; \\frac{l1.start + l2.start}{2}\\ \\ merge.end &#x3D; \\frac{l1.end + l2.end}{2}$$ According to the author’s description, this method is used to reduce the impact of outlier evidence on the breakpoint interval. After merging the evidence, breakpoints with more than a preset threshold will be detected as structural variants. To determine detected structural variants, information from the breakpoint instances needs to be considered. Here are the default calculation schemes: The possible interval of the variant The probability that each point in the interval is the true breakpoint First, the variant’s possible interval (interval) takes the maximum value of the left breakpoints and the minimum value of the right breakpoints (i.e., the smallest possible interval). While the probability for each point in the interval can be calculated as follows (taking the left breakpoint as an example): $$l.p[i] &#x3D; \\prod_{e{\\in}E}e.l.p[i-o]$$ $i$ is a certain gene coordinate position $l.p$ is the vector of probabilities that a single point is the true breakpoint, with the prefix $l.$ representing the left breakpoint $E$ is the set of all evidence $e$ is each piece of evidence in the evidence set $e.l.p$ is the probability vector for the left breakpoint in a single piece of evidence $o$ is the offset value, $o &#x3D; e.l.s - s.l.s$, i.e., the difference between the start position of the current evidence’s left breakpoint and the predicted start position of the structural variant. $e.l.p[i-o]$ might seem a bit confusing, but in essence, it’s similar to coverage. The final probability for each point in the interval is the product of the probabilities from all pieces of evidence that have this point. So, theoretically, the closer a point is to having more overlapping regions with original evidence intervals, the smaller its p-value will be, indicating a higher likelihood of being the true breakpoint position. During evidence merging, since the start and end points are continuously averaged to determine the merged interval’s start and end points, it is possible that there will be no interval between the final start and end points (equal or reversed? No instances have been verified). In such cases, the author identifies the maximum point among all sum distributions in $b.E$, removes any distribution not intersecting this point, and processes the resulting subset normally. Thus, a final result entry is formed. The final result entry’s breakpoint interval can also be reduced according to a preset percentage, for example, taking only the top 99.9%. Besides the above scheme, there is a Summation scheme used for low-quality data, which will not be interpreted here. From the above schemes, it is clear that Lumpy essentially reads information-containing alignment records from BAM files, extracts necessary information, and then interprets structural variants based on this information. A crucial point is extracting which information from the signals. To define the required information, Lumpy designed a called Breakpoint evidence interface or commonly used evidence class: This interface requires three pieces of information: is_bp: Whether it contains breakpoint position information get_v: Structural variant type information get_bpi: Breakpoint possible interval information That is, for each signal&#x2F;evidence, the above three pieces of information are interpreted and then added as a separate evidence $e$ to the evidence set $E$ for further interpretation. So, how does one interpret the above information from signals&#x2F;evidences? The article explains this with examples from SR, RP, and general breakpoint interface: RP Evidence InterpretationIn paired-end sequencing, the insert fragment length of Reads Pair has a normal range, and the directions of Read1 and Read2 should be +&#x2F;-. Any abnormal alignment indicates the presence of a breakpoint. Therefore, the interpreted information is as follows: is_bp: Yes, it can determine that a breakpoint exists get_v: When two reads are on the same chromosome, based on the actual alignment direction of Read1 and Read2, the mutation type (DEL&#x2F;INV&#x2F;DUP) can be inferred. Otherwise, it will be uniformly labeled as inter-chromosomal get_bpi: The final breakpoint interval and the p values for each point are obtained through the alignment information of two reads. It is somewhat complex; see below. Obtaining the breakpoint interval essentially involves mapping the left (l) and right (r) breakpoints from one read’s alignment information to determine the l (containing the interval and the midpoint p value) and r. For example, for l: Determine whether the breakpoint is upstream or downstream based on the alignment direction of the read. Before making a judgment, the program estimates the distribution of insert fragment lengths from BAM files without alignment issues to get the average length $l$ and standard deviation $s$ (note that it’s the standard deviation Standard Deviation, not standard deviation). Additionally, a user-specified variable $v$ is required, with $l + vs$ as the expected value for normal insert fragment lengths. Since breaks are unlikely to result in fragments exceeding this expected length, it can be inferred that the breakpoint should be within the range of $l + vs$ (bp) upstream or downstream of the alignment position. This determines the interval range. For each point $i$ in the above interval, its probability of being a true breakpoint is calculated as $P(S(y).e-S(x).s &gt; i - R(x).s)$ (the original text does not clearly explain what $S()$ represents; I infer it’s a sample abbreviation, with $R()$ representing actual fragment information). $y$ represents Read2, and $x$ corresponds to Read1. $S(y).e$ is the end position of Read2 in the actual fragment, and $S(x).s$ is the start position of Read1 in the actual fragment. $S(y).e-S(x).s$ is the length of the actual fragment. $i-R(x).s$ is the distance from point $i$ to the start position of Read1’s alignment. $P()$ should be a function that gets the p value from the distribution of normal alignment read fragment lengths. For example, $P(Z&gt;2.94)$ in a standard normal distribution gives the area under the curve, allowing you to evaluate the probability that point $i$ is the true breakpoint. SR Evidence InterpretationIn SR evidence, one read’s adjacent parts align to non-adjacent genomic positions. In Lumpy, if a read has more than two small fragments, these small fragments are split into pairs of smaller segments for consideration (e.g., three fragments are divided into $x_1,x_2$ and $x_2,x_3$ groups). The information parsed from each pair of small fragments is as follows: is_bp: Yes, it can determine that a breakpoint exists get_v: When two fragments have the same alignment direction, the structural variant might be DEL or DUP (depending on whether the alignment directions are +&#x2F;-. If the alignment directions are different but on the same chromosome, it’s INV. If they align to different chromosomes, it’s inter-chromosomal. get_bpi: The interval position of the breakpoint is related to the inferred structural variant type. Considering potential sequencing errors, the SR evidence gives a range for the breakpoint position. This range size depends on the input parameter $v$, sample quality (?), and the algorithm used for alignment (?). Since the description in the literature about this part is not as detailed as that of RP, here are some personal speculations; further testing may be needed to confirm: About interval acquisition: Ideally, the two parts of a Split-Read断裂 should be non-overlapping. However, in practice, there might be overlapping between the two parts. From Lumpy’s own SR Read extraction script, it indeed allows for some overlap between the two parts. So, the statement that the interval range of SR evidence is related to sample quality and alignment algorithm may refer to this. About p value determination within the interval: The author only mentioned that the midpoint p value is the smallest, with p values increasing closer to the edges. It might be a proportional distribution… This would need further testing by creating evidence BAM files. general breakpoint interface InterpretationThis type of evidence essentially records structural variant information in BEDPE files. Since BEDPE files originally record both sides of the breakpoint interval and the structural variant type, these can be directly mapped to get_v and get_bpi. The description of the BEDPE format will be discussed later; for now, it’s omitted. The existence of this class of evidence serves two purposes: first, in genetic analysis, it makes it easier to detect variants that are known to exist in a population. Second, when using multiple tools simultaneously, it allows reliable variants detected by other tools to be merged with Lumpy results.","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"Structural Variants","slug":"Structural-Variants","permalink":"https://silenwang.github.io/en/tags/Structural-Variants/"}]},{"title":"Basic Concepts in Structural Variation Detection Using NGS","slug":"结构变异检测基本","date":"2019-03-16T20:04:36.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2019/03/16/结构变异检测基本/","permalink":"https://silenwang.github.io/en/2019/03/16/%E7%BB%93%E6%9E%84%E5%8F%98%E5%BC%82%E6%A3%80%E6%B5%8B%E5%9F%BA%E6%9C%AC/","excerpt":"“I know a lot about structural variation information,” but the content of this blog post is too much, and it’s not convenient for me to look back. So I decided to split it into smaller parts and write a comprehensive one using GitBook later.","text":"“I know a lot about structural variation information,” but the content of this blog post is too much, and it’s not convenient for me to look back. So I decided to split it into smaller parts and write a comprehensive one using GitBook later. Structural Variations&#x2F;Copy Number Variations&#x2F;Gene FusionsStructural variations are different from small-scale gene mutations like SNPs&#x2F;InDels, referring to all relatively large-scale gene mutations (the exact size threshold is not clearly defined). Most literature refers to these variations as structural variations (Structural Variation), which are believed to originate from chromosomal reorganization or rearrangement. These variations come in various sizes and styles, ranging from whole chromosome exchanges mentioned in biology class to simple segment deletions, which are just relatively larger segments. Currently, structural variations are mainly classified into the following types: Insertion&#x2F;Deletion (Insertion&#x2F;Deletion, INS&#x2F;DEL): Essentially the same as InDel but with larger fragments. Duplication (Duplication, DUP): Represents the continuous repetition of a certain region. Additionally, there is ‘tandem duplication’, which requires further investigation. Inversion (Inversion, INV): The 5’ and 3’ ends of a chromosomal segment are reversed. Translocation (Translocation, ITX, CTX): Refers to the transfer of a segment, which could be within the same chromosome or between different chromosomes. Translocations themselves are complex structural variations. For example, if a segment from position A on a chromosome is moved entirely to position B, then there will be a deletion at position A and an insertion at position B. If the inserted segment is not inserted in its original direction, then inversion also occurs. The following image is taken from Gawroński’s doctoral thesis and vividly illustrates the types of structural variations. Based on structural variations, we can further explore copy number variations (Copy Number Variation) and gene fusions (Gene Fusion). The key to copy number variation is ‘copy’. For example, ‘ATGCGGTAGCCGTAT’ represents a complete gene sequence, which we call one copy. Since humans are diploid, normally there should be two copies of this gene in a person. Therefore, when detecting genes, if the number of copies of this gene in the genome is not 2, we say that a copy number variation has been detected. From the above concept, we can see that copy number variation is essentially a structural variation or more precisely, a result of structural variation. If there is a deletion in the genome, then the number of gene copies must decrease. Duplication obviously leads to an increase in copy numbers. Why are copy number variations singled out? I personally think this may be related to when the concept was proposed or its underlying significance. Assuming each different copy of the gene has the same expression capacity, then an increase in gene copy numbers will inevitably lead to increased gene expression, while a decrease in copy numbers will result in decreased gene expression or even disappearance of related expression products. In short, compared to structural variations that focus on figuring out what kind of changes occurred in a gene segment (such as where the deleted segment starts and ends, whether it has moved to another location in the genome, etc.), copy number variations focus more on whether the copy number of a certain gene&#x2F;segment has changed. Due to different focuses, copy number variations can be detected together with structural variations or separately. In fact, most of the time they are detected separately using different strategies. This article mainly focuses on structural variations, so there will not be specific content about copy number variations. Similarly, gene fusions (Gene Fusion) are also results of structural variations. For example, if a segment of a gene is completely deleted, then the upstream and downstream segments will inevitably connect. If the breakpoints before and after the missing segment happen to be in the expression regions of two genes, then these two genes will be connected together, which is gene fusion. Therefore, we can see that all structural variations may lead to gene fusions, but the final way they fuse is different. Gene fusions are of particular interest because they are involved in confirming some disease causes. For example, EML4-ALK fusion in cancer and BCR-ABL fusion in chronic myeloid leukemia. Since gene fusions focus on whether there are genes connected together and how they connect (which affects whether gene expression increases or decreases), they have specific strategies and tools for detection, which will be detailed later. Using NGS Data to Detect Structural Variations&#x2F;Gene FusionsThe basic principle of second-generation genome resequencing is to break the complete gene sequence into fragments, then sequence these fragmented sequences and align them back to the reference genome to reconstruct the original sequence. During this alignment process, some “abnormal” alignment results will be produced. Among these results, some can indicate that the test genome has undergone structural variations relative to the reference genome, and our detection is based on these alignment results, which we call structural variation signals&#x2F;evidence. Structural Variation Detection StrategiesThe basic strategies for detecting structural variations are generally classified into four categories: 1. Split-ReadsIf you have ever worked with NGS sequence alignment files (SAM&#x2F;BAM), you must have seen Clip-Reads in the file. Simply put, these sequences cannot be fully aligned to a specific location on the genome during alignment and instead align partially to one place and partially to another. These sequences may come from fragments containing structural variation breakpoints, thus resulting in this abnormal “chimeric alignment“. By extracting these sequences and tracing back the alignment of the two clipped sequences, we can infer the type of structural variation to some extent. The strategy for detecting structural variations based on this information is called SR strategy. 2. Discordant Read PairsIf paired-end sequencing (Pair-End Sequencing) was used, then the sequencing results will appear in pairs. According to the sequencing principle, paired-end sequencing yields a single DNA strand’s 5’ and 3’ sequences. Since the DNA library preparation breaks DNA into a fixed range, under normal circumstances, the distance between Read1 and Read2 on the genome should be within a fixed range, and one should align positively with the reference genome while the other should align negatively (complementary). However, in actual alignment results, there will always be some sequence pairs that do not meet these conditions, such as a large distance or even different chromosomes, or both Read1 and Read2 aligning positively on the genome. In this case, these sequences are called “Discordant Read Pairs”. Since this signal comes from a pair of sequences, it is directly referred to as “Pair-End Evidence, PE” in Lumpy, but based on this signal, detection strategies seem to be more frequently called RP (Read Pair) strategy (this needs further confirmation by reading more evaluation articles). Such read pairs may also come from fragments containing structural variation breakpoints. The difference lies in the position of the breakpoint. For Split-Reads, the breakpoint is at one end of the sequence, so it is detected directly. For Discordant Read Pairs, the breakpoint should be within the insert fragment range, meaning it is not directly reflected in the sequenced fragments. 3. Read Count or DepthThis strategy is based on the sequencing depth of specific segments. Clearly, this is used to detect copy number variations. Regardless of what kind of structural variation occurs, if it ultimately results in a decrease in the number of copies of a gene, it should be intuitively reflected in the alignment results of that gene—i.e., the sequencing depth of that gene region will clearly decrease. Conversely, if there is an increase in copy numbers, the sequencing depth should also correspondingly increase. This strategy is theoretically simple from a principle standpoint, but it is not necessarily much simpler than the previous two strategies in actual application. The main issue it faces is a seemingly simple but very troublesome problem: how high does the depth need to be considered significantly high, and how low does it need to be considered significantly low? This problem becomes more complex when using targeted or exome sequencing for copy number variation detection. 4. De novo AssemblyHere, de novo assembly refers to assembling the genome from scratch without relying on a reference sequence, which is essentially the same as second-generation sequencing. Applied in structural variation detection, it involves assembling the obtained sequences before looking for SR or PE signals. Since assembled sequences yield longer fragments, these fragments can be used for better alignment results. It can be seen that this AS strategy will not be used alone because its purpose is to obtain longer fragments and then re-align them before detecting using SR. Personal Thoughts on Detection Strategies1. Theoretical and Practical Aspects of Sequencing MethodsAs mentioned in Professor Huang Shujia’s blog post, to obtain more reliable structural variation detection results, increasing fragment length and performing whole-genome sequencing is the simplest direct method. However, both methods mean increased single-test costs. Until sequencing becomes truly affordable, using as little data as possible to achieve more reliable results will be a necessary problem in applying structural variation detection in clinical settings. 2. Difficulty of Obtaining PE and SR SignalsAssuming that the possibility of a structural variation breakpoint being sequenced is equal, then obtaining PE signals should be greater than obtaining SR signals because the length of insert fragments is usually longer than the reads themselves, so more breakpoints will fall within the insert fragment range. Additionally, since most detection methods still rely on alignment, some SR signals may also lose due to poor breakpoint positions (too close to read ends), making correct alignment difficult and causing signal loss. Therefore, under the same sequencing depth, applying PE signals theoretically can improve detection sensitivity. 3. Detection Bottlenecks Caused by AlignmentSince most tools still rely on sequence alignment situations, if real variation signals exist but are difficult to align (sequences too short for alignment), some signals will inevitably be lost and affect detection accuracy. This has also led to attempts at non-alignment-based detection methods. Gene Fusion-Specific Detection StrategiesAs mentioned earlier, gene fusion is a result of structural variation. Some gene fusions are of interest because they lead to abnormal gene expression, resulting in diseases. Therefore, in gene fusion detection, the focus is on detecting whether genes have fused together and how they have fused. Due to this different focus, besides using structural variation detection strategies to detect structural variations from the genome and then interpret possible fusions, specialized fusion detection methods have also been developed: RNA-Seq fusion detection. This method directly detects fusions in the transcriptome. If sequences from different gene transcripts are detected, it indicates a gene fusion event. Additionally, since detecting is done with RNA, there’s no need to speculate whether fusion will occur because detecting it proves its effectiveness. Of course, I don’t think this method is perfect either. Some potential drawbacks include: Higher detection difficulty compared to DNA, as RNA is easily degraded… As an applied clinical testing method, using RNA testing should have sample limitations and cost constraints. Since I haven’t used such tools in my actual work yet and don’t plan to use them, I’ll come back to fill this pit later. Additionally, most current structural variation&#x2F;fusion detection methods require alignment to a reference genome, which is currently time-consuming (except for using FPGA chips for specialized optimization). However, gene fusion essentially only needs to know whether there are sequences showing two genes fused together. Therefore, some commercial companies have developed non-alignment-based detection schemes based on fastq data (such as [GeneFuse by Shenzhen Haplos]), which indeed significantly speeds up the detection process. Finally, one thing to note is that due to different focuses, some specialized gene fusion detection software only reports fused genes and specific exons in their results, in various forms, making it difficult to directly compare with structural variation detection tools.","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"Structural Variation","slug":"Structural-Variation","permalink":"https://silenwang.github.io/en/tags/Structural-Variation/"}]},{"title":"Logistic Regression in R","slug":"R中的逻辑回归","date":"2019-03-16T14:07:19.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2019/03/16/R中的逻辑回归/","permalink":"https://silenwang.github.io/en/2019/03/16/R%E4%B8%AD%E7%9A%84%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/","excerpt":"I didn’t expect to do regression again after four years… this time using R instead of SPSS. Previously, I was doing statistical analysis, but now… it’s machine learning.","text":"I didn’t expect to do regression again after four years… this time using R instead of SPSS. Previously, I was doing statistical analysis, but now… it’s machine learning. IntroductionLogistic (logit) regression is a type of generalized linear model that has the characteristic of converting a categorical dependent variable into a continuous variable by comparing all categories to one reference category. In logistic regression, the coefficients of the variables are taken as exp(x) after which they represent the OR value for this variable, meaning that for every unit increase in this variable, the probability of being classified as 1 increases by exp(x). Model ConstructionThis time, my dependent variable is binary, so I can use the basic package’s glm() function. The specific model construction uses: family=binomial(link=&#39;logit&#39;) to specify calling a binary logistic regression model 12model &lt;- glm(y ~ x1 + x2, family=binomial(link=&#x27;logit&#x27;), data=fit_data)summary(model) # Display model information When performing regression, it’s difficult to guarantee that the first regression will yield the best results. Among the included covariates, there may be variables with coefficients too small (meaning they have little effect), or statistical tests are not significant (which may not have an effect in a resampling). Therefore, we need to change variables multiple times for regression. In R, step() can help automatically perform regression and remove variables that contribute little and are not significant: 12model.step &lt;- step(model)summary(model.step) Of course, this is just an automated reference. Ultimately, how the model is determined still depends on specific judgment by a person based on actual circumstances. Model Prediction Situation EstimationAfter building the model, you can use predict() to predict test datasets and evaluate the model. Then, use some third-party packages through ROC curves to determine fitting situations and select final classification thresholds. 12345678910test_data$prob &lt;- predict(model.step, test_data, type = &quot;response&quot;) # response means getting a probability between 0~1, not a constant# Use the pROC packagelibrary(pROC)obj_roc &lt;- roc(test_data$Real, test_data$prob) # Real is the correct answer, prob is the predicted probabilityplot(obj_roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1, 0.2), # Make a plot grid.col=c(&quot;green&quot;, &quot;red&quot;), max.auc.polygon=TRUE, auc.polygon.col=&quot;skyblue&quot;, print.thres=TRUE)# The pROC package gives an ROC curve graph with sensitivity (true positive) on the y-axis and specificity (true negative) on the x-axis, not the more common false positive rate, so the x-axis is 1-&gt;0 instead of 0-1 Small TipsThe model object itself contains detailed model information, but directly taking information from the model does not match what summary() sees. For example, the coefficient table, summary() gives a table that includes coefficient values, standard errors, and statistical test P-values, but directly taking it from the model will only have coefficient values. To take summary() information, you need to take it from the generated object or assign it first: 12345# First assignmentmodel_info &lt;- summary(model)model_info$coefficients# Directly takesummary(model)$coefficients Blog Article LearningI found a few blog posts on using R for logistic regression to supplement model evaluation methods. Logistic Regression Balance of binary variables: In logistic regression, if the two categories of the dependent variable are balanced, the classification results will be ideal. Therefore, before performing regression, you should check the situation of the two categories: table(your_data$val). If they are not balanced, theoretically, you should sample the larger category to match the number of the smaller category. Multicollinearity detection: Logistic regression and linear regression require routine detection of multicollinearity issues. The blog recommends VIF&lt;4. How to perform a Logistic Regression in R $R^2$ calculation: Logistic regression cannot calculate $R^2$. The article suggests calculating the McFadden $R^2$ index as an alternative. More information about this metric can be found at R squared in logistic regression. Logistic regression on biased data Evaluation metrics selection in imbalanced classification: When the categories are imbalanced, accuracy is not a good evaluation metric. It is recommended to use F1-score. Solutions for imbalanced classification include two types: oversampling the minority class and fixing the model by altering the INVALID POST SLUG PROVIDED or INVALID POST SLUG PROVIDED . According to the article, oversampling the minority class and undersampling the majority class can improve detection results to some extent, but they are not good enough for testing data. 12library(pscl)pR2(model) Other Materials Logistic Regression Algorithm Cross-validation idea in machine learning Dealing with unbalanced data in machine learning","categories":[{"name":"Statistic","slug":"Statistic","permalink":"https://silenwang.github.io/en/categories/Statistic/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"}]},{"title":"Switching Themes & Hexo Framework Blog Setup Notes","slug":"换新主题-Hexo框架搭建博客记录","date":"2019-03-12T00:20:02.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2019/03/12/换新主题-Hexo框架搭建博客记录/","permalink":"https://silenwang.github.io/en/2019/03/12/%E6%8D%A2%E6%96%B0%E4%B8%BB%E9%A2%98-Hexo%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E8%AE%B0%E5%BD%95/","excerpt":"Yesterday, due to some unparsable symbols in my markdown post, I thought Hexo had crashed, so I took the opportunity to switch to a new theme…and reviewed the setup process again.","text":"Yesterday, due to some unparsable symbols in my markdown post, I thought Hexo had crashed, so I took the opportunity to switch to a new theme…and reviewed the setup process again. General Setup ProcessHexo is based on Node.js. On Arch-based systems, simply run sudo pacman -S nodejs npm After installation, use npm to install hexo and hexo-cli globally (hexo-cli is essential, whether hexo itself is necessary will be tested next time) 123# -g stands for global module, after installation you can use hexo command line toolssudo npm install -g hexosudo npm install -g hexo-cli Next, use hexo to create a new blog project with just hexo init. Note that this needs to be done in an empty directory, otherwise it will prompt that the directory is not empty and cannot proceed. The blog created by hexo comes with a default theme, but generally we don’t use it…Search online for a theme you like, then clone it to the project’s theme directory using git. After that, modify the _config.yml file in the project to change blog settings. After modifications, hexo g &amp; hexo s can generate and preview the blog locally. hexo d can publish&#x2F;update the blog website, but this requires specifying the publishing method in the configuration file (I use git), and installing the corresponding publishing module as prompted (just npm install, this doesn’t need to be installed globally) Blog Project BackupIn many cases, you may need to edit blog content in multiple locations. This is quite simple…just git the entire project. The only thing to note is that if you use git to install themes, each folder under theme is originally a git project. Personally, I think it’s better to set all files in the theme folder to be ignored in .gitignore when establishing git for the entire project, and then separately backup the theme’s _config.yml.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://silenwang.github.io/en/tags/hexo/"}]},{"title":"Some Useful Features of Snakemake","slug":"Snakemake的一些有用特性","date":"2019-03-06T23:41:40.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2019/03/06/Snakemake的一些有用特性/","permalink":"https://silenwang.github.io/en/2019/03/06/Snakemake%E7%9A%84%E4%B8%80%E4%BA%9B%E6%9C%89%E7%94%A8%E7%89%B9%E6%80%A7/","excerpt":"After using snakemake for some time, I found that it has many practical features to facilitate daily analysis. Here is a record and整理.","text":"After using snakemake for some time, I found that it has many practical features to facilitate daily analysis. Here is a record and整理. Resource LimitsIn the rule, you can use the resources keyword to limit resources. This is very useful when you need to use a GPU (only one GPU) or have limited disk performance (cannot perform too many high IO operations at the same time). For example, the following setting: 12345678910111213141516171819202122232425rule all: expand(&quot;&#123;sam&#125;.vcf&quot;, sam=[&quot;sam1&quot;, &quot;sam2&quot;, &quot;sam3&quot;, &quot;sam4&quot;])rule samtools_sort: input: rules.samtools_flt.output output: &quot;&#123;sam&#125;.flt.srt.bam&quot; resources: IO=8 shell: &quot;samtools sort -@ 8 &#123;input&#125; &gt; &#123;output&#125;&quot;rule samtools_call: input: &quot;&#123;sam&#125;.bam&quot; output: &quot;&#123;sam&#125;.vcf&quot; resources: IO=1 shell: &#x27;&#x27;&#x27; bcftools mpileup -Ou -f reference.fa &#123;input&#125; \\\\ | bcftools call -mv -O -o &#123;output&#125; &#x27;&#x27;&#x27; In the above workflow, the sort process uses 8 threads simultaneously and consumes a lot of IO. Therefore, its IO usage is set to 8, while the call operation sets IO=1. When running, you can control the number of high IO tasks by setting the maximum IO value: snakemake --resources IO=16. Task GroupsWhen using a cluster, in certain situations, grouping related tasks to be submitted to the same node can save on IO (prevent来回 transmission of intermediate files between nodes, roughly speaking…), and the keyword used is group. Tasks within the same group will be submitted to the same node for execution at once (the node will start another snakemake instance to control them). The actual usage is very simple, so I won’t write an example. However, there is one thing to note: after tasks are submitted to a node using this feature to utilize the local disk of the node, although there is a snakemake controlling them, when the task submission ends, the local snakemake process will once again check if the input and output meet the requirements. This point needs to be noted. For example, I used this function to take advantage of the local disk of the compute node, but these disks are limited to the compute nodes and cannot be accessed from the login node. This will cause the login node’s process to fail when checking if the tasks are completed because some intermediate results cannot be found.&#96;&#96;&#96;","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"snakemake","slug":"snakemake","permalink":"https://silenwang.github.io/en/tags/snakemake/"}]},{"title":"itertools in Python","slug":"python中的itertools","date":"2019-02-13T00:37:27.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2019/02/13/python中的itertools/","permalink":"https://silenwang.github.io/en/2019/02/13/python%E4%B8%AD%E7%9A%84itertools/","excerpt":"","text":"I’ve never used many useful functions when writing scripts before, so I decided to learn a bit about itertools and make some notes. This time, I was dealing with large files, but most of the time, we handle them line by line. So, I thought about using itertools to create generators. This way, I might be able to split the generator into smaller parts and apply it to parallel operations. The three functions from itertools that I used this time are: groupby tee chain groupby is used to cluster items in an iterable object. For example, when dealing with UMI grouping, you need to compare reads that are close in position and group them together for further UMI analysis. 123# Here&#x27;s a function that takes a single object from iter_obj as input and returns the value to be used for comparisongrp_generator = groupby(iter_obj, lambda x: x[0])# Note that groupby only groups adjacent objects, so you might need to sort them yourself. tee is simple. If the iterable object is a generator, since generators can only be used once, you can use the tee function to create a copy of it for multiple uses. 1iterable_fork_1, iterable_fork_2 = tee(iterable, 2) # The number 2 is optional and defaults to 2. chain concatenates multiple iterable objects into one. I used it to flatten nested results: 12flat_iter = chain.from_iterable(ori_iter)# [[1,2],3] -&gt; [1,2,3] Of course, the actual result is a generator, so you need to use list(flat_iter) to get the complete result.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"itertools","slug":"itertools","permalink":"https://silenwang.github.io/en/tags/itertools/"}]},{"title":"Multiprocessing in Python","slug":"python中的多进程","date":"2019-02-12T23:45:59.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2019/02/12/python中的多进程/","permalink":"https://silenwang.github.io/en/2019/02/12/python%E4%B8%AD%E7%9A%84%E5%A4%9A%E8%BF%9B%E7%A8%8B/","excerpt":"R provides convenient multiprocessing capabilities, and Python has similar functionality.","text":"R provides convenient multiprocessing capabilities, and Python has similar functionality. First, let’s quote from Liao Xuefeng’s Python Tutorial to explain processes and threads: For an operating system, a task is a Process. For example, opening a browser starts a browser process, opening Notepad starts a Notepad process, opening two Notepads starts two Notepad processes, and opening Word starts a Word process. Some processes do more than one thing at the same time. For example, Word can perform typing, spell checking, and printing simultaneously. Within a process, to perform multiple “subtasks” at the same time, we call these “subtasks” Threads. Since each process must do at least one thing, a process has at least one thread. Of course, complex processes like Word can have multiple threads that execute simultaneously. The execution of multiple threads is similar to multiple processes - the operating system quickly switches between threads, making each thread run alternately for short periods, appearing to execute simultaneously. True simultaneous execution of multiple threads requires a multi-core CPU. All the Python programs we’ve written before are single-task processes with only one thread. How do we execute multiple tasks simultaneously?There are two solutions:One is to start multiple processes, where each process has only one thread, but multiple processes can execute multiple tasks together. Another method is to start one process and multiple threads within that process, so multiple threads can execute multiple tasks together. There’s also a third method - start multiple processes, each with multiple threads - but this model is more complex and rarely used in practice. The tasks I need to execute don’t involve long IO waits, so I didn’t choose Python’s multithreading or coroutines, but instead tried multiprocessing. The main code is as follows: 12345678910111213141516171819202122from multiprocessing import Pooldef func_to_process(args): do someting return resultdef main(): sub_procs = Pool(10) # 10 is the number of child processes to use results = [] # To store results for obj in iterable: result = sub_procs.apply_async(func_to_process, (args)) # Submit task to process pool, apply_async means submit and continue # Will block when pool is full results.append(result) sub_procs.close() # Close pool, no new tasks can be submitted sub_procs.join() # Wait for all processes to complete results = [result.get() for result in results] # Use get() to retrieve all resultsif __name__ == &quot;__main__&quot;: main() There are two issues with this code: This approach splits the file by lines with each line as a process, which is too fragmented. Especially when the task has lengthy preparation steps, this actually reduces efficiency (single process prepares once, while multiprocess prepares many times) The results may be out of order. If the task requires maintaining order, the results need to be sorted again For issue 1, after some research, it might be possible to use some functions from itertools to implement quantitative splitting of the original file. Will update tomorrow. For issue 2, we could use the corresponding map method instead of apply. This requires further testing.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"Parallel Computing","slug":"Parallel-Computing","permalink":"https://silenwang.github.io/en/tags/Parallel-Computing/"},{"name":"Multiprocessing","slug":"Multiprocessing","permalink":"https://silenwang.github.io/en/tags/Multiprocessing/"}]},{"title":"Using the parallel package in R","slug":"R中parellel包的使","date":"2019-01-01T02:04:56.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2019/01/01/R中parellel包的使/","permalink":"https://silenwang.github.io/en/2019/01/01/R%E4%B8%ADparellel%E5%8C%85%E7%9A%84%E4%BD%BF/","excerpt":"Recently, I wrote a script to draw many images using ggplot. Although all the places where loops are needed have already been replaced with apply, it still can’t keep up when drawing hundreds of images at once. So, I fiddled around with parallel and managed to parallelize the plotting part to speed things up.","text":"Recently, I wrote a script to draw many images using ggplot. Although all the places where loops are needed have already been replaced with apply, it still can’t keep up when drawing hundreds of images at once. So, I fiddled around with parallel and managed to parallelize the plotting part to speed things up. Before using Python, I was familiar with multithreading and multiprocessing. However, in R… it doesn’t seem to emphasize threads and processes as much; instead, it focuses on parallel computing. The only relevant resources I found in Chinese were parallel and foreach. Of course, I used parallel, but I haven’t studied the differences between the two… Using it is actually quite simple. First, you register a cluster, then you throw your tasks at the cluster to run (in function form), and finally, you stop the cluster. The “cluster” here doesn’t refer to a cluster server; it’s just what English calls ‘cluster’, which I don’t know how to translate well… I’ll just call it that for now. Here is the specific implementation code: 1234567891011# Prepare the plotting functionplot_process &lt;- function (data_list, args) &#123; library(ggplot2) plot &lt;- ggplot() + geom_line() ggsave(&quot;plot.png&quot;, plot)&#125;cl &lt;- makeCluster(core_num) # RegisterparLapply(cl, data_list, plot_process, args) # The first parameter specifies the cluster, and the rest are the same as lapplystopCluster(cl) # Stop# Since I&#x27;m only processing data separately and plotting, there&#x27;s no need for the merging step that many tutorials have. So it’s actually quite simple… The only slightly troublesome issue is that for each cluster node, you need to load the variables, packages, and functions separately. For variables, there are specific functions to pass them; for packages, you can write the loading statements inside plot_process. However, for custom functions, there isn’t a direct way to do it…After searching for a long time without finding a solution, I suddenly had an idea of passing the functions as arguments to plot_process, and it worked… After writing this script, I also saw another solution: write these custom functions in another file and then import them using source() inside plot_process. This should be a better approach, so I’ll try it out later….","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"},{"name":"Parallel Computing","slug":"Parallel-Computing","permalink":"https://silenwang.github.io/en/tags/Parallel-Computing/"}]},{"title":"teamviewer+ssh jump = VPN","slug":"teamviewer-ssh跳转-vpn","date":"2018-12-23T23:00:13.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2018/12/23/teamviewer-ssh跳转-vpn/","permalink":"https://silenwang.github.io/en/2018/12/23/teamviewer-ssh%E8%B7%B3%E8%BD%AC-vpn/","excerpt":"It’s been a while since the last update. Today, I’ll record the upgraded version of my homemade VPN!","text":"It’s been a while since the last update. Today, I’ll record the upgraded version of my homemade VPN! Previously, due to some reasons, my laptop was running Windows 10, and then I switched back to Manjaro. So, I had to reconfigure some things again, one of which was the VPN I used for working from home. Since our company doesn’t have a VPN set up, I’ve always been using TeamViewer. However, it’s quite inconvenient. Because I use a virtual machine on my company computer, there are often mysterious issues, and during busy times, it shows noticeable lag. During the time when I was running Windows 10, I偶然 discovered that the latest version of TeamViewer (version 14) has a VPN mode, so I tried it out, and it really works… So, I followed the homemade VPN setup from my previous company to create one again: Materials needed VirtualBox with Micro XP TeamViewer 14 Cygwin with OpenSSH Tip: This time, I tried the Tiny XP image… found that Cygwin couldn’t work properly inside it… so I had to download the original backup image from Baidu Cloud… SSH Settings: Configure ~/.ssh/config, set up the jump point there, where jumping to the company’s work computer is done via TeamViewer 14’s VPN. The address can be obtained after connecting through TeamViewer. It seems to be fixed, so I directly wrote it into the configuration file. 12345678910111213141516171819Host vpn_vm User Administrator HostName 127.0.0.1 Port 8022 IdentityFile ~/.ssh/vpn_vm_keyHost Jump HostName 123.456.789.0 Port 8022 User silen IdentityFile ~/.ssh/jump_key ProxyCommand ssh Administrator@vpn_vm -W %h:%pHost Target HostName 123.456.789.1 Port 22 User silen IdentityFile ~/.ssh/target_key ProxyCommand ssh silen@Jump -W %h:%p Here, a two-level jump is set up. vpn_vm is the local virtual machine where TeamViewer is installed for VPN connection and OpenSSH is responsible for allowing this machine to jump through the virtual machine. Jump is the company’s machine, which has Manjaro installed in VirtualBox and starts the SSHD service. Target is the final server. The ProxyCommand sets up the command for each level of jump, and also sets up keys to achieve passwordless login. When logging in, you can directly use ssh Target, which is very convenient. scp and sftp can be used directly as well.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"ssh","slug":"ssh","permalink":"https://silenwang.github.io/en/tags/ssh/"},{"name":"teamviewer","slug":"teamviewer","permalink":"https://silenwang.github.io/en/tags/teamviewer/"}]},{"title":"Using Snakemake for Workflow Control in Unconventional Ways","slug":"用外门邪道的方式使用snakemake实现流程控制","date":"2018-11-17T12:47:57.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2018/11/17/用外门邪道的方式使用snakemake实现流程控制/","permalink":"https://silenwang.github.io/en/2018/11/17/%E7%94%A8%E5%A4%96%E9%97%A8%E9%82%AA%E9%81%93%E7%9A%84%E6%96%B9%E5%BC%8F%E4%BD%BF%E7%94%A8snakemake%E5%AE%9E%E7%8E%B0%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6/","excerpt":"Snakemake is indeed a very useful tool for workflow development and management. However, in certain scenarios, it can also bring some issues. Coincidentally, I discovered a rather unconventional way of using it: only use Snakemake’s dependency handling and task management, while generating scripts separately.","text":"Snakemake is indeed a very useful tool for workflow development and management. However, in certain scenarios, it can also bring some issues. Coincidentally, I discovered a rather unconventional way of using it: only use Snakemake’s dependency handling and task management, while generating scripts separately. The reason was that when writing the workflow, I had to place an actual executable script at a specific location so that users could read these scripts and modify specific steps before resubmitting for execution. The original intention was good: allowing users who are not familiar with Snakemake to easily modify analysis parameters for specific samples. However, the problem is that Snakemake does not support exporting actual executable scripts as files; it can only output the execution content directly in the log using the -p parameter. Moreover, before finding a solution, I was unaware that wildcards could be used in cluster submission commands… So, after several twists and turns, I found this rather unconventional solution. Divide the entire workflow into several parts and write programs to generate all required scripts. When submitting to SGE, use the -sync y parameter with qsub so that the submission does not end immediately but waits for the completion&#x2F;failure of the submitted job. If the task fails, it will return a non-zero value. Use Snakemake to build workflow dependencies as usual, specifying input and output normally. Add the execution script to input, and only write the submission command in shell:. This way, we can solve the problem of not being able to directly generate scripts by generating them ourselves and submitting them.At the same time, we can also take advantage of Snakemake’s dependency resolution and task management features for progress monitoring and breakpoint continuation.However, this approach also digs a huge pit: the script execution content and file names are not specified within Snakemake. If they do not match, the workflow will fail, and there is no good method to locate such issues; it relies entirely on the writer’s caution. For example, you write a script test.sh like this: 12#$ -sync ycat test.txt | cut -f 2 &gt; tset.col2.txt And your Snakemakefile is like this: 12345678910111213rule all: input: test.col2.txtrule test: input: &#123;sample&#125;.txt output: &#123;sample&#125;.col2.txt shell: &#x27;&#x27;&#x27; qsub test.sh &#x27;&#x27;&#x27; When using Snakemake to run, it will never succeed… because the script generates a result file name that does not match the target file name in Snakemake.By default, since rule test is determined to have failed, other result files used for judgment (if they exist) will be deleted… In other words, using this method requires you to ensure consistency between the target file names. Excluding this drawback, this approach can fully leverage Snakemake’s advantages in task management while avoiding the time required to understand the wildcard mechanism when initially getting started with Snakemake.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"snakemake","slug":"snakemake","permalink":"https://silenwang.github.io/en/tags/snakemake/"}]},{"title":"snakemake's Usage","slug":"snakemake的使用","date":"2018-10-30T12:40:03.000Z","updated":"2026-02-10T14:03:13.686Z","comments":true,"path":"2018/10/30/snakemake的使用/","permalink":"https://silenwang.github.io/en/2018/10/30/snakemake%E7%9A%84%E4%BD%BF%E7%94%A8/","excerpt":"Writing workflows is a common task in bioinformatics analysis, and a mature and well-designed tool can greatly improve the efficiency of work.","text":"Writing workflows is a common task in bioinformatics analysis, and a mature and well-designed tool can greatly improve the efficiency of work. The simplest way to write a workflow is to write all the commands you need into a shell script and specify the order of tasks using &amp;&amp; or other methods. Then, you can run it in the background. Although this approach is easy, its repetitiveness and error resistance are really worrying. Therefore, various workflow management tools have emerged: I used sjm at my previous unit, 10x Genomics developed Martian, Broad Institute’s WDL, and I am currently using snakemake. Introduction to snakemakesnakemake is a bioinformatics analysis workflow management tool developed by the Köster lab at the University of Duisburg-Essen in Germany. Its entire code is written in Python, so it can be easily integrated with Python programs (although it also has a complete set of command-line programs). The features of snakemake include but are not limited to: Simple and easy to use Can directly use Python code Built-in distributed computing cluster support (SGE, LSF, etc.) Built-in Conda environment deployment support Built-in container support (note that this container is not Docker) Automatic handling of dependencies and breakpoint restarts Built-in testing functionality (benchmarking) Same execution logic as make, so people with a background in make can quickly get up to speed Simple Example of snakemakeIn a snakemake file, the most basic unit is a rule, which represents a step in the workflow. Under a rule, you can define input files, output files, and the commands to be executed. Since snakemake is based on Python, its syntax format is consistent with Python, using indentation to define control scopes, such as writing a simple rule: 1234567rule samtools_sort: input: &quot;test.bam&quot; output: &quot;test.srt.bam&quot; shell: &quot;samtools sort &#123;input&#125; &gt; &#123;output&#125;&quot; This rule named “samtools_sort” executes the step of sorting the input test.bam using the sort command from samtools, and then specifies the output result as “test.srt.bam”. The final shell: indicates that this is a shell command, with the command given as a string. Snakemake also supports directly executing Python code; in this case, the keyword needs to be changed to run:. Additionally, it supports calling R code and other external scripts. For more details, refer to the official manual. Using snakemake for Batch Processing of SamplesAfter understanding the basic writing of a rule, you can write a series of rules and then link them into a workflow. Although snakemake is simple and easy to learn, writing a complete workflow that can handle multiple samples still requires some knowledge. Special String Formatting in snakemakeFrom the previous simple example, it can be seen that commands, input files, and output files in the snakefile are given as strings. These strings can use similar formatting string methods to reduce hardcoding and increase applicability. For example, in the &quot;samtools sort &#123;input&#125; &gt; &#123;output&#125;&quot; from the previous example, &#123;input&#125; will be replaced with the string defined after input within this rule (if there are multiple inputs, they will be listed separated by spaces). Similarly, we can call any content defined within a rule to fill in text templates and generate the final commands or strings that need to be executed. However, one thing to note is that for those who are used to using format(), it might look like this: 123456789path=&quot;path/to/input&quot;rule samtools_flt: input: &quot;test.bam&quot; output: &quot;test.flt.bam&quot; shell: &quot;samtools sort &#123;path&#125;/&#123;input&#125; &gt; &#123;output&#125;&quot;.format(path=path) This is mixing Python’s string formatting with snakemake’s string formatting. This kind of writing cannot be correctly handled by snakemake; a string can either use the built-in formatting method or the original formatting method of Python. The above example, using format(), will be processed according to Python’s original method, filling in the string based on the information given in format(). Unspecified input and output will be left empty. Snakemake can handle this situation; however, the writing style needs to be changed: 123456789path=&quot;path/to/input&quot;rule samtools_flt: input: &quot;test.bam&quot; output: &quot;test.flt.bam&quot; shell: &quot;samtools sort &#123;path&#125;/&#123;&#123;input&#125;&#125; &gt; &#123;&#123;output&#125;&#125;&quot;.format(path=path) For those who are more familiar with Python, you might know that &quot;\\&#123;\\&#123;\\&#125;\\&#125;&quot; represents not escaping the &#123;&#125; within a string but treating them as general symbols. Combining snakemake’s execution logic, it is known that snakemake will handle the pure string input itself after Python part execution is complete. Execution Logic of snakemake WorkflowsSnakemake does not have a dedicated syntax for specifying order or dependencies; workflow connections are automatically completed based on input and output file dependencies. Since snakemake defaults to parsing and completing the first rule, it is recommended by the official documentation to create a rule named “all”. In this rule‘s input (note: not output), specify all final files. Then, the program will find which rule can produce the required file based on the output in the snakefile. It continues to parse the dependencies of this rule until it finds the source or determines that there is no source and reports an error, for example: 123456789101112131415161718rule all: &quot;test.flt.srt.bam&quot;rule samtools_flt: input: &quot;test.bam&quot; output: &quot;test.flt.bam&quot; shell: &quot;samtools sort &#123;input&#125; &gt; &#123;output&#125;&quot;rule samtools_sort: input: rules.samtools_flt.output output: &quot;test.flt.srt.bam&quot; shell: &quot;samtools sort &#123;input&#125; &gt; &#123;output&#125;&quot; In the above snakefile, the workflow execution logic is as follows: The final target is test.flt.srt.bam, search for a rule that can produce this file. Discover that samtools_sort can produce the required file, and this rule requires input from samtools_flt. Therefore, samtools_flt executes first. samtools_flt needs input test.bam, check if the file exists. If it exists, follow the order samtools_flt &gt; samtools_sort &gt; all to execute the workflow. If not found, report a dependency error and stop the workflow. Due to this backtracking execution logic of workflows, it is very easy to implement breakpoint restarts in workflows: simply backtrack through each step’s dependencies to check if they are satisfied. If satisfied, start executing from the current step; if not, continue backtracking until finding the source or failing to find one and reporting an error. Additionally, snakemake checks output file updates and integrity during workflow execution to determine whether rules run successfully. If a rule fails, the result files of that rule are deleted by default, so unless the snakemake process is terminated abnormally, it is unlikely for a step to be incomplete while the workflow continues running. Built-in Useful Functions in snakemakeSince snakefiles can directly write Python code, people familiar with Python can easily use list comprehensions and other similar things to quickly generate information needed for batch processing. For those without much Python experience, snakemake also provides some built-in functions to quickly achieve similar functionality. For example: expand: A function used to generate string lists, equivalent to combining list comprehensions and formatted strings. By default, it combines multiple replacement variables in pairs, but it can be set to a mode similar to zip(), where it sequentially pairs them, which can meet different needs. samples = expand(&quot;&#123;sample_id&#125;.&#123;fq&#125;.fasta&quot;, sample_id=[&#39;sam1&#39;, &#39;sam2&#39;], fq=[&#39;fq1&#39;, &#39;fq2&#39;]) Batch Processing of Multiple Samples&#x2F;FilesBatch processing involves a built-in object in snakemake called wildcards. Although I am not very clear on how to translate this term… In general, this object is used for pattern matching with capture effects (regular expressions). For example, an example given in the official documentation: 1234567rule complex_conversion: input: &quot;&#123;dataset&#125;/inputfile&quot; output: &quot;&#123;dataset&#125;/file.&#123;group&#125;.txt&quot; shell: &quot;somecommand --group &#123;wildcards.group&#125; &lt; &#123;input&#125; &gt; &#123;output&#125;&quot; Here, the input and output of the rule contain &#123;dataset&#125; and &#123;group&#125;, which are two wildcard objects. Each wildcard can match almost any character (corresponding to a regular expression of .+). If a rule needs a file named file_path/file.001.txt, snakemake will find that this file’s naming matches the pattern specified in the output of complex_conversion. Therefore, snakemake considers file_path/file.001.txt to be produced by this rule and sets dataset to “file_path”, group to “001”, then passes these variables up to input, continuing dependency resolution. Thus, through dependencies, when the requested result files are named according to a certain rule (usually containing sample&#x2F;project IDs), you can use wildcard to match which steps each sample&#x2F;project needs to perform and automatically generate the complete workflow route before starting execution. snakemake &amp; condaTo ensure the repeatability of completed workflows, snakemake includes built-in functionality for using Conda to manage workflow dependencies.However, this feature is different from what I initially understood. snakemake &amp; ContainersIn addition to Conda, snakemake supports using containers to manage dependencies. However, this container is not Docker; it is Singularity. I only discovered that Docker is just one implementation of container counting when using snakemake. The key features of snakemake’s Singularity compared to Docker are: No need for root permissions Lower resource consumption This project started migrating from C++ to Go in version 3.0, and although I overcame network pressure to install it, due to distribution issues, I cannot use it normally at present, so I do not have practical experience with it. Problem Solutions CollectionThroughout my use of snakemake, I occasionally encounter some large problems. Since the number of Chinese users for this software is not many, although the English materials are comprehensive, due to my lack of relevant knowledge, I cannot quickly find solutions from them. Therefore, I have collected the problems and their solutions here. Some descriptions of the problems may not be professional; in the future, I will gradually correct them.","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"snakemake","slug":"snakemake","permalink":"https://silenwang.github.io/en/tags/snakemake/"}]},{"title":"bash-it","slug":"bash-it","date":"2018-10-29T21:17:23.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2018/10/29/bash-it/","permalink":"https://silenwang.github.io/en/2018/10/29/bash-it/","excerpt":"bash-it is a collection of bash plugins inspired by oh-my-zsh. Installing bash-it not only makes the bash interface more stylish but also makes the command line more convenient.","text":"bash-it is a collection of bash plugins inspired by oh-my-zsh. Installing bash-it not only makes the bash interface more stylish but also makes the command line more convenient. InstallationAccording to the installation tutorial on the bash-it homepage, it’s very easy to install. 12git clone --depth=1 https://github.com/Bash-it/bash-it.git ~/.bash_it~/.bash_it/install.sh However, when choosing, you need to pay attention that it will ask whether to retain the original .bashrc and .bash_profile. Of course, if you don’t retain these two files, they won’t be deleted; only backed up. If there are any problems after installation, you can manually switch back. Usagebash-it -h can display all available commands, and bash-it show plugins can show all available plugins. Currently, I have only used base and git, but I will gradually learn about the others later.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"bash","slug":"bash","permalink":"https://silenwang.github.io/en/tags/bash/"},{"name":"plugin","slug":"plugin","permalink":"https://silenwang.github.io/en/tags/plugin/"}]},{"title":"Python Command Line Parsing","slug":"Python命令行解析","date":"2018-10-22T12:12:50.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2018/10/22/Python命令行解析/","permalink":"https://silenwang.github.io/en/2018/10/22/Python%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%A7%A3%E6%9E%90/","excerpt":"I have been using argparse for command line parsing, but it wasn’t until recently that I discovered it can be used to create subcommands.","text":"I have been using argparse for command line parsing, but it wasn’t until recently that I discovered it can be used to create subcommands. General Argument ParsingGeneral argument parsing is quite simple. You just instantiate an argparse, and then call the object’s add argument method. 1234567891011import argparsefrom argparse import RawTextHelpFormatterparser = argparse.ArgumentParser( prog=&#x27;program&#x27;, description=&#x27;Some description&#x27;, formatter_class=RawTextHelpFormatter)parser.add_argument(&#x27;-s&#x27;, help=&quot;description for args&quot;, required=False, type=str) Subcommand CreationSubcommands need to first create a subparser object based on the main argument parser (above is parser), and then call this object’s method to add subcommands. 1234567sub_parser = parser.add_subparsers(title=&#x27;sub-commands&#x27;, description=&#x27;Some description&#x27;, help=&#x27;Some help description&#x27;)sub_parser_a = sub_parser.add_parser(&#x27;a&#x27;, help=&#x27;Some help description&#x27;)sub_parser_a.add_argument(&#x27;-s&#x27;, help=&quot;description for args&quot;, type=str) This way, you can define subprograms under the main program. argparse also provides a method to specify a function to run when a specific subcommand is called. This function will receive all parameter values under the subcommand as a namespace and can start executing the subprogram after parsing the parameters. 12345sub_parser_a.set_defaults(func=subFun)def countFun(args): args = vars(args) print(args) Otherargparse can also implement multiple subprograms and inherit main program parameters for subprograms. This has not been used yet, so I will update it later.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"argparse","slug":"argparse","permalink":"https://silenwang.github.io/en/tags/argparse/"},{"name":"Argument Parsing","slug":"Argument-Parsing","permalink":"https://silenwang.github.io/en/tags/Argument-Parsing/"}]},{"title":"Writing a Program Isn't Easy Either","slug":"写一个程序真不容易","date":"2018-10-18T22:19:45.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2018/10/18/写一个程序真不容易/","permalink":"https://silenwang.github.io/en/2018/10/18/%E5%86%99%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F%E7%9C%9F%E4%B8%8D%E5%AE%B9%E6%98%93/","excerpt":"","text":"There’s not much to say, just commemorating my first independent completion of a command-line program~","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"}]},{"title":"conda Install sscClust","slug":"conda安装sscClust","date":"2018-10-09T08:19:01.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2018/10/09/conda安装sscClust/","permalink":"https://silenwang.github.io/en/2018/10/09/conda%E5%AE%89%E8%A3%85sscClust/","excerpt":"It seems I hit a bug again…","text":"It seems I hit a bug again… I previously tried to install sscClust using conda, but it failed because of a bug. In certain cases, the R installed via conda might not be able to find the bioconductor packages installed via conda, leading to R reinstalling the dependencies when running. I recorded the complete installation method as follows (assuming conda is already installed): 123456789101112conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/mro/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/conda config --set show_channel_urls yes# After that, delete the &#x27;- defualt&#x27; in ~/.condarc. This step is very crucial!conda install -y bioconductor-SingleCellExperiment bioconductor-scran bioconductor-SC3 bioconductor-zinbwave bioconductor-BiocParallel r-base r-devtools r-rcolorbrewer r-rtsne r-class r-factoextra r-cowplot r-data.table r-ggplot2 r-mass r-rjson r-cluster r-ks r-fields r-doparallel r-plyr r-igraph r-densityclust r-e1071 After that, supplement the packages not included in R: 123456options(repos=&quot;https://mirrors.shu.edu.cn/CRAN/&quot;)options(BioC_mirror=&quot;http://mirrors.ustc.edu.cn/bioc/&quot;)source(&quot;http://bioconductor.org/biocLite.R&quot;)install.packages(c(&#x27;RhpcBLASctl&#x27;, &#x27;ADPclust&#x27;, &#x27;varSelRF&#x27;))options(unzip = &quot;internal&quot;)devtools::install_github(&quot;SilenWang/sscClust&quot;, dependencies=F, ref=&quot;dev&quot;) Finally, when using devtools::install_github, make sure to set the parameter to not install dependencies. It might be due to issues with the build files, forcing a reinstallation of dependencies? Or is it because the package versions do not meet the requirements?","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"conda","slug":"conda","permalink":"https://silenwang.github.io/en/tags/conda/"}]},{"title":"Testing Conda Inside Docker","slug":"docker内进行conda测试","date":"2018-10-06T19:01:12.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2018/10/06/docker内进行conda测试/","permalink":"https://silenwang.github.io/en/2018/10/06/docker%E5%86%85%E8%BF%9B%E8%A1%8Cconda%E6%B5%8B%E8%AF%95/","excerpt":"When installing software, I thought that the portability and feasibility of using conda were better, so I decided to try it out and see how Snakemake works…","text":"When installing software, I thought that the portability and feasibility of using conda were better, so I decided to try it out and see how Snakemake works… Creating a Docker Container and Installing Miniconda Initialize a CentOS 7 container and install minicoda because the container is newly initialized, so there’s nothing installed… need to install some necessary things. 123456docker run --name &quot;conda_test&quot; -dti IMAGE_ID /bin/bashdocker exec -it DOCKER_ID /bin/bashyum -y install wget bzip2.x86_64 vimwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.shsh Miniconda3-latest-Linux-x86_64.shsource ~/.bashrc # conda modified the environment variables, manually make it effective Installing R and sscClust This time, we want to simulate a regular user installing sscClust (root access is not available), so all necessary packages will not be installed using yum. R packages will be installed within R itself, not using the binary versions provided by conda (to prevent issues with packages not included in conda or dependency problems). Set up the conda mirror and then install R. 1234567conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/conda config --set show_channel_urls yesconda install r Install necessary packages. Since conda does not provide the compilation components, we still need to compile some parts ourselves. 12345wget ftp://xmlsoft.org/libxml2/libxml2-2.7.2.tar.gz ftp://ftp.gnu.org/gnu/gsl/gsl-2.5.tar.gz https://www.openssl.org/source/openssl-1.1.1.tar.gz https://curl.haxx.se/download/curl-7.61.1.tar.gztar xvf openssl-1.1.1.tar.gztar xvf gsl-2.5.tar.gztar xvf libxml2-2.7.2.tar.gz Extract and enter the corresponding directories to compile and install, note that before compiling, add prefix=/path/to/install, then configure the PATH to add the installation path of the above software at the beginning of PATH, so it will prioritize reading from there. curl needs to be installed before libgit2. Install sscClust within R. 1234options(repos=&quot;https://mirrors.shu.edu.cn/CRAN/&quot;)options(BioC_mirror=&quot;http://mirrors.ustc.edu.cn/bioc/&quot;)install.packages(&quot;BiocInstaller&quot;,repos=&quot;https://bioconductor.org/packages/3.7/bioc&quot;)install.packages(&quot;devtools&quot;) Testing Pause During the testing process, various dependency issues were encountered. Since conda does not provide the compilation components needed for *-devel series packages, if all build dependencies need to be compiled from scratch, it will take a lot of time. Change in Approach For newer packages that cannot be obtained from R source or Bioconductor, check their dependency situation and install the dependency items using conda before installing the package within R. 123456789101112yum install -y unzip wget curl make gcc gcc-gfortran gcc-c++ bzip2.x86_64 vimconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/conda config --set show_channel_urls yesconda install -y -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/ bioconductor-SingleCellExperiment bioconductor-scran bioconductor-SC3 bioconductor-zinbwave bioconductor-BiocParallelconda install -y r-RColorBrewer r-Rtsne r-class r-factoextra r-cowplot r-data.table r-ggplot2 r-MASS r-rjson r-cluster r-ks r-fields r-doParallel r-plyr r-igraph r-densityClust r-e1071 r-devtools Install some packages within R. 123456options(repos=&quot;https://mirrors.shu.edu.cn/CRAN/&quot;)options(BioC_mirror=&quot;http://mirrors.ustc.edu.cn/bioc/&quot;)install.packages(c(&#x27;varSelRF&#x27;, &#x27;RhpcBLASctl&#x27;, &#x27;ADPclust&#x27;))options(unzip = &quot;internal&quot;)source(&quot;http://bioconductor.org/biocLite.R&quot;)devtools::install_github(&#x27;SilenWang/sscClust&#x27;, dependencies=FALSE, ref=&quot;dev&quot;) Even though the R package installed using conda still has dependency issues, it will be unable to load (dependencies are not fully installed). ConclusionInstalling R packages without root access is particularly troublesome. It’s simpler to install necessary components first and then compile within R itself, or for administrators to set up Docker and give usage permissions to the user, allowing them to operate within Docker themselves.&#96;&#96;&#96;","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"conda","slug":"conda","permalink":"https://silenwang.github.io/en/tags/conda/"},{"name":"docker","slug":"docker","permalink":"https://silenwang.github.io/en/tags/docker/"}]},{"title":"Single Cell Related Notes","slug":"单细胞相关记录","date":"2018-09-28T21:35:18.000Z","updated":"2026-02-10T14:03:13.688Z","comments":true,"path":"2018/09/28/单细胞相关记录/","permalink":"https://silenwang.github.io/en/2018/09/28/%E5%8D%95%E7%BB%86%E8%83%9E%E7%9B%B8%E5%85%B3%E8%AE%B0%E5%BD%95/","excerpt":"Because of my work, I had to learn some basic concepts and methods for single-cell research. I wrote this note to record it.","text":"Because of my work, I had to learn some basic concepts and methods for single-cell research. I wrote this note to record it. Overall ApproachIn general, single-cell sequencing is a research method that considers the differences between cells by using cells as the unit for sequencing. Currently, what I see are mainly single-cell transcriptome sequencing, not genome sequencing. The experimental methods I have seen so far for implementing single-cell sequencing are: Using flow cytometry to sort cells and then performing subsequent reactions in individual wells The single-cell sequencing solution provided by 10X Genomics. Although it has a hint of flow-based principles, instead of putting each cell in an individual well, it delivers the cells into GEM droplets, adds barcodes within the droplets, and then sequences them on Illumina machines From a principle standpoint, these two approaches differ in library construction and splitting. The former essentially involves selecting cells through flow cytometry and performing reactions and library construction individually in each well. Therefore, it treats each cell as an individual sample and mixes them for sequencing On the other hand, the latter adopts a special library construction method using GEM-based experimental systems. Before constructing the library, it tags each cell’s nucleic acid with labels (16nt 10X Barcode and 10nt UMI), then mixes all the nucleic acids together to construct the library. In this way, the nucleic acids under one Illumina sample index actually come from different cells. Cell data is split based on the barcodes on the nucleic acids, and due to the presence of UMIs, they can be used for duplicate processing in subsequent steps Technical RouteFlow Cytometry-Based Approach Since it essentially follows a classic method for constructing transcriptome libraries, but uses flow cytometry to reduce sample size to individual cells, the software&#x2F;technological route under this approach is basically no different from that of single-sample methods. However, one may need to pay attention to issues such as duplicates and running efficiency 10X GEM-Based Approach Due to the use of a special library construction method, the most convenient way is to use the cellranger software suite provided by 10X Genomics. This software covers the entire analysis process from data demultiplexing to generating expression matrices and some downstream analyses (dimensionality reduction visualization, cell clustering, differential expression, marker selection), making it very convenient Downstream Analysis ApproachCell classification&#x2F;clustering -&gt; Differential expression -&gt; Marker selection -&gt; Association?","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"single cell","slug":"single-cell","permalink":"https://silenwang.github.io/en/tags/single-cell/"}]},{"title":"Git Usage Notes","slug":"Git使用笔记","date":"2018-09-28T12:34:12.000Z","updated":"2026-02-10T14:03:13.683Z","comments":true,"path":"2018/09/28/Git使用笔记/","permalink":"https://silenwang.github.io/en/2018/09/28/Git%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/","excerpt":"","text":"I had read about Git tutorials a long time ago, but I never actually used it. Now that I’m using it, I feel like I’ve become a programmer in some way! Operation Method Records User Initialization, when first using on a new device: 123git config --global user.name &quot;Silen Wang&quot; # Set the user namegit config --global user.email &quot;mymail@gmail.com&quot; # Set the user emailgit config --global core.editor &#x27;vim&#x27; # Set the editor used for commit messages Clone a project’s entire code: 1git clone PROJ_URL Pull updates 12git pull # Applicable when cloning a project, if there are multiple sources, specify which source to pull fromgit pull origin master # Specify pulling the master branch of the origin remote repository Push updates 12git pushgit push origin master # Specify pushing to the master branch of origin, if not specified, it might use a default setting? Not sure exactly what happens. Remote management 12git remote # View remote informationgit remote add REMOTE_URL # Add remote repository information Modify commits, Git stores local file modifications in a staging area. To submit these changes, first confirm the commit modifications. 123git add FILE # Stage new files or mark changes to files as ready for submissiongit commit # Commit all uncommitted modifications with a simple description of what was modified. This description can be used to trace back previous actions.git commit -a # Directly commit all uncommitted modifications with the same description Branch operations, the concept of branches allows creating a backup of the current project (a copy), then making modifications on this branch without affecting the original branch (e.g., master). Once the modified branch is tested and has no issues, merge the changes into the branch that needs modification. 123git checkout -b dev # Create and switch to the dev branchgit checkout dev # Switch to the dev branchgit merge dev # Merge changes from the dev branch into the current branch","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"git","slug":"git","permalink":"https://silenwang.github.io/en/tags/git/"}]},{"title":"Gogs Deployment","slug":"Gogs部署-md","date":"2018-09-21T18:32:00.000Z","updated":"2026-02-10T14:03:13.683Z","comments":true,"path":"2018/09/21/Gogs部署-md/","permalink":"https://silenwang.github.io/en/2018/09/21/Gogs%E9%83%A8%E7%BD%B2-md/","excerpt":"Bought a VPS; the monthly traffic usage is so low that it’s really a waste… So let’s make better use of it. After all, it costs $5 per month.","text":"Bought a VPS; the monthly traffic usage is so low that it’s really a waste… So let’s make better use of it. After all, it costs $5 per month. Deployment StepsGogs is a lightweight Git hosting service implemented in Go. Its resource consumption is so small that it can even run on a Raspberry Pi. I previously set it up once but didn’t document the process. This time, I had to look it up again… Overall, using Docker for deployment (it’s simple), so basically just configuring a file and running a few commands. The installation of Docker isn’t explained here; there are instructions in ArchWiki. The main installation command is: 1docker pull docker.io/gogs/gogs Then prepare a docker-compose.yml file with the following content, execute it in the directory containing this file, and start using Gogs. The access port for Gogs is 10080. 12345678910111213version: &quot;2&quot;services: gogs: image: docker.io/gogs/gogs restart: always ports: - &quot;10022:22&quot; - &quot;10080:3000&quot; volumes: - /var/gogs:/data Then to enable passwordless push, you need to set up SSH. By default, Gogs doesn’t have the SSH server enabled. You need to edit /var/gogs/gogs/conf/app.ini and modify it as follows (this is my configuration file directory; adjust according to the actual mount path in volumes): 12345678[server]DOMAIN = http://XXX.XXX.XXX.XXXHTTP_PORT = 3000ROOT_URL = http://localhost:10080/DISABLE_SSH = falseSSH_PORT = 10022START_SSH_SERVER = trueOFFLINE_MODE = false Then set up the SSH keys as you would with GitHub. Solving the Issue of Adding Multiple Keys (update@20180921)Although Gogs has a graphical interface for setting up SSH keys, there might be a bug in the version I pulled. When adding the first key, it works fine, but when trying to add more than one key from multiple machines, even though it shows as added in the graphical interface, it can’t normally log in via SSH with passwordless access. You need to further set up: Since my Docker container mounts /data to a directory outside the container, there’s no need to log into the Docker system and modify directly. The corresponding file is: 挂载目录/gogs/git/.ssh/authorized_keys. Add public key content according to the format. For example: 1command=&quot;/app/gogs/gogs serv key-1 --config=&#x27;/data/gogs/conf/app.ini&#x27;&quot;,no-port-forwarding,no-X11-forwarding,no-agent-forwarding,no-pty ssh-rsa AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA test Save it and you can log in without restarting the container.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"gogs","slug":"gogs","permalink":"https://silenwang.github.io/en/tags/gogs/"},{"name":"vps","slug":"vps","permalink":"https://silenwang.github.io/en/tags/vps/"}]},{"title":"Gabe Newell is awesome!!!!!!!!!!!!!!!","slug":"G胖牛逼","date":"2018-09-07T20:12:02.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2018/09/07/G胖牛逼/","permalink":"https://silenwang.github.io/en/2018/09/07/G%E8%83%96%E7%89%9B%E9%80%BC/","excerpt":"This must be a historic moment!","text":"This must be a historic moment! A week ago, I read news that Steam’s latest Linux version integrated a feature called Steam Play, which combined Wine and Proton, allowing users to run Windows games directly on Linux. They also provided a list of the first 27 games to be fully compatible (when this was written). At the time, I didn’t care much because I thought this technology was still in development and would take a long time to mature. But last night before bed, feeling bored, I decided to try Steam……………………………………… ……………………………………………………………………. Wow, it’s amazing, truly super amazing!!!!!!!!!!!!!!!!!!! I was running an old version of Xian Jie 4 that I had previously purchased. Just by clicking the mouse, I could play the game! On a cheap laptop with just Core M5! F*ck…… my mood hasn’t calmed down for hours. How many ordinary users are unable to play enough games on Windows and end up being stuck with it, or delete Linux after trying dual-booting? This situation is expected to change dramatically in the next year and a half! GTMD windows!","categories":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/categories/Gaming/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://silenwang.github.io/en/tags/linux/"},{"name":"steam","slug":"steam","permalink":"https://silenwang.github.io/en/tags/steam/"}]},{"title":"R Small Tips Collection","slug":"R小问题收集","date":"2018-09-07T12:01:44.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2018/09/07/R小问题收集/","permalink":"https://silenwang.github.io/en/2018/09/07/R%E5%B0%8F%E9%97%AE%E9%A2%98%E6%94%B6%E9%9B%86/","excerpt":"Working with R often involves encountering various issues. Here are some collected tips.","text":"Working with R often involves encountering various issues. Here are some collected tips. Data Frame OperationsExtracting a Single Column&#x2F;Row Without Returning a Vector When using df[] to subset a data frame, if you extract a single column or row, or delete all but one column or row, the resulting object will be a vector instead of a data frame. You need to specify the parameter to ensure it returns a data frame. This method also applies to matrices. 1df[c(1, 2, 3), -2, drop = F] Merging Data Using row.names as Key Many Chinese materials describe merging data by using one column as the key for row-wise merging. However, you can also use row.names as the key, but after merging, row.names will become a column of data. You need to specify row.names again. 12345# Two-step processmerge_data &lt;- merge(x, y, all.x = T, by = 0)merge_data &lt;- data.frame(merge_data), row.names = 1)# One-step mergemerge_data &lt;- data.frame(merge(x, y, all.x = T, by = 0), row.names = 1) OtherR’s Python Dict Alternative Many online resources use lists combined with the match function to implement dictionary-like functionality. However, in R, named vectors can be used to achieve key-value mapping, which is very convenient. However, it’s unclear whether this approach is efficient. 1234a &lt;- c(1, 2, 3)names(a) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)tmp &lt;- a[&quot;a&quot;]print(tmp)","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"}]},{"title":"hexo syntax highlighting plugin","slug":"hexo高亮主题插件","date":"2018-09-05T21:53:50.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2018/09/05/hexo高亮主题插件/","permalink":"https://silenwang.github.io/en/2018/09/05/hexo%E9%AB%98%E4%BA%AE%E4%B8%BB%E9%A2%98%E6%8F%92%E4%BB%B6/","excerpt":"I found that the default theme of Hexo is not very appealing, so I wanted to find a bright one and tried using a plugin.","text":"I found that the default theme of Hexo is not very appealing, so I wanted to find a bright one and tried using a plugin. The plugin can be found here. Installation is quite simple; just run npm i -S hexo-prism-plugin. Then, open the blog’s configuration file _config.yml, disable the built-in highlighting, and add the settings for this plugin. 1234567highlight: enable: falseprism_plugin: mode: &#x27;preprocess&#x27; # realtime/preprocess theme: &#x27;default&#x27; line_number: false # default false custom_css: &#x27;path/to/your/custom.css&#x27; # optional After that, update as usual. This plugin supports multiple highlighting modes and custom CSS. The supported languages are listed on the above link.&#96;&#96;&#96;","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://silenwang.github.io/en/tags/hexo/"},{"name":"syntax","slug":"syntax","permalink":"https://silenwang.github.io/en/tags/syntax/"}]},{"title":"R Install GitHub Package Timeout Issue Solution","slug":"R安装git包出现timeout问题","date":"2018-09-05T08:57:17.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2018/09/05/R安装git包出现timeout问题/","permalink":"https://silenwang.github.io/en/2018/09/05/R%E5%AE%89%E8%A3%85git%E5%8C%85%E5%87%BA%E7%8E%B0timeout%E9%97%AE%E9%A2%98/","excerpt":"When installing packages from GitHub in R, sometimes you may encounter a connection timeout issue with GitHub. Here’s how to solve it.","text":"When installing packages from GitHub in R, sometimes you may encounter a connection timeout issue with GitHub. Here’s how to solve it. Essentially, this involves loading the curl package and using curl to download the package. This might only work on Linux systems. 123options(download.file.method = &quot;libcurl&quot;)library(&#x27;curl&#x27;)install_github(PACKAGENAME) If the installed package still cannot return an object, try manually downloading the package and then modifying the source code to install directly from the source. 1install.packages(path_to_file, repos = NULL, type=&quot;source&quot;)","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"}]},{"title":"Docker Usage Record","slug":"docker使用记录","date":"2018-09-02T20:15:02.000Z","updated":"2026-02-10T14:03:13.685Z","comments":true,"path":"2018/09/02/docker使用记录/","permalink":"https://silenwang.github.io/en/2018/09/02/docker%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/","excerpt":"To make good use of the old, broken device… I decided to set up a server to accumulate experience. Considering the portability of the things I will be setting up, using Docker in containers is convenient for migration and usage.","text":"To make good use of the old, broken device… I decided to set up a server to accumulate experience. Considering the portability of the things I will be setting up, using Docker in containers is convenient for migration and usage. InstallationThe environment system uses Manjaro, since this is what I am most familiar with. And ArchWiki can provide help for many strange problems. Installing Docker is very simple: 123456789sudo pacman -S docker# To allow ordinary users to use Docker, you need to add the corresponding user group and add the user to the group.sudo groupadd dockersudo gpasswd -a $&#123;USER&#125; docker# Alternatively, you can use the `usermod` command:sudo usermod -Ga docker $&#123;USER&#125;# If it has already been enabled, restart it; no need to enable it again.systemctl start dockersystemctl enable docker To speed up image (images) downloads, set up a domestic mirror source (registry mirror) for Docker Hub. The configuration file is /etc/docker/daemon.json. After configuring, execute sudo systemctl restart docker: 123&#123; &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn/&quot;]&#125; Image Creation and Internal OperationsPrepare to install R and the sscClust package in a container. The company’s server is CentOS6, so pull the corresponding image: 1docker pull centos:6 However, after pulling the above image, it could not start normally. According to here, this might be an issue with the official image. So I switched to pulling centos:7, which worked fine after changing versions. 123docker run -dti IMAGE_ID /bin/bash# Log in to Docker for operations. Note that when logged in, you are root; operate carefully...docker exec -it CONTAINER_ID /bin/bash After logging in, install vim and set up basic environment variables. Then start installing R. 1234567891011yum -y install epel-release# Change the image sourceyum install wgetmv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bakmv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.bakwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repowget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo# Different image sources have different package versions, delete existing cache to prevent errors.yum clean allyum -y makecacheyum -y install R Install some system software packages required for R package installation. 1yum install -y openssl-devel libcurl-devel libxml2-devel gsl-devel After entering R, start installing sscClust (installing R will automatically install a lot of dependencies; installing this package will have even more dependencies…) 123456options(&quot;repos&quot; = c(CRAN=&quot;https://mirrors.ustc.edu.cn/CRAN/&quot;))options(BioC_mirror=&quot;http://mirrors.ustc.edu.cn/bioc/&quot;)source(&quot;http://bioconductor.org/biocLite.R&quot;)install.packages(&quot;devtools&quot;)biocLite(c(&#x27;SingleCellExperiment&#x27;, &#x27;scran&#x27;, &#x27;SC3&#x27;, &#x27;zinbwave&#x27;, &#x27;BiocParallel&#x27;))devtools::install_github(&quot;Japrin/sscClust&quot;) If there are no special network issues, sscClust should install smoothly. Finally, save the changes made to the Docker container as an image. After saving, it will display a series of sha256 values. Use docker images to see the saved images. 1docker commit CONTAINER_ID r/sscclust","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://silenwang.github.io/en/tags/docker/"}]},{"title":"SGE System Usage Notes","slug":"SGE系统使用笔记","date":"2018-08-30T09:36:38.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2018/08/30/SGE系统使用笔记/","permalink":"https://silenwang.github.io/en/2018/08/30/SGE%E7%B3%BB%E7%BB%9F%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/","excerpt":"When using SGE at a new company, the approach is somewhat different. Here are some notes.","text":"When using SGE at a new company, the approach is somewhat different. Here are some notes. Script WritingAfter writing the script, it is not specified when submitting; instead, it is written into the script to be submitted and then directly qsub shell.sh. 123456789#！/bin/bash#$ -S /bin/bash // Indicates that this script is for bash#$ -V // Passes all environment variables from the current command#$ -cwd // Sets the current path as the working directory#$ -N WorkName // Task name#$ -o WorkName.log // Task output log file name#$ -j y // Specifies whether the standard error stream of the task is merged with the standard output stream [yes] noshell script Submitting a script containing the above content is essentially equivalent to: 1234567qsub -S /bin/bash \\ -V \\ -cwd \\ -N WorkName \\ -o WorkName.log \\ -j y \\ shell script qlogin UsageFor testing purposes, you can use qlogin to log in to a compute node. Then, the operations performed on the compute node environment are consistent with submitting a script using qsub.&#96;&#96;&#96;","categories":[{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"}],"tags":[{"name":"sge","slug":"sge","permalink":"https://silenwang.github.io/en/tags/sge/"}]},{"title":"Pandas Operation Notes","slug":"Pandas操作笔记","date":"2018-08-29T12:16:24.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2018/08/29/Pandas操作笔记/","permalink":"https://silenwang.github.io/en/2018/08/29/Pandas%E6%93%8D%E4%BD%9C%E7%AC%94%E8%AE%B0/","excerpt":"Recently, I’ve been using Pandas a lot and have compiled some commonly used functions for future reference.","text":"Recently, I’ve been using Pandas a lot and have compiled some commonly used functions for future reference. Data Reading Basic reading can be done directly with read_csv, which involves several parameters. 1234import pandas as pddf = pd.read_csv(file_path, sep=&quot;\\t&quot;, index_col=0, header=0)# index_col is equivalent to row.names in R; it specifies a column to use as the row labels and removes that column from the data. If not specified, a 0-length numeric label will be generated.# header functions are different from R&#x27;s logic; by default, header=0, which means the first row is used as the header. If you don&#x27;t want a header, use header=None. If you specify another row as the header, rows above that header will be discarded. When dealing with large datasets to prevent memory overflow, it’s necessary to read the file in chunks. 1234567891011import pandas as pd# reader is a generatorreader = pd.read_csv(file_path, sep=&quot;\\t&quot;, iterator=True, chunksize=1000)# iterator / chunksize specifying either will generate an iterator. If you specify chunksize, you don&#x27;t need to write iterator.# If reader was specified with chunksize, you can directly call get_chunk()# To obtain a specific number of rows; otherwise, use get_chunk(num), which I haven&#x27;t thoroughly tested.# Note: The row count here refers to the data rows and does not include the header because when generating reader,# it defaults to using the first row as the header. Therefore, all subsequent generated dfs have headers.chunk = reader.get_chunk()df = pd.DataFrame(chunk) Additionally, Pandas can intelligently recognize whether a file is compressed (by extension) and read&#x2F;write files with specified delimiters without issues. Deprecated `read_table` I don’t remember from which version onwards, read_table was deprecated in favor of read_csv, which is said to have better performance for CSV files.However, it’s quite annoying that bioinformatics fields often use tsv files or similar formats… Data Writing My most commonly used formats are tsv and xlsx. Plain Text File12345678```## Write to Excel- The method is very simple:```pythondata.to_excel(FILE, sheet_name=&#x27;SHEET&#x27;) Note that if you want to write multiple data tables to multiple sheets in one go, you can do this: 1data.to_excel(FILE, sheet_name=[&#x27;SHEET1&#x27;, &#x27;SHEET2&#x27;]) Subset Selection Similar to R, Pandas supports various subset selection methods. The logic for some selections is similar to R, but there are differences. Generally, whatever can be achieved in Pandas can also be done in R, but the reverse may not be true. Here, I only record methods similar to R’s selection logic. 123456789import pandas as pddf = pd.read_table(file_path, sep=&quot;\\t&quot;)# Use .loc[row, col] for selecting rows and columns based on labels.df = df.loc[[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;], [&#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;]]# Note that unlike R, when selecting a single column, there should be no space before the comma. When selecting a single row, you can leave the space after the comma.df = df.loc[:, [&#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;]]df = df.loc[[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;], ]# Corresponding to .loc is .iloc, which uses row/column indices instead of labels. Note that it starts from 0. Row and Column DeletionIteration Processing","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"Pandas","slug":"Pandas","permalink":"https://silenwang.github.io/en/tags/Pandas/"}]},{"title":"Installing R Packages with Conda and Configuring Jupyter","slug":"使用conda安装R包","date":"2018-08-28T22:37:57.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2018/08/28/使用conda安装R包/","permalink":"https://silenwang.github.io/en/2018/08/28/%E4%BD%BF%E7%94%A8conda%E5%AE%89%E8%A3%85R%E5%8C%85/","excerpt":"Due to work requirements, I needed to install a package mentioned in a literature. Initially, I thought it would be done with two commands, but who knew there were so many dependencies to resolve… and R packages on Linux need to be compiled, which is quite time-consuming… Fortunately, there’s Conda!","text":"Due to work requirements, I needed to install a package mentioned in a literature. Initially, I thought it would be done with two commands, but who knew there were so many dependencies to resolve… and R packages on Linux need to be compiled, which is quite time-consuming… Fortunately, there’s Conda! Speaking of which, when reading WeChat public accounts before, I didn’t know why they recommended using Conda to install necessary tools… This experience has made me realize how annoying the dependencies in Linux can be… And as a normal user without full permissions, many paths are restricted… Conda’s method is very useful, even though it wastes some space. Conda Installation Conda comes in two versions… I don’t know if they can be called distributions. One is Anaconda, and the other is Miniconda. You can think of them as full installation and minimal installation. Anaconda comes with a complete Python environment, even Jupyter is included, while Miniconda only includes the basic Python and not much else. Since I mainly use it to install R, I chose Miniconda. The downloaded Linux installation file is an exceptionally large shell script. Running it will start the installation. After installation, you can use the conda command to install what you want. For example, I can start installing R. 1conda install r Everything installed using Conda is uniformly placed in the Conda installation directory. The folder structure inside seems a bit different from the root directory. When initially completing the Conda installation, if no special instructions are given, Conda will modify the original ~/.bashrc file and add the Conda bin directory to the PATH variable. However, its priority is after the system default directories (/bin). Since I want to use Conda’s configuration to override the system defaults, I manually added this line to ~/.bash_profile and placed Conda’s bin directory at the front of the system defaults. Conda also has mirror sites in China. Edit ~/.condarc (create it if it doesn’t exist) and add the following content to use the USTC mirror (Tsinghua’s reportedly has issues, so I didn’t use it). Of course, this is just an example; under pkg, there are other sources as well. Add them if needed. 1234channels: - https://mirrors.ustc.edu.cn/anaconda/pkgs/free/ - defaultsshow_channel_urls: true After that, execute R and install the relevant packages inside. The installed R packages will not be placed in ~/R but will also be entirely in Conda’s directory. Refer to the installation output for specifics. Since R packages need to be compiled, I used Miniconda, so there were unsatisfied dependencies. Those packages can be installed using conda install. Additionally, you can check all available versions on Anaconda’s website and then execute the corresponding commands. This time, with Conda’s assistance, most dependency issues were resolved. The only one that couldn’t be resolved was compiled locally and uploaded… It took two days to finally package it up… I hope there won’t be any problems when using it. Jupyter Installation and ConfigurationR is on the cluster, which makes debugging somewhat inconvenient since I’m used to writing and testing in RStudio or Jupyter Notebook. Fortunately, Conda can also install Jupyter, so I searched for remote access configuration methods. First, use Conda to install Jupyter. 1conda install jupyter After completion, configure Jupyter. 1234# Generate the configuration filejupyter notebook --generate-config# Set a password. Follow the prompts and enter twice.jupyter notebook password Open the generated configuration file with an editor (for regular users, it’s usually ~/.jupyter/jupyter_notebook_config.py, as indicated by the prompt when generating the file). Find the following items, uncomment them, and change the corresponding values. The password is in the JSON file mentioned during generation; there will be a long string inside. Copy and paste it all over. 1234c.NotebookApp.ip=&#x27;*&#x27;c.NotebookApp.password = u&#x27;your_key_str&#x27;c.NotebookApp.open_browser = False # Represents starting the notebook service without opening the browser and accessing it.c.NotebookApp.port = 8888 # This can be omitted; a port will be automatically assigned. Then, you can open your browser on any computer that can connect to the cluster IP with 0.0.0.0:8888 and access it by entering the password. Due to work requirements, I needed to install a package mentioned in a literature. Initially, I thought it would be done with two commands, but who knew there were so many dependencies to resolve… and R packages on Linux need to be compiled, which is quite time-consuming… Fortunately, there’s Conda! Speaking of which, when reading wechat public accounts before, I didn’t know why they recommended using Conda to install necessary tools… This experience has made me realize how annoying the dependencies in Linux can be… And as a normal user without full permissions, many paths are written forbidden… Conda’s method is very useful, even though it wastes space. Conda Installation Conda comes in two versions… I don’t know if they can be called distributions. One is Anaconda, and the other is Miniconda. You can think of them as full installation and minimal installation. Anaconda comes with a complete Python environment, even Jupyter is included, while Miniconda only includes the basic Python and not much else. Since I mainly use it to install R, I chose Miniconda. The downloaded Linux installation file is an exceptionally large shell script. Running it will start the installation. After installation, you can use the conda command to install what you want. For example, I can start installing R. 1conda install r Everything installed using Conda is uniformly placed in the Conda installation directory. The folder structure inside seems a bit different from the root directory. When initially completing the Conda installation, if no special instructions are given, Conda will modify the original ~/.bash file and add the Conda bin directory to the PATH variable. However, its priority is after the system default directories (/bin). Since I want to use Conda’s configuration to override the system defaults, I manually added this line to ~/.bash_profile and placed Conda’s bin directory at the front of the system defaults. Conda also has mirror sites in China. Edit ~/.condarc (create it if it doesn’t exist) and add the following content to use the USTC mirror (Tsinghua’s reportedly has issues, so I didn’t use it). Of course, this is just an example; under pkg, there are other sources as well. Add them if needed. 1234channels: - https://mirrors.ustc.edu.cn/anaconda/pkgs/free/ - defaultsshow_channel_urls: true After that, excute R and install the relevant packages inside. The installed R packages will not be placed in ~/R but will also be entirely in Conda’s directory. Refer to the installation output for specifics. Since R packages need to be compiled, I used Miniconda, so there were unsatisfied dependencies. Those packages can be installed using conda install. Additionally, you can check all available versions on Anaconda’s website and then execute the corresponding commands. This time, with Conda’s assistance, most dependency issues were resolved. The only one that couldn’t be resolved was compiled locally and uploaded… It took two days to finally package it up… I hope there won’t be any problems when using it. Jupyter Installation and ConfigurationR is on the cluster, which makes debugging somewhat inconvenient since I’m used to writing and testing in RStudio or Jupyter Notebook. Fortunately, Conda can also install Jupyter, so I searched for remote access configuration methods. First, use Conda to install Jupyter. 1conda install jupyter After completion, configure Jupyter. 1234# Generate the configuration filejupyter notebook --generate-config# Set a password. Follow the prompts and enter twice.jupyter notebook password Open the generated configuration file with an editor (for regular users, it’s usually ~/.jupyter/jupyter_notebook_config.py, as indicated by the prompt when generating the file). Find the following items, uncomment them, and change the corresponding values. The password is in the JSON file mentioned during generation; there will be a long string inside. Copy and paste it all over. 1234c.NotebookApp.ip=&#x27;*&#x27;c.NotebookApp.password = u&#x27;your_key_str&#x27;c.NotebookApp.open_browser = False # Represents starting the notebook service without opening the browser and accessing it.c.NotebookApp.port = 8888 # This can be omitted; a port will be automatically assigned. Then, you can open your browser on any computer that can connect to the cluster IP with 0.0.0.0:8888 and access it by entering the password.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"},{"name":"conda","slug":"conda","permalink":"https://silenwang.github.io/en/tags/conda/"},{"name":"jupyter","slug":"jupyter","permalink":"https://silenwang.github.io/en/tags/jupyter/"}]},{"title":"Using minicap to Capture Screenshots from a Phone","slug":"使用minicap从手机中截图","date":"2018-08-28T12:05:17.000Z","updated":"2026-02-10T14:03:13.687Z","comments":true,"path":"2018/08/28/使用minicap从手机中截图/","permalink":"https://silenwang.github.io/en/2018/08/28/%E4%BD%BF%E7%94%A8minicap%E4%BB%8E%E6%89%8B%E6%9C%BA%E4%B8%AD%E6%88%AA%E5%9B%BE/","excerpt":"Practicing automated phone operations inevitably involves capturing the current image of the phone. The phone’s built-in screenshot is too slow, so replacing it with minicap.","text":"Practicing automated phone operations inevitably involves capturing the current image of the phone. The phone’s built-in screenshot is too slow, so replacing it with minicap. Software Compilation123456789# Clone the project code and initializegit clone https://github.com/openstf/minicapcd minicap/git submodule initgit submodule update# Compile, ndk is required (archlinuxcn source has it, just install directly)sudo pacman -S android-ndk# Compile, after installation, it won&#x27;t be added to bin automatically, so run with absolute path/opt/android-ndk/ndk-build Software Deployment to Phone12345678910111213141516# To use adb connect to the device, if not installed first install adb# manjaro community has it, I remember the official version is自带的, now using the community edition may be due to minimalization considerations not default installingsudo pacman -S android-tools# Turn on USB debugging on the phone and connect to the computer, if device is not registered in phone select allow current device debuggingadb devices# Get phone basic information to push corresponding version of minicap to the phoneABI=$(adb shell getprop ro.product.cpu.abi | tr -d &#x27;\\r&#x27;)SDK=$(adb shell getprop ro.build.version.sdk | tr -d &#x27;\\r&#x27;)adb push libs/$ABI/minicap /data/local/tmp/adb push jni/minicap-shared/aosp/libs/android-$SDK/$ABI/minicap.so /data/local/tmp/adb shell chmod 777 /data/local/tmp/minicap# Test if it can runadb shell LD_LIBRARY_PATH=/data/local/tmp /data/local/tmp/minicap -P 1080x1920@1080x1920/0 –t# If running successfully, the last will display &#x27;OK&#x27; Screenshot123# The environment variable setting is necessary because it needs to specify the library path, -P后面的参数是截图参数, 详情可参见minicap的项目网页, -s but表截图并输出到# 标准输出, 所以重定向就好adb shell LD_LIBRARY_PATH=/data/local/tmp /data/local/tmp/minicap -P 1080x1920@1080x1920/0 –s &gt; /sdcard/minicap/test.jpg To be completed Wrap the above commands into a script, and set up wifi connection information at the same time, so you can control multiple devices simultaneously.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"android","slug":"android","permalink":"https://silenwang.github.io/en/tags/android/"},{"name":"adb","slug":"adb","permalink":"https://silenwang.github.io/en/tags/adb/"},{"name":"minicap","slug":"minicap","permalink":"https://silenwang.github.io/en/tags/minicap/"}]},{"title":"Use Sqlite3 in Python","slug":"Python使用Sqlite3","date":"2018-08-26T23:54:24.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2018/08/26/Python使用Sqlite3/","permalink":"https://silenwang.github.io/en/2018/08/26/Python%E4%BD%BF%E7%94%A8Sqlite3/","excerpt":"To prepare for future operations using Sqlite3, I recently tried using it to operate on databases at work. I decided to use sqlite3 because it seemed convenient. However, I found that not all SQL statements are supported by sqlite3, and its free-form shell commands cannot be called directly by the Python sqlite module…","text":"To prepare for future operations using Sqlite3, I recently tried using it to operate on databases at work. I decided to use sqlite3 because it seemed convenient. However, I found that not all SQL statements are supported by sqlite3, and its free-form shell commands cannot be called directly by the Python sqlite module… Simple Usage1234567import sqlite3 # Import the moduleconn = sqlite3.connect(&#x27;test.db&#x27;) # Connect to the database (it will be created if it doesn&#x27;t exist)cursor = conn.cursor() # Create a pointer from the database objectcursor.execute(&#x27;sql cmd&#x27;) # Pass SQL statements to the database, note that operations are not immediately saved/synchronized to the databasecursor.close() # Close the pointerconn.commit() # Confirm changesconn.close() # Close the connection Usage Notes: Only standard SQL statements are supported. Since sqlite is being used, unsupported SQL syntax cannot be used, and sqlite-specific shell commands cannot be used either because these commands are essentially executed in the shell, not as SQL statements. If you must use shell commands, you can use the os module or another module that can call the shell, effectively executing shell commands. Not all operations need to be completed through the pointer object. Some can be done using the database object itself, which can save a few lines of code. See the introduction on the Coder’s Tutorial website for more details. Remember to confirm changes (obj.commit()), otherwise the content in the database will not change. I tried using the with statement to connect, but it doesn’t automatically close the connection outside the with statement scope (obj.close()), so you still need to write it explicitly. Calling Shell Commands1234567891011121314151617181920import os, textwrap# This is where textwrap is used to adjust shell command indentation for better readability when outputting. It&#x27;s not necessary.def cmd_gen(cmd_str): cmd_str = textwrap.dedent(cmd_str) cmd_str = cmd_str.strip() return cmd_strdb_file = &#x27;test.db&#x27;index_file = &#x27;index.txt&#x27;# Generate the command, this is the only valid method I found online. The specific meaning will be explained later...cmd = cmd_gen(&#x27;&#x27;&#x27; sqlite3 &#123;db_file&#125; &lt;&lt; EOF .separator &quot;\\\\t&quot; .import &#123;index_file&#125; idx_tab EOF &#x27;&#x27;&#x27;.format(db_file=db_file, index_file=index_file))os.system(cmd)","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"sqlite","slug":"sqlite","permalink":"https://silenwang.github.io/en/tags/sqlite/"}]},{"title":"Desktop Migration","slug":"桌面叒迁移","date":"2018-08-26T21:26:54.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2018/08/26/桌面叒迁移/","permalink":"https://silenwang.github.io/en/2018/08/26/%E6%A1%8C%E9%9D%A2%E5%8F%92%E8%BF%81%E7%A7%BB/","excerpt":"","text":"After using Linux for a while, I often come across a famous quote: “Life goes on, fiddling never stops.” However, to be honest, after getting a job last year, due to the company being stingy and not providing a computer, I had to use my own computer for work… really不敢乱折腾 (I still remember the despair of losing my thesis papers on Baidu Cloud Disk…). But after changing jobs, putting everything on Baidu Cloud Disk… I was once again tempted. Actually, this time the fiddling wasn’t due to being annoyed; rather, it was because the current system was actually having problems —- inexplicable screen flickering.I don’t remember when this started, but my laptop’s screen would randomly and irregularly black out and flash. Sometimes there were horizontal snowflakes flashing. At first, I thought it might be related to the warranty expiration… after all, it was bought from a factory store (really cheap…), and when I got the computer, it seemed like it had been refurbished (the box was somewhat damaged, and the screws on the back of the laptop were worn out). Then, after using it for three months, the keyboard backlight failed. But after observing for some time, I found that this flickering was quite peculiar because it happened more when an external screen was connected or when using a browser; if not using a browser and not connecting an external screen, it almost never happened. Moreover, the flickering occurred in ordinary users, but if logged in as root, no matter how you played around, it wouldn’t happen…So I concluded that I must have accidentally modified some configuration files causing software issues, so I decided to reinstall (absolutely not because contacting the merchant for warranty was ignored~). Since we’re reinstalling, and since this laptop is no longer used for work, why don’t we try something new? Actually, I’ve been watching Deepin’s DDE for a long time… to be honest… it’s always better to know your own people… I feel that the Deepin team really understands the needs of domestic users. From the very beginning, it was just fancy but had lots of bugs and didn’t allow customization (at first, it was really pretty, but there were many bugs and you couldn’t customize it), but now it still looks good, and its functionality, usability, and performance have improved… it’s really made great progress. As a result, Deepin has gained quite a few users internationally (Deepin’s international community also has many posts, and the Manjaro community maintenance distribution board often discusses Deepin). Although I’ve always downloaded the latest Deepin community edition live CD to try it out whenever there was an update, it wasn’t until recently that I realized that some XFCE shortcuts and features were not available in Deepin. Moreover, before this version, Deepin’s resource consumption, fluidity, and stability were far inferior to XFCE, so I always tried for a night but ultimately didn’t install it on my real machine. This time it’s different, one thing is that I don’t need to use my personal laptop for work anymore, secondly, I’m relatively free now, so there’s no lack of time to adapt to the new operation mode. Plus, Deepin updated to version 15.7 recently, and Manjaro’s Deepin source has also been updated. So I decided to format my computer and start using it. I have to say… in the first few hours, the differences were still not too great, but after two days, I gradually got used to it… so people… once they accept a set… By the way, I also discovered something really cool: Deepin had touchpad gesture support last year. It supports three-finger maximize and restore windows, as well as quickly splitting windows left and right. This gesture can greatly alleviate the discomfort of not being able to use super+mouse drag window. Note that this gesture is built into DDE and does not conflict with libinput-gesture, so if you install libinput-gesture again, both gestures will fight for control. The current DDE gesture configuration file is /usr/share/dde-daemon/gesture.json, which is in JSON format, feeling easier to understand than libinput-gesture. Still, here are my desktop screenshots from the past: Mint (Cinnamon) Manjaro (Cinnamon) Manjaro (Xfce) Manjaro (Deepin)","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"manjaro","slug":"manjaro","permalink":"https://silenwang.github.io/en/tags/manjaro/"},{"name":"deepin","slug":"deepin","permalink":"https://silenwang.github.io/en/tags/deepin/"},{"name":"de","slug":"de","permalink":"https://silenwang.github.io/en/tags/de/"}]},{"title":"Job Change Memoir","slug":"换工作小记","date":"2018-08-14T22:41:00.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2018/08/14/换工作小记/","permalink":"https://silenwang.github.io/en/2018/08/14/%E6%8D%A2%E5%B7%A5%E4%BD%9C%E5%B0%8F%E8%AE%B0/","excerpt":"Leaving one job for another, I wonder if it’s always going to be a struggle…","text":"Leaving one job for another, I wonder if it’s always going to be a struggle… In July 2017, I finally bid farewell to my student life and officially joined the workforce. My position was a bioinformatics engineer at Tianjin Nuohe Zhiyuan. In August 2018, after barely managing to complete the handover of tasks, I finally left Nuohe after one year and one month. My new company is Hangzhou Ruipei Gene, and I am still working as a bioinformatics engineer. I don’t have many thoughts about my previous employer. Although I really worked like crazy since getting promoted early, I did learn something and felt that my efforts were rewarded. However, after accidentally learning about the IT department’s situation… well, let me shout out my frustration loudly. And wish all my fellow colleagues who are still stuck in this pit to find better opportunities soon and get out of it.","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[]},{"title":"Regular Expression Symbols Collection","slug":"正则匹配符号收录","date":"2018-07-29T02:04:01.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2018/07/29/正则匹配符号收录/","permalink":"https://silenwang.github.io/en/2018/07/29/%E6%AD%A3%E5%88%99%E5%8C%B9%E9%85%8D%E7%AC%A6%E5%8F%B7%E6%94%B6%E5%BD%95/","excerpt":"Regular expressions are still quite useful in many situations. However, there are some differences in how they are written across different programs, so it’s a good idea to make a cheat sheet for reference!","text":"Regular expressions are still quite useful in many situations. However, there are some differences in how they are written across different programs, so it’s a good idea to make a cheat sheet for reference! Basic General PartTable source: yirlin’s blog \\: Escapes the next character, which can be a special character, an ordinary character, a backreference, or an octal escape. For example, n matches the character n. \\n matches a newline character. The sequence \\\\ matches \\, and \\( matches (. ^: Matches the start of the input string. If the Multiline property is set on the RegExp object, ^ also matches \\n or \\r after positions. $: Matches the end of the input string. If the Multiline property is set on the RegExp object, $ also matches \\n or \\r before positions. *: Matches zero or more occurrences of the preceding subexpression. For example, zo* can match z and zoo. * is equivalent to &#123;0,&#125;. +: Matches one or more occurrences of the preceding subexpression. For example, zo+ can match zo and zoo, but not z. + is equivalent to &#123;1,&#125;. ?: Matches zero or one occurrence of the preceding subexpression. For example, do(es)? can match do or does in do. ? is equivalent to &#123;0,1&#125;. &#123;n&#125;: n is a non-negative integer. Matches exactly n occurrences. For example, o&#123;2&#125; cannot match the o in Bob, but it can match all os in food. &#123;n,&#125;: n is a non-negative integer. Matches at least n occurrences. For example, o&#123;2,&#125; cannot match the o in Bob, but it can match all os in foooood. &#123;1,&#125; is equivalent to +. &#123;0,&#125; is equivalent to *. &#123;n,m&#125;: m and n are non-negative integers, where n &lt;&#x3D; m. Matches at least n occurrences and no more than m occurrences. For example, o&#123;1,3&#125; will match the first three os in fooooood. &#123;0,1&#125; is equivalent to ?. Note that there should be no space between the comma and the two numbers. ?: When this character follows any other quantifier (*, +, ?, &#123;n&#125;, &#123;n,&#125;, &#123;n,m&#125;), it makes the matching pattern non-greedy. Non-greedy mode matches as few characters as possible in the searched string, while the default greedy mode matches as many characters as possible. For example, for the string oooo, o+? will match a single o, and o+ will match all os. .: Matches any single character except \\n. To match any character including \\n, use the pattern [.\\n]. (pattern): Matches pattern and captures this match. Captured matches can be obtained from the generated Matches collection, in VBScript using SubMatches collection, in JScript using $0…$9 properties. To match round bracket characters, use \\( or \\). (?:pattern): Matches pattern but does not capture the match result, i.e., a non-capturing group, which is useful when combining parts of a pattern with “or” characters. For example, industr(?:y|ies) is a more concise expression than industry|industries. (?=pattern): Positive lookahead, matches at the start position of any string that matches pattern. This is a non-capturing group, which means this match does not need to be captured for later use. For example, Windows (?=95|98|NT|2000) can match Windows 2000 in Windows 2000, but it cannot match Windows 3.1 in Windows 3.1. Lookahead does not consume characters, so after a match occurs, the search for the next match starts immediately after the last match without starting from the character following the lookahead. (?!pattern): Negative lookahead, matches at the start position of any string that does not match pattern. This is a non-capturing group, which means this match does not need to be captured for later use. For example, Windows (?!95|98|NT|2000) can match Windows 3.1 in Windows 3.1, but it cannot match Windows 2000 in Windows 2000. Lookahead does not consume characters, so after a match occurs, the search for the next match starts immediately after the last match without starting from the character following the lookahead. x|y: Matches x or y. For example, z|food can match z or food. (z|f)ood then matches zood or food. [xyz]: Character set. Matches any one of the characters contained within it. For example, [abc] can match a in plain. [^xyz]: Negative character set. Matches any character not contained within it. For example, [^abc] can match p in plain. [a-z]: Character range. Matches any character within the specified range. For example, [a-z] can match any lowercase letter from a to z. [^a-z]: Negative character range. Matches any character not within the specified range. For example, [^a-z] can match any character that is not a lowercase letter from a to z. \\b: Matches a word boundary, i.e., the position between a word and a space. For example, er\\b can match er in never, but it cannot match er in verb. \\B: Matches a non-word boundary. er\\B can match er in verb, but it cannot match er in never. \\cx: Matches the control character specified by x. For example, \\cM matches a Control-M or carriage return. The value of x must be A-Z or a-z; otherwise, c is treated as an ordinary c character. \\d: Matches any digit character. Equivalent to [0-9]. \\D: Matches any non-digit character. Equivalent to [^0-9]. \\f: Matches a form feed. Equivalent to \\x0c and \\cL. \\n: Matches a newline character. Equivalent to \\x0a and \\cJ. \\r: Matches a carriage return character. Equivalent to \\x0d and \\cM. \\s: Matches any whitespace character, including spaces, tabs, form feeds, etc. Equivalent to [\\f\\n\\r\\t\\v]. \\S: Matches any non-whitespace character. Equivalent to [^\\\\f\\\\n\\\\r\\\\t\\\\v]. \\t: Matches a tab character. Equivalent to \\x09 and \\cI. \\v: Matches a vertical tab character. Equivalent to \\x0b and \\cK. \\w: Matches any word character, including underscores. Equivalent to [A-Za-z0-9_]. \\W: Matches any non-word character. Equivalent to [^A-Za-z0-9_]. \\xn: Matches n, where n is a two-digit hexadecimal escape value. The hexadecimal escape value must be exactly two digits long. For example, \\x41 matches A. \\x041 is equivalent to \\x04 &amp;1. Regular expressions can use ASCII encoding. \\num: Matches num, where num is a positive integer. A reference to the captured match. For example, (.)&#39;\\1 matches two consecutive identical characters. \\n: Identifies an octal escape value or a backreference. If \\n has at least n captured subexpressions before it, then n is a backreference. Otherwise, if n is an octal digit (0-7), then n is an octal escape value. \\nm: Identifies an octal escape value or a backreference. If \\nm has at least nm captured subexpressions before it, then nm is a backreference. If \\nm has at least n captured subexpressions, then n is a backreference followed by the letter m. If none of the above conditions are met, if both n and m are octal digits (0-7), then \\nm matches the octal escape value nm. \\nml: If n is an octal digit (0-3) and both m and l are octal digits (0-7), then it matches the octal escape value nml. \\un: Matches n, where n is a four-digit hexadecimal representation of a Unicode character. For example, \\u00A9 matches the copyright symbol (?)&#96;.","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"Regular Expressions","slug":"Regular-Expressions","permalink":"https://silenwang.github.io/en/tags/Regular-Expressions/"},{"name":"Perl","slug":"Perl","permalink":"https://silenwang.github.io/en/tags/Perl/"},{"name":"Bash","slug":"Bash","permalink":"https://silenwang.github.io/en/tags/Bash/"}]},{"title":"System Migration Record","slug":"系统迁移记录","date":"2018-07-29T00:58:36.000Z","updated":"2026-02-10T14:03:13.691Z","comments":true,"path":"2018/07/29/系统迁移记录/","permalink":"https://silenwang.github.io/en/2018/07/29/%E7%B3%BB%E7%BB%9F%E8%BF%81%E7%A7%BB%E8%AE%B0%E5%BD%95/","excerpt":"Since I started using Linux desktop distributions in my sophomore year, I have been constantly switching between different发行 versions. After a few years of fiddling around, I finally settled on Manjaro.","text":"Since I started using Linux desktop distributions in my sophomore year, I have been constantly switching between different发行 versions. After a few years of fiddling around, I finally settled on Manjaro. Mint Migration RecordDue to Ubuntu crashing, during the process of reinstalling Ubuntu, I decided to switch to the world’s most popular Linux distribution - Mint. It is said to be more user-friendly and comes with everything out of the box. However, practice makes perfect… For someone who was already used to Unity, it wasn’t that simple… Installation went smoothly, almost identical to Ubuntu’s liveCD. Initially, I thought about not formatting the home directory, but after realizing that yesterday’s Ubuntu still had issues, I decided to bite the bullet and format it. After copying some files back, it took me a while to get everything set up. After logging into the Mint interface, the login window was really more beautiful than Ubuntu, with ultra-high-definition clear photos instead of dark tones. However, after logging in, the interface… It took half a year to get used to Unity’s layout, switching back to GNOME&#x2F;Win-like interfaces was really annoying. Besides the different layouts, Mint’s display effects didn’t feel as good as Ubuntu for me. Chinese fonts felt blurry, possibly due to Noto Sans? After switching to WenQuanYi Microhei, most places improved, but it still wasn’t perfect. Baidu said that WenQuanYi Microhei tends to be blurry when the font size is small… So I directly increased all the font sizes. Input Method SettingsMint doesn’t come with any input methods by default. Although Firefox has them, without an input method, you can’t fully type in English… The input method settings are under Language Settings, although it lists supported input methods, there are no packages available in the software sources, so they cannot be installed directly from the Software Center. PPA installation code: 123sudo add-apt-repository ppa:fcitx-team/nightlysudo apt-get updatesudo apt-get install fcitx After installation, some dependencies are needed. I didn’t use the PPA to install, instead, I went to Language -&gt; Input Method and selected Fcitx to directly install the necessary dependencies. After installation, download Sogou from the Sogou website and install it. Restart (logout whether works or not), and you can use the input method. When reinstalling, I copied my previous Ubuntu’s &#x2F;home over, but the Sogou input method’s dictionary didn’t seem to be retained. Software Installation PPA installation: VirtualBox GoldenDict Shadowsocks-Qt5 PlayOnLinux Okular WizNote R-base Numix-circle PPA addition code 1sudo add-apt-repository ppa:hzwhuang/ss-qt5 ppa:wiznote-team ppa:numix/ppa R-base directly added the source information deb http://mirror.bjtu.edu.cn/cran/bin/linux/ubuntu trusty/ to the software sources 1sudo gedit /etc/apt/sources.list PPA installation 1sudo apt-get install VirtualBox GoldenDict Shadowsocks-Qt5 PlayOnLinux Okular WizNote R-base numix-icon-theme-circle kolourpaint Deb installation mendeley haroopad Rstudio bcloud wps Configuration MigrationBackup the original &#x2F;home, after reinstalling, copy it over. Most software configurations will be retained (Firefox plugins, configuration lost, reason unknown) Program SettingsSince I was already used to Unity, I made some adjustments to the Mint interface to lower the adaptation difficulty. Panel moved up User小程序 placed at the far left Firefox Settings and PluginsDue to Firefox configuration loss, I’m recording related settings here to prevent them from being lost again next time. Using plugins: Adblock Plus FxoyProxy Hide Cpation Titlebar Plus Stylish Stylish setting (another post has it, repeating the record) 1234567891011@-moz-document regexp(&quot;https?://(?!.+\\\\.coursera\\\\.org).*&quot;)&#123;*:not([class*=&quot;icon&quot;]):not(i)&#123;font-family: &quot;WenQuanYi Microhei&quot; !important;&#125; span.fui-logo-text&#123;font-family: &quot;IterCast-Icon&quot; !important;&#125; span.diffstat-bar&#123;font-family: &quot;octicons&quot; !important;&#125; span.diff-added&#123;font-family: &quot;octicons&quot; !important;&#125; #indexbutton&#123;font-family: &quot;html5test&quot; !important;&#125; .toolbox ins a&#123;font-family: &quot;2345&quot; !important;&#125; .tool_ul li a ins&#123;font-family: &quot;2345&quot; !important;&#125; .panel ins a&#123;font-family: &quot;2345&quot; !important;&#125; span.nba_matchnav_score&#123;font-family: &quot;Sans-Serif&quot; !important;&#125;&#125; Ubuntu Unity vs Mint Cinnamon Usage Comparison Display effects&#x2F;Interface layout Personally, I’m more used to Unity. Although Ubuntu’s default colors aren’t very appealing, after installing Numix themes and changing the wallpaper, it still looks quite nice. Unity places the program panel on the left, and the top panel for integrating title bars. This design is indeed saving a lot of space on 16:9 screens, allowing more content to be displayed. And Ubuntu’s default fonts and icons are quite large, making them easier to adapt after switching back to Windows or Mint. The display effects of Ubuntu also seem better than Mint, possibly because the fonts are larger, using WenQuanYi Microhei won’t be blurry… Usability Mint is indeed more user-friendly than Ubuntu. It’s no wonder it has the highest click-through rate. Mint provides a higher degree of graphicalization in its settings center, integrating many more options compared to Ubuntu. The login window software sources can be managed very conveniently through GUI. File operations have right-click menu options to open with root privileges without needing command-line operations. Of course… once you know the path, dragging and dropping is actually quite fast. Details are also very贴心, for example, when opening a manager with root privileges, there will be special prompts, avoiding the confusion that Ubuntu users often encounter. Mint’s panel extensions and theme switching are also very convenient, with resources directly downloadable without needing to find them separately. Of course, because I migrated from Unity back then, some habits have a significant impact on usability. If it were for someone who directly migrated from Windows, Mint would be extremely user-friendly. Then I think in terms of software updates, Mint really does a great job… It sets special protection for core parts, preventing users from easily updating the kernel, which is very friendly to me as a half-newbie with强迫症 (I don’t know if my previous Ubuntu was because of an update that crashed). Small problemsFrom 3 years ago when I first used various Ubuntu-based distributions, small issues have never stopped. Due to the large user base of Windows, font乱码 is completely unavoidable. Besides some scattered issues that may not affect usage… we can only wait for future fixes. Ubuntu-related small problems Dual graphics card, independent graphics card driver (could not be solved) Touchpad (turned on by default at boot) Screen brightness (could not remember the last brightness setting) Error reports (unknown error prompts that don’t have any actual impact) Occasionally freezes when watching videos for a long time Occasionally lags, I’m not sure why it lags (not many programs running) Mint-related small problems Dual graphics card, independent graphics card driver Screen brightness Occasionally lags Sleep&#x2F;suspend cannot wake up Fonts always feel a bit blurry Some wireless mice automatically go to sleep Firefox download menu clicking on open file location makes Firefox freeze Interface ReorganizationSometimes, when I’m bored, I do boring things. Recently, for some reason, I felt that looking at my desktop wasn’t very satisfying and decided to start tweaking.Basically, it boils down to preferring Unity’s layout style, although many people find this strange, after accepting this setting… really feels quite cool…. Unfortunately, the graphics card driver I couldn’t figure out. At first, my Mint layout was like this: Basically, it was like Win (and XP) under the menu-&gt;quick start-&gt;open windows, notification bar. Initially, when switching from Ubuntu to Windows, I still retained the window button left placement, but after repeatedly mis-clicking the menu, I eventually switched back to the right… But using it for a while felt that the position of the open windows in the middle was too small, and with four or more window tabs, they would be partially obscured, making it difficult to switch between them…. So I tried using dock tools to replace the toolbar’s quick start, resulting in this layout:Ah, a little bit like Unity! But not as good as Unity…..I tried using the toolbar mode, but docky was blocked by the system自带 toolbars….This layout used for a few days and then I found problems, after all, quite many software have buttons or menus on the left side…. Habitually flying my mouse over would often mistakenly open other programs…. So, I still put docky at the bottom…&#x3D; &#x3D;|||Ah….So my desktop layout became similar to most GNOME2-era users…….Was this arrangement really tested?……So my struggles were meaningless?…God… Mint to ManjaroFrom Ubuntu-based to Arch-basedI started trying Linux systems from my junior year, and I’ve always been using Ubuntu and its derivative distributions. This time, switching to a Manjaro based on Arch is quite a big change. Actually, before the switch, I was very anxious, after all, I’m known for being blacklisted…. And although I’ve used Linux for a long time, I’m still considered a beginner in Linux, so I was afraid that during the process, I might cause serious problems and crash my computer….. However, from this experience, things went well. To date, it has been exactly one week since switching systems, and all the commonly used software I need have been successfully installed (including games), there haven’t been any severe issues hindering use (although there are many small bugs scattered around…), and I plan to continue running for two more months. If everything goes well in the next two months, I might consider uninstalling Mint. Why Choose ManjaroBecause it’s simple! Although I was recommended to switch to Arch-based systems, since I’m not very familiar with Linux, I didn’t consider installing the original Arch directly, instead, I searched for a popular Arch derivative on DistrioWatch, which is now what I’m using - Manjaro.Although true Arch fans might think this belongs to heresy… because Manjaro doesn’t follow Arch’s principle of being as minimalistic as possible and leaving everything up to the user, but for me, who wants to experience Pacman but don’t want to spend too much time reading Arch Wiki, Manjaro is a very good solution… Installation PlanSince this was my first time using a Linux system other than Ubuntu-based ones, I naturally chose a dual-boot installation plan to leave myself an escape route.My laptop’s hard drive uses GPT partitioning, and I only installed one Linux Mint, no Windows at all. Solving the boot problem was quite convenient, during installation, I set Manjaro to install its Grub2 into the existing EFI partition, then manually deleted the Linux Mint’s EFI files in the EFI partition.There were two reasons for directly deleting: From Mint’s Grub2, it couldn’t correctly boot Manjaro, but Manjaro’s Grub2 could correctly boot Mint, the reason was unclear, possibly because Mint’s version was too low? I didn’t know how to let the system automatically choose to use Manjaro’s installed Grub2, so I just deleted Mint’s and left only Manjaro’s.Overall, the installation went quite smoothly, similar to installing Ubuntu or Mint, just a few mouse clicks were needed. However, after the first installation, I tried fiddling with the graphics card drivers, and then the desktop resolution became abnormal and couldn’t be restored, so I reinstalled it again…. Manjaro(Cinnamon Edition) vs Linux Mint ComparisonDesktop EnvironmentManjaro supports multiple desktop environments, to reduce my migration resistance, I chose the version with Cinnamon. So although they are different systems, their appearance looks almost the same, in terms of default installed software, there are some differences: Some system options and driver management need to be done using “Manjaro Setting Manager”, while Mint’s settings are all integrated into the settings panel. In Mint 17, language and input method were located under the same setting item, allowing simultaneous management of language and input method. In Manjaro, there is only an independent language setting item in “Manjaro Setting Manager” with no input method management. For dual graphics card computers, Manjaro defaults to installing bumblebee, and there are no Nvidia Prime-related software packages in the software library, speculation that Arch-based systems use bumblebee for dual graphics card switching. The package manager software is different from the source update software. Manjaro does not have a default font management program. Calculator programs are different. Chinese LocalizationIn this aspect, Manjaro is much better than Mint. After selecting Chinese during installation, most of the Chinese localization was completed (including Gimp &#x2F; Firefox &#x2F; Libre Office). And for subsequent installed software with Chinese language packages, they would also be installed together, such as Okular &#x2F; Kolour Paint these KDE-based software.While Mint 17 often has incomplete Chinese localization, after installing the system, only some common menu items are Chinese. Gimp &#x2F; Firefox &#x2F; Libre Office are all English, and you need to reinstall the language package yourself. And for subsequent installed software, they will always be in default English. Font EffectsWithout any configuration, personally, I feel that Manjaro’s font effects are slightly worse than Mint. This was a surprise to me because I previously used the Live CD to experience Manjaro’s KDE version, and the fonts looked quite comfortable. Using the system settings’ font item for simple adjustments still felt like it wasn’t as good as Mint, feeling that Chinese characters had special blurriness… However, this can be solved by installing fontconfig-ubuntu and freetype2-ubuntu packages, after installation, you can get consistent display effects with Mint. Common Software AcquisitionA system based on Ubuntu, one based on Arch, naturally have different software package management. Arch is loved by many users for its almost everything available and always keeping the latest in its software repositories.The official repository and AUR indeed have what software there is, all the software I used in U-series systems can be found in both the official repository and AUR. And like Rstudio &#x2F; sogou-pinyin &#x2F; WPS &#x2F; Steam etc., some software previously required downloading deb packages or adding PPA sources for installation, but after switching to Manjaro, a single command or clicking a few times in Pamac could solve it.However, this convenience is established on the condition of good network environment.In my school’s campus network, which is often unstable and slow, theoretically with a download speed of 512kb&#x2F;s, actual usage is generally only 100-200kb&#x2F;s, if downloading from overseas websites, the speed could be as low as 50kb&#x2F;s… So the software installation experience was really quite bad…… especially when installing from AUR, even though you don’t need to confirm, most packages require entering a password to get permissions. Because of the slow download speed and long time, it’s not possible to automatically install while installing software, I have to stand by one by one entering passwords…… and sometimes it can’t be installed successfully……Good that there is an “archlinuxcn” source provided by Arch enthusiasts for domestic users, which has many packages from AUR that download very fast and install without any errors. According to ArchWiki’s guidance to add “archlinuxcn” source after adding it, I successfully installed most of the commonly used software.Worth mentioning is that during the installation process, I found that not all software in the Arch repository are kept up to date. For example, R.The official Arch repository provides version 3.2.4 of R, while the latest version is 3.3.0. Although AUR’s r-kml is the latest 3.3.0 version, I was afraid it might cause installation issues so I didn’t consider using it.If using Mint, you can easily get the latest R through adding third-party PPAs, but in Arch-based systems, this cannot be done, so in the end, I chose to compile and install it, see the detailed installation process in my previous another post. Graphics Card DriversLinux’s dual graphics card drivers have always been a problem I couldn’t figure out, which was also the main reason for me to migrate from the original Ubuntu to Mint: being able to install graphics card drivers in one click.Perhaps Manjaro shares similar design ideas with Mint, so in terms of graphics card drivers, it does quite well. The system installs all drivers by default and there are no major issues when using them. However, I used Nvidia-prime for switching graphics cards in Mint, while Manjaro seems to only use Bumblebee. Recently, since I only play炉石传说和火炬之光2, I don’t really need the dedicated GPU, so I haven’t bothered with it… Input MethodManjaro doesn’t have a dedicated input method management item, directly installing input methods from the library is fine. It’s true that the Sogou Pinyin feels very good! Although I was used to Rime in Mint before, overall, it still isn’t as convenient as Sogou! Pray that fcitx won’t disappear again! I switched systems and you can’t let me down!!!!!! Small Bugs and IssuesI don’t know if my face is really black, but from the moment I started using Linux, various small bugs have never stopped. This time, after switching from Mint to Manjaro, a part of it was also because Firefox’s Flash said it crashed, and I couldn’t find a solution……..However, I switched to Manjaro……still encountered various small bugs continuously…..Organized them, the small bugs and issues encountered since switching to Manjaro are: Leanote default editor font is abnormal (solved after upgrading the kernel) Vivaldi installing flash causes flashing (gave up using it) Startup prompt: [failed] failed to start setup virtual console (already solved) Startup without mouse pointer, re-login shows pointer (already solved) Using screenshot program’s area screenshot function, the program does not respond (not solved) unzip solves乱码 invalid (already solved) Font blurry (already solved) Steam cannot be started directly (already solved) Neemo cannot lock to docky after starting, because docky recognizes it as root manager (not solved) Rstudio cannot input Chinese (not solved) Telegram cannot input Chinese (already solved) PDF does not display Chinese (already solved) Desktop display is abnormal, only low resolution can be used (reason unknown, reinstallation solves) Text dragging and copying, dragging cannot cross windows through the panel (not solved) Rename does not support regular expressions (already solved) Global shortcuts occasionally fail (not solved) …There are so many problems, that my friends said, you’re not using Arch, but a version of Arch filled with bugs…However, above issues did not significantly affect usage, and my computer can normally hibernate&#x2F;suspend, the failure rate of global shortcuts is also not as severe as in Mint. So overall, I successfully completed the migration from Mint to Manjaro. Learning ResourcesArch Wiki’s detailed and meticulous content has always been praised by all Arch users and some non-Arch users. After using one week of an Arch derivative, I have a bit of experience with this too. But for me, compared to Ubuntu and its derivatives, searching for solutions to problems in Arch is much more difficult.For example, the problem of Vivaldi installing Flash causing flashing, I directly searched “Arch Vivaldi Flash flashing” on Chinese&#x2F;Bing&#x2F;Google but couldn’t find any useful results……Considering that Vivaldi is a small crowd within a small crowd, so changing “Vivaldi” to “Chrome” &#x2F; “Chromium”, still couldn’t find any useful results……This was quite awkward…Actually, in Mint, I directly searched Chinese, if I really couldn’t find anything, then I would search English. After all, Mint is based on Ubuntu, and the number of users using Ubuntu domestically is still quite a lot, most of the time you can find effective solutions. Even if you can’t find an effective solution, through the Chinese search results, you can get a preliminary understanding of the problem, and then try to search for English results to find the solution.Switching to Manjaro isn’t like that, not only are there few domestic users of Manjaro (no灌水 in forums&#x2F;bbs), but even the active degree of Archlinux Chinese forum&#x2F;bbs is far from Ubuntu, so the difficulty of searching in Chinese is really high by a notch or two.Although many Linux problems are similar, using “Linux” as a keyword can get some problem solutions, but it’s not as convenient and quick as directly using “Ubuntu” as a keyword. After Migration ReflectionOver one week, I can now complete all the things I previously completed on Manjaro, although there are still small bugs (like before…), there are also many surprises (Sogou works, hibernate&#x2F;suspend works normally) supporting me to continue using it. Looking forward to Manjaro staying stable for the next two months…….As for the kernel……should I update or not update good? AcknowledgmentHerein, I solemnly thank the person who introduced me into this! Having someone who has used it before is always more reassuring~ Desktop Screenshot RemembranceActually, the screenshots really don’t show much difference, after all, both are Cinnamon, and both have similar themes… Mint screenshot Manjaro screenshot Remembrance (Extra Installed Software and Package List)Official Repository docky virtualbox steam fcitx fcitx-sogoupinyin steam-manjaro steam-native kdegraphics-okular kdegraphics-kolourpaint texmaker uget qbittorrent flashplugin perl-rename gnome-font-viewer goldendict Archlinuxcn telegram-desktop mendeleydesktop rstudio-desktop-bin shadowsocks-qt5 atom-editor AUR crossover Download and Use Leanote Unipro Ugene VirtualBox Extension Pack Compiled Installation R-3.3.0 Manjaro Interface Settings (manjaro)Today I saw a video on YouTube about Manjaro Deepin 16.08 experience, I couldn’t help but download DDE to try. It turns out that the Deepin team has put a lot of effort into making software, the latest Deepin 15.3 is quite smooth, and the entire interface design is simple and beautiful (in my opinion, second only to KDE…), feeling very promising~However, since I’ve been using it for almost half a year now, switching to DDE feels so empty… Besides, since it’s still in development, there are fewer plugins and configurable options (well… for me, it’s through GUI configuration) compared to KDE. So although it looks beautiful… 15 minutes later, I uninstalled it and continued with XFCE… …However… after watching the young and pretty one, returning to my original environment… cough… feels a bit uncomfortable…(I’m also a fickle old man…) so I started various theme changes again… while browsing Pamac’s software, I accidentally found a plugin: xfce4-windowck-plugin, this plugin can put the title bar and title buttons on the panel, which finally reminded me of Unity… Thinking about what I said earlier, I still like Unity’s layout. Unfortunately, I couldn’t find a better solution when configuring it before. With this plugin, although it can’t reach Unity’s effect (Unity simultaneously integrates the title bar and menu to the panel), it at least saves some space!Specific plan as follows: Install xfce4-windowck-plugin Put the plugin on the top panel In the window manager settings, set the window maximized to hide the title bar Set all buttons to the left Move the menu button to the left panel, workspace indicator to the bottom panel Ding ding ding~ Manjaro Crashed AfterwardCrashed so suddenly, had to die bigWhen I migrated, I thought… with my luck, it would crash at least once within half a year, and sure enough, it did…On Tuesday 2nd, for an unknown reason, the GUI couldn’t enter, stuck at the Manjaro Logo loading place, switching to the console showed nothing… completely didn’t know what was wrong, then I urgently needed to use my computer, thinking that previously when reinstalling Mint, I retained home, after reinstalling and mounting home, all software settings and personal files wouldn’t be lost, so I thought about not reinstalling!Then I went! big!! dead!!!了!!!!Reinstalling entered the GUI instantly confused… my home was formatted!!!Speculation is that during reinstallation, I accidentally checked to format it…Ah… really a thunderbolt lightning strike… all my experimental data were in home without backup…After sulking for half an hour, I started looking for recovery methods, asked several stores on Taobao about data recovery services, but they were all Windows-based… really wanted to cry…Tried using photorec and testdisk for repair, but it didn’t work at all…On Tuesday night… finally realized the reality, had to start over…(you know what I mean…) Desktop Environment Change (don’t know how many times)Anyway, since the whole system needs to be reinstalled and reconfigured, so this time I wanted not only to change the desktop environment this time, but also to try something new. After all, I’ve always wanted to try the legendary Xfce… Fortunately, Xfce is officially supported by Manjaro, so I directly downloaded Manjaro Xfce prepared for a fresh reinstall. Installation ProcessManjaro’s various versions are quite simple to install… just click and click… as long as you don’t accidentally check to format it like me… Xfce vs CinnamonXfce is different from Unity or Cinnamon. Unity and Cinnamon aim to provide beautiful, user-friendly, and feature-rich operating interfaces, while Xfce prioritizes lightness as an important consideration. So the feeling when using them is quite different.In simple terms, Xfce gives a very very fast feeling! Whether it’s starting the computer or loading programs, the speed difference is really visible to the naked eye… This can only be felt after trying. When I switched from Mint to Manjaro Cinnamon, I felt that the speed had a noticeable improvement. Now switching from Cinnamon to Xfce, the speed has improved again. Now reopening Mint feels so slow that it’s almost unbearable…Lightweight &amp; Simple, Xfce fits these two points very well, but for me, who switched from Cinnamon, some small places being too simple made me feel uncomfortable: The volume control plugin on the panel is extremely simple… simple to the point where I often don’t know how to use this plugin… frequently accidentally adjusting the sound… unzip does not support -O parameter… causing zip files to be displayed in乱码. Keyboard shortcut settings do not have many preset schemes… need to first find out what command needs to be written to achieve the desired function (my laptop has no dedicated touchpad switch key, I must set up a shortcut myself)… The built-in file manager does not support forward and backward keys, and there is no corresponding option in the settings… additionally… this thing really looks nothing like Nemo. No disk (program for managing disk partitions and mounting)… instantly confused when setting automatic mount. Cannot find the mousepad (text editor) setting anywhere… staring at a white background with small black text made me feel uncomfortable… The built-in music player looks like it has many functions but completely doesn’t know how to use it… also does not support dragging to open files… No network settings page for system proxy, only VPN These issues are not very big, more belong to suddenly switching desktop environments and not adapting. As a user, I can choose to slowly change habits to adapt, on the other hand, actually, you can modify the environment according to your habits… after all, this is Linux, as long as you want to mess around, most of it can be achieved.For me personally, Xfce’s flying speed gave me enough motivation to continue using it. After all, fancy effects are nice, but there’s no object to look at, the feeling of being smooth and comfortable when using is more important… Revisiting Arch WikiWhen I switched from U-series to A-series, I felt that A-series system documentation wasn’t as rich as U-series. Now after using Manjaro for a month and a half, my opinion has changed. Speaking of this… the documentation is indeed richer than Arch Wiki… in the past month and a half, over 80% of problems could be found in it. Problems that weren’t in Arch Wiki were also very difficult to find.Actually, A-series documentation isn’t hard to find, but it’s not as user-friendly as U-series.When using U-series systems, materials online generally include directly runnable problem-solving commands or codes. As long as the description of the problem is consistent, you can copy those few lines of code and run them in the terminal. Arch Wiki is different, when writing, it assumes that the reader has some Linux basics. It writes about the reason for the problem and the solution, but rarely directly provides one-click runnable codes.From this angle, Arch is not suitable for beginners, but very good for learning Linux. System ConfigurationMy configuration principle is to尽量 fit my original work habits in Cinnamon, so I mainly reconfigured the panel (in the end, actually returned to Gnome2’s layout…), deleted some built-in software and replaced them with what I used in Cinnamon. Additionally, Steam’s runtime library had issues again in Xfce… previously using local runtime libraries in Cinnamon made all games run normally, but this didn’t work in Xfce. So I found another solution from Arch Wiki.Specifically as follows: Replace Volume Control Plugin Uninstall the built-in volume control plugin pa-applet, install xfce4-pulseaudio-plugin, manually add it to the panel (doesn’t support appearing in the notification bar) Install the above plugin, adjusting the volume will have two volume change notifications. Right-click on the plugin’s properties and turn off one (the other might be the system default… don’t know how to turn it off). Replace unzip package Install unzip-iconv (archlinuxcn repo) instead of unzip Set up touchpad switch shortcut key Solution from Arch Wiki: Touchpad_Synaptics#Software_toggle Go to the page, copy the script code and save it to a designated location. Then go to Xfce’s keyboard shortcut settings page for key binding. Replace file manager Install nemo (software source has it) Also install nemo-fileroller, nemo-previewer and nemo-share to ensure complete functionality Add disk management program Install gnome-disk-utility Replace text editor Uninstall mousepad, install gedit Replace music player Uninstall guayadeque, install pragha, netease-cloud-music Configure Chinese input method Follow the Arch Wiki instructions to declare environment variables for input methods, so most programs can normally input Chinese Steam cannot start Solution from:Arch Wiki:Steam_runtime_issues Follow the instructions to add environment variables. Unresolved Bugs Pamac cannot update Archlinuxcn source software, can be updated normally in command line. When using wireless network card, after waking up from sleep&#x2F;resume, the wireless network card cannot receive signals, must log out and re-login. Leanote ordinary editor font display is not quite normal. Lanting Ya Yuan font installation fails to display properly. Bluetooth headphones pause button is useless. Software List Installed software (repository):dockyvirtualboxsteamfcitxfcitx-sogoupinyinsteam-manjarosteam-nativekdegraphics-kolourpainttexmakerugetqbittorrentflashpluginperl-renamegnome-font-viewergoldendicttelegram-desktopmendeleydesktoprstudio-desktop-binrshadowsocks-qt5atom-editorinkscape AUR:crossover Download and use:ugeneleanote New Desktop Remembrance As usual, a new desktop screenshot. And show off the built-in drop-down console, actually using it feels quite cool~","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://silenwang.github.io/en/tags/linux/"},{"name":"manjaro","slug":"manjaro","permalink":"https://silenwang.github.io/en/tags/manjaro/"},{"name":"linuxmint","slug":"linuxmint","permalink":"https://silenwang.github.io/en/tags/linuxmint/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://silenwang.github.io/en/tags/ubuntu/"}]},{"title":"Notes on Drawing Maps with R","slug":"用R绘制地图的笔记","date":"2018-07-29T00:38:09.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2018/07/29/用R绘制地图的笔记/","permalink":"https://silenwang.github.io/en/2018/07/29/%E7%94%A8R%E7%BB%98%E5%88%B6%E5%9C%B0%E5%9B%BE%E7%9A%84%E7%AC%94%E8%AE%B0/","excerpt":"During my graduate studies, I had the need to display results using maps and used R to create them. I recorded the code from that time below.","text":"During my graduate studies, I had the need to display results using maps and used R to create them. I recorded the code from that time below. Epidemiological Data Map VisualizationCode SourceThe script code is mainly adapted from an article on Statistical Analysis:Drawing Maps of China and Displaying Epidemiological Data with R Code Body (Includes Comments)####loading packages Load necessary libraries#### library(maptools) # Used for reading and manipulating map data library(ggplot2) # Used for plotting maps ####set working directory#### getwd() setwd(&quot;/home/silen/R _Data/Map&quot;) # Set the current working directory to the location of the map data ####loading map data(shape data)#### map &lt;- readShapePoly(&quot;countries_shp/countries.shp&quot;) # Read *.shp format map data, note that *.shp files must be placed together with other files like *.shx to be readable; otherwise, errors will occur. ####check loaded data Debugging part, used to check the loaded map data#### names(map) str(map$NAME) table(map$EU) ####Normal method, using geom_polygon / geom_path Using basic polygon and path methods for map plotting (unclear how to display epidemiological data)#### DB &lt;- fortify(map) # Important statement: Through this step, the imported map data is converted into a format that ggplot can recognize. p &lt;- ggplot(DB, aes(x=long,y=lat, group = group)) # Create a plot, initialize the graph, determine the use of the database and map longitude and latitude to the x and y axes; group=group ensures that the plotted graph is not chaotic. # World map, using geom_path instead of geom_polygon (from ggplot help documentation, unclear why) World &lt;- p + geom_polygon(color=&quot;white&quot;, fill = &quot;grey&quot;) + # Use polygon (polygon) to draw the map, set the fill color to grey and line color to white theme(panel.grid.major=element_blank(),# Theme() within are all style effect settings, mainly hiding x.y axis and corresponding text, background removal, grid removal, panel.background=element_blank(), axis.title.x=element_blank(), axis.title.y=element_blank(), axis.text.x=element_blank(), axis.text.y=element_blank(), axis.line=element_blank(), axis.ticks=element_blank())+ ggtitle(&quot;World&quot;)+ scale_y_continuous(breaks = (-2:2) * 30) +# Forgot what it does scale_x_continuous(breaks = (-4:4) * 45) + #coord_map(&quot;orthographic&quot;) # Orthographic view #coord_map(&quot;gilbert&quot;) # Ball shape view coord_map(&quot;lagrange&quot;) # Flat # coord_map() within are special coordinate axis mapping, see ggplot package documentation for details ####Epi method using geom_map Using the dedicated geom_map method to draw maps and display epidemiological data (heat map)#### # Load data and prepare for plotting Generate the required map data, load part at the beginning DB &lt;- fortify(map, region = &quot;NAME&quot;)# Transform map shape data into an object that ggplot can read # DB &lt;- transform(DB, id = iconv(id, from = &#39;GBK&#39;), code = iconv(code, from = &#39;GBK&#39;))# Transform coding from GBK head(DB) # Check the data situation names(DB)[c(1, 2)] = c(&quot;x&quot;, &quot;y&quot;)# Change the header of the data, for the purpose of using expand_limits() Change the names of variables in the data table as required by geom_map method # Prepare epidemiological data -&gt; crude death rate Prepare the epidemiological data, note the correspondence issue mor &lt;- read.csv(&quot;mapRate.csv&quot;,head =T, sep =&quot;,&quot;) # Read epidemiological data from *.csv file epi &lt;- data.frame(id = unique(sort(DB$id)))# Use epi data frame to save epidemiological data, obtain all region (or location) names from DB (map data) epi &lt;- merge(epi, mor, by.x= &quot;id&quot;, all.x = T)# Merge region names and corresponding epidemiological data based on id variable # write.csv(epi, &quot;1234.csv&quot;) # Debugging use # epi p1 &lt;- ggplot(epi, fill = &quot;#595959&quot;) + geom_map(aes(map_id = id, fill = rate ),# Use geom_map to draw the map and display data colour = &quot;white&quot;, map = DB) + expand_limits(DB) +# Important statement: Without this statement, the map cannot be drawn. The meaning of the statement is pending investigation # Changing theme, make background blank/transparent theme(panel.grid.major = element_blank(),# Theme settings, mainly hiding x.y axis and removing background, grid to make background transparent panel.background = element_blank(), plot.background = element_rect(fill = &quot;transparent&quot;,colour = NA), legend.background = element_rect(fill = &quot;transparent&quot;,colour = NA), legend.title = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.line = element_blank(), axis.ticks = element_blank())+ # ggtitle(&quot;World&quot;) + # Title of plot scale_fill_gradient(high = &quot;#F70909&quot;, low = &quot;#E99799&quot;) + # Set heat map color scale_y_continuous(breaks = (-2:2) * 30) +# Meaning unclear scale_x_continuous(breaks = (-4:4) * 45) + # coord_map(&quot;orthographic&quot;) # coord_map(&quot;gilbert&quot;) # Ball coord_map(&quot;lagrange&quot;) # Flat# Coordinate setting ####print plot Output image, only used when a transparent background image is needed#### png(&#39;world.png&#39;,width=600,height=600,units=&quot;px&quot;,bg = &quot;transparent&quot;)# Set name, size, size unit and set background to transparent print (p1)# Output p1 dev.off()# Indicates completion of output","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"}]},{"title":"R Training Notes","slug":"R培训笔记","date":"2018-07-29T00:35:55.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2018/07/29/R培训笔记/","permalink":"https://silenwang.github.io/en/2018/07/29/R%E5%9F%B9%E8%AE%AD%E7%AC%94%E8%AE%B0/","excerpt":"Graduate training on R at the Guangdong Provincial CDC was my first serious programming course.","text":"Graduate training on R at the Guangdong Provincial CDC was my first serious programming course. R Training Notes SummaryOverview The training lasted three days and covered five main topics: Introduction to R language and basic concepts, operations Using R to draw statistical graphs (graph &amp; ggplot2) Introduction to time series analysis and the use of Generalized Linear Models (GLM) and Generalized Additive Models (GAM) Use of Distributed Lag Nonlinear Models Meta-analysis in R Introduction to R Language and Basic Concepts, Operations R Features Pros: Free, rich features, fast updates, process visualization, beautiful plots Cons: Difficult to master, inconsistent, requires time to learn, slow (can be optimized with specific packages) Software Installation R and RStudio can be downloaded from the official website Packages can be installed manually or through the software itself Functions in R need to load the corresponding package before use, and help can only be viewed after loading. Software Interface Introduction (omitted)Common R Syntax Case-sensitive Statements are composed of functions and assignments All symbols must be half-width English # is used for comments, #text#### can be indexed (script mode) No special symbol at the end of each line; if a statement is incomplete, the program will continue to execute on the next line until it is complete &lt;- is the assignment operator, e.g., x&lt;-3, = can replace it == is the logical equality operator, &amp; for AND, | for OR, &gt;= greater than or equal to, &lt;= less than or equal to Single and double quotes are mostly interchangeable There are two equivalent ways to express paths: C:\\\\R\\\\Data is equivalent to C:/R/Data; R has poor support for Chinese,尽量 use full English paths $ can be used to extract variables within an object; after extraction, the variable can be assigned or modified. [] can be used to extract subsets of a data frame ! is the negation symbol Setting Working Directory &amp; Data Reading&#x2F;Writing Directory:getwd(&quot;path&quot;)/setwd(&quot;path&quot;) Data Import: Excel (only supports .csv): read.csv(&quot;path+filename, header= , sep=&quot;,&quot;) txt: read.table(&quot;path+filename&quot;, header= , ) SPSS (.sav): read.spss(&quot;path+filename&quot;, use.value.labels= ) SAS (.sas7bdat): read.sas7bdat(&quot;path+filename&quot;) R (.rda): load(&quot;path+filename&quot;) Direct read: read.table(textConnection(&quot;content&quot;), header=, sep=) Data Export R: save(对象, file=&quot;path+filename&quot;) Excel (.csv): write.csv(对象, file=&quot;path+filename&quot;) txt: write.table(对象, file=&quot;test.txt&quot;,sep = &quot;delimiter&quot;) Viewing Objects (Data) Multiple functions can be used to view already loaded (created) objects, each function has its own focus. View(对象) opens a window on the left to display object&#x2F;data information; head(对象, 条目数) displays the first n rows of data in the console; if no number is set, it defaults to 6 rows; tail(对象, 条目数) displays the last n rows of data in the console; if no number is set, it defaults to 6 rows; names(对象) displays all variable names (headers) of the object in the console. str(对象) displays the number of entries and variables of the object, as well as each variable’s name, type, and the first 10 values. dim(对象) displays the number of rows and columns of the object Statistical Description of Objects (Data) Function basic form Function purpose Notes summary(对象) Displays maximum, minimum, arithmetic mean, median, quartile range for numerical variables; Counts for categorical variables; If missing values exist, displays the number of missing values. Use rm.na= parameter to automatically remove missing values when calculating corresponding statistics. Simple Operations on Objects (Data) Function expression Operation effect Notes names(data)&lt;-c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;) Renames the first three variables of data frame data to 1, 2, 3 If there are more than three variables in the data frame, the names of the variables after the third will be lost; if too many names are assigned, it cannot be executed data=rename(data,c(&quot;old1&quot;=&quot;new1&quot;, &quot;old2&quot;=&quot;new2&quot;)) Replaces specified variable names in data frame data rename() is a function from the plyr package as.date(data)as.numeric(data)as.character(data)as.factor(data)etc. Converts specified variables to types, after conversion, can be assigned to an object subset(data, select=c(v1,v2))subset(data, v1==n)data[row,colum] Extracts specified rows&#x2F;columns from data frame Whether the two methods are completely equivalent is unknown; t[] defaults to taking rows data$v=data$v1+test$v2 Performs calculations on existing data in data frame and writes it as a new variable substr(strData,1,3) Extracts a specified number of characters from a string variable mutate(data,) Simplifies code by performing multiple operations simultaneously on data frame data The main function of mutate() is to simplify code join(data1, data2, by= ,type=) Merges different data frames according to certain standards join() function does not require sorting of the table Using R to Draw Statistical Graphs Two common graph drawing packages in R are: graph (built-in): Basic plotting ggplot2 (needs installation): Advanced plotting, high customization, beautiful finished products","categories":[{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"}],"tags":[{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"}]},{"title":"Talse of Destiny 2","slug":"宿命传说2","date":"2016-07-22T18:50:07.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2016/07/22/宿命传说2/","permalink":"https://silenwang.github.io/en/2016/07/22/%E5%AE%BF%E5%91%BD%E4%BC%A0%E8%AF%B42/","excerpt":"","text":"I had a cold with allergies… I stayed home for two and a half days, not doing anything. I finally got to play Destiny 2 (PSP version)… As expected, playing games is more motivating than anything else for me… I knew this game when I was in elementary school… Back then, there were still many TV programs introducing video games, and convenience stores also sold various game strategy magazines. I remember seeing it on at least two TV shows and three magazines, and they all had very good reviews of the game… However, I’m not a console player (and couldn’t have been back then, too expensive…), so for such a big game, I could only feel envious… But thanks to technological progress, 14 years later, I finally got the chance to play this game! But maybe it’s true that high expectations lead to disappointment… This game is really terrible in many aspects… Firstly, the graphics… Should I say people are changing too fast? Back then when I watched the opening animation, I thought it was still quite good. But now, looking at it again, these character portraits look so uncomfortable… Huge eyes… Thin body and limbs… And especially the protagonist’s clothes… It’s really a bit… Then there’s the character setting… Mainly for the protagonist… Just like most people’s feelings, he’s a typical middle school boy with a big ego… Ah… Am I really grown up and can’t face my past self? &#x3D; &#x3D;||| Next is the map and some puzzles… Especially the world map. Honestly, at first, I thought the game’s guidance was quite good, marking target locations on the mini-map. However, I found that apart from the Lang ruins where you can walk straight there, almost all other locations require detours to reach! And it’s impossible to see how to get there from the mini-map! So every time I go to a new location, I have to run around everywhere to find my way, and during the detour process, I have to kill countless monsters… Some trigger points are really not obvious enough… This also leads to running around and killing countless monsters before realizing how to do it… Finally, there’s the battle… At first, when I played, I thought this battle system was quite interesting… But later, the enemies became more disgusting… The front-line enemies are difficult to knock down, and the back-line crystal chanting is incredibly fast. If you cast a higher-level crystal spell, at least two will fall… Trying to use a control character to directly go around and interrupt it, that person mostly jumps in and doesn’t come back… In the end, I can only avoid disgusting combinations by not fighting them… Overall… From a game system perspective… Personally, this is a very tiring game to play… Regarding the plot, due to the protagonist being too middle school boyish, repeatedly crossing time with various illogicalities, villains’ IQ hard points, and finally forcing an unreasonable reunion at the end, it can’t add anything to this game either… So for me, beating this game is just eliminating my residual desire. As for playing again or recommending it to others… Forget it…","categories":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/categories/Gaming/"}],"tags":[{"name":"Talse of Destiny 2","slug":"Talse-of-Destiny-2","permalink":"https://silenwang.github.io/en/tags/Talse-of-Destiny-2/"},{"name":"Black History Plan","slug":"Black-History-Plan","permalink":"https://silenwang.github.io/en/tags/Black-History-Plan/"}]},{"title":"Melody's Escape","slug":"Melody-s-Escape","date":"2016-06-05T17:06:41.000Z","updated":"2026-02-10T14:03:13.684Z","comments":true,"path":"2016/06/05/Melody-s-Escape/","permalink":"https://silenwang.github.io/en/2016/06/05/Melody-s-Escape/","excerpt":"","text":"I don’t know where the original song came from, but I heard it while playing Melody’s Escape… One of the main features of this game is music selection. The official version only comes with 5 songs (including this Regain Control), but you can choose any music you have (supporting MP3s, WMAs, iTunes, FLAC formats, etc.). Running through your favorite ACG songs one by one is quite enjoyable~ For fans of video games, Melody’s Escape should be relatively easy. There are only eight keys in total… If I were a handless beginner like me when first playing a video game, starting with Medium (four keys) would be easier to get the hang of. Once both hands have some feel for it, you can try the eight-key version. I think it’s best to start with songs that have strong rhythm if you’re new to the game. Most of the keys are on the beat points in these songs, making them easier to press. Personally, I played for about a week and found that relatively simple and enjoyable songs were: AlieZ, βios, X.U, Hero’s Come Back, only my railgun, hollow World The only regret is that this game is currently a single-player game. I wonder if the author will add local multiplayer mode in the future… Steam link","categories":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/categories/Gaming/"}],"tags":[{"name":"Video game","slug":"Video-game","permalink":"https://silenwang.github.io/en/tags/Video-game/"},{"name":"Melody's Escape","slug":"Melody-s-Escape","permalink":"https://silenwang.github.io/en/tags/Melody-s-Escape/"},{"name":"Idle game","slug":"Idle-game","permalink":"https://silenwang.github.io/en/tags/Idle-game/"},{"name":"Black history plan","slug":"Black-history-plan","permalink":"https://silenwang.github.io/en/tags/Black-history-plan/"}]},{"title":"Bilibili - ( ゜- ゜)つロ Cheers~","slug":"小电视鼠标垫","date":"2016-02-21T16:28:08.000Z","updated":"2026-02-10T14:03:13.689Z","comments":true,"path":"2016/02/21/小电视鼠标垫/","permalink":"https://silenwang.github.io/en/2016/02/21/%E5%B0%8F%E7%94%B5%E8%A7%86%E9%BC%A0%E6%A0%87%E5%9E%AB/","excerpt":"","text":"Ah, the mousepad arrived yesterday… This is my first time buying a Bilibili peripheral… I received a big box and was fantasizing about receiving a small TV along with it…. Of course, reality is always cruel… After all, I just bought a mousepad &#x3D; &#x3D;|||| But seeing the实物 after opening the box still made me exclaim, it’s really huge!!!!! The effect on my desk is quite nice~ It makes me feel like 2233 and the small TV can sell for a year 2333333 I happily shared the photo with my friends, but his response &#x3D; &#x3D;||||……… Well… I wish you and your girlfriend a happy Lantern Festival &#x3D; &#x3D;|||||","categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"}],"tags":[{"name":"Bilibili Merchandise","slug":"Bilibili-Merchandise","permalink":"https://silenwang.github.io/en/tags/Bilibili-Merchandise/"},{"name":"Small TV","slug":"Small-TV","permalink":"https://silenwang.github.io/en/tags/Small-TV/"},{"name":"Mechanical Heart Desk Mat","slug":"Mechanical-Heart-Desk-Mat","permalink":"https://silenwang.github.io/en/tags/Mechanical-Heart-Desk-Mat/"},{"name":"Extravagant Spending","slug":"Extravagant-Spending","permalink":"https://silenwang.github.io/en/tags/Extravagant-Spending/"}]},{"title":"Transistor","slug":"晶体管","date":"2016-01-01T19:44:34.000Z","updated":"2026-02-10T14:03:13.690Z","comments":true,"path":"2016/01/01/晶体管/","permalink":"https://silenwang.github.io/en/2016/01/01/%E6%99%B6%E4%BD%93%E7%AE%A1/","excerpt":"This is a very distinctive game with stunning visuals. It’s a strategy game wrapped in an ARPG skin. The story narrative is a bit like Souls games (if you don’t explore the hidden information, you won’t know what it’s about). I remember this being the first game I bought on Steam when the region was just opened (it took a few days to arrive), and that’s how I started following the development team. Later, I also completed Bastion and later Pyre (I haven’t opened Pyre yet…).","text":"This is a very distinctive game with stunning visuals. It’s a strategy game wrapped in an ARPG skin. The story narrative is a bit like Souls games (if you don’t explore the hidden information, you won’t know what it’s about). I remember this being the first game I bought on Steam when the region was just opened (it took a few days to arrive), and that’s how I started following the development team. Later, I also completed Bastion and later Pyre (I haven’t opened Pyre yet…).","categories":[{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/categories/Gaming/"}],"tags":[{"name":"steam","slug":"steam","permalink":"https://silenwang.github.io/en/tags/steam/"},{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/tags/Gaming/"},{"name":"Transistor","slug":"Transistor","permalink":"https://silenwang.github.io/en/tags/Transistor/"},{"name":"transistor","slug":"transistor","permalink":"https://silenwang.github.io/en/tags/transistor/"}]}],"categories":[{"name":"Others","slug":"Others","permalink":"https://silenwang.github.io/en/categories/Others/"},{"name":"Coding","slug":"Coding","permalink":"https://silenwang.github.io/en/categories/Coding/"},{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/categories/Bioinformatics/"},{"name":"Other","slug":"Other","permalink":"https://silenwang.github.io/en/categories/Other/"},{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/categories/Gaming/"},{"name":"Script","slug":"Script","permalink":"https://silenwang.github.io/en/categories/Script/"},{"name":"Statistic","slug":"Statistic","permalink":"https://silenwang.github.io/en/categories/Statistic/"}],"tags":[{"name":"pixi","slug":"pixi","permalink":"https://silenwang.github.io/en/tags/pixi/"},{"name":"rattler-build","slug":"rattler-build","permalink":"https://silenwang.github.io/en/tags/rattler-build/"},{"name":"conda-build","slug":"conda-build","permalink":"https://silenwang.github.io/en/tags/conda-build/"},{"name":"Go","slug":"Go","permalink":"https://silenwang.github.io/en/tags/Go/"},{"name":"devssh","slug":"devssh","permalink":"https://silenwang.github.io/en/tags/devssh/"},{"name":"Git","slug":"Git","permalink":"https://silenwang.github.io/en/tags/Git/"},{"name":"Version Control","slug":"Version-Control","permalink":"https://silenwang.github.io/en/tags/Version-Control/"},{"name":"cherry-pick","slug":"cherry-pick","permalink":"https://silenwang.github.io/en/tags/cherry-pick/"},{"name":"Devpod","slug":"Devpod","permalink":"https://silenwang.github.io/en/tags/Devpod/"},{"name":"Open Source Maintenance","slug":"Open-Source-Maintenance","permalink":"https://silenwang.github.io/en/tags/Open-Source-Maintenance/"},{"name":"Community Fork","slug":"Community-Fork","permalink":"https://silenwang.github.io/en/tags/Community-Fork/"},{"name":"Project Sustainability","slug":"Project-Sustainability","permalink":"https://silenwang.github.io/en/tags/Project-Sustainability/"},{"name":"Docker Compose","slug":"Docker-Compose","permalink":"https://silenwang.github.io/en/tags/Docker-Compose/"},{"name":"Open Source Software","slug":"Open-Source-Software","permalink":"https://silenwang.github.io/en/tags/Open-Source-Software/"},{"name":"aider","slug":"aider","permalink":"https://silenwang.github.io/en/tags/aider/"},{"name":"opencode","slug":"opencode","permalink":"https://silenwang.github.io/en/tags/opencode/"},{"name":"AI programming","slug":"AI-programming","permalink":"https://silenwang.github.io/en/tags/AI-programming/"},{"name":"code generation","slug":"code-generation","permalink":"https://silenwang.github.io/en/tags/code-generation/"},{"name":"remote development","slug":"remote-development","permalink":"https://silenwang.github.io/en/tags/remote-development/"},{"name":"devpod","slug":"devpod","permalink":"https://silenwang.github.io/en/tags/devpod/"},{"name":"openhands","slug":"openhands","permalink":"https://silenwang.github.io/en/tags/openhands/"},{"name":"DeepSeek","slug":"DeepSeek","permalink":"https://silenwang.github.io/en/tags/DeepSeek/"},{"name":"automation tools","slug":"automation-tools","permalink":"https://silenwang.github.io/en/tags/automation-tools/"},{"name":"conda-forge","slug":"conda-forge","permalink":"https://silenwang.github.io/en/tags/conda-forge/"},{"name":"Bioconda","slug":"Bioconda","permalink":"https://silenwang.github.io/en/tags/Bioconda/"},{"name":"singler-py","slug":"singler-py","permalink":"https://silenwang.github.io/en/tags/singler-py/"},{"name":"biocutils","slug":"biocutils","permalink":"https://silenwang.github.io/en/tags/biocutils/"},{"name":"biocframe","slug":"biocframe","permalink":"https://silenwang.github.io/en/tags/biocframe/"},{"name":"summarizedexperiment","slug":"summarizedexperiment","permalink":"https://silenwang.github.io/en/tags/summarizedexperiment/"},{"name":"singlecellexperiment","slug":"singlecellexperiment","permalink":"https://silenwang.github.io/en/tags/singlecellexperiment/"},{"name":"package management","slug":"package-management","permalink":"https://silenwang.github.io/en/tags/package-management/"},{"name":"dependency resolution","slug":"dependency-resolution","permalink":"https://silenwang.github.io/en/tags/dependency-resolution/"},{"name":"grayskull","slug":"grayskull","permalink":"https://silenwang.github.io/en/tags/grayskull/"},{"name":"conda","slug":"conda","permalink":"https://silenwang.github.io/en/tags/conda/"},{"name":"tree_sitter_languages","slug":"tree-sitter-languages","permalink":"https://silenwang.github.io/en/tags/tree-sitter-languages/"},{"name":"PR","slug":"PR","permalink":"https://silenwang.github.io/en/tags/PR/"},{"name":"feedstock","slug":"feedstock","permalink":"https://silenwang.github.io/en/tags/feedstock/"},{"name":"Python","slug":"Python","permalink":"https://silenwang.github.io/en/tags/Python/"},{"name":"Word Cloud","slug":"Word-Cloud","permalink":"https://silenwang.github.io/en/tags/Word-Cloud/"},{"name":"Literature Management","slug":"Literature-Management","permalink":"https://silenwang.github.io/en/tags/Literature-Management/"},{"name":"Data Visualization","slug":"Data-Visualization","permalink":"https://silenwang.github.io/en/tags/Data-Visualization/"},{"name":"AI","slug":"AI","permalink":"https://silenwang.github.io/en/tags/AI/"},{"name":"Automation","slug":"Automation","permalink":"https://silenwang.github.io/en/tags/Automation/"},{"name":"Pixi","slug":"Pixi","permalink":"https://silenwang.github.io/en/tags/Pixi/"},{"name":"Development Environment","slug":"Development-Environment","permalink":"https://silenwang.github.io/en/tags/Development-Environment/"},{"name":"RenderCV","slug":"RenderCV","permalink":"https://silenwang.github.io/en/tags/RenderCV/"},{"name":"Container","slug":"Container","permalink":"https://silenwang.github.io/en/tags/Container/"},{"name":"Aider","slug":"Aider","permalink":"https://silenwang.github.io/en/tags/Aider/"},{"name":"qwen-image-edit","slug":"qwen-image-edit","permalink":"https://silenwang.github.io/en/tags/qwen-image-edit/"},{"name":"wan2.5-i2i-preview","slug":"wan2-5-i2i-preview","permalink":"https://silenwang.github.io/en/tags/wan2-5-i2i-preview/"},{"name":"Image Editing","slug":"Image-Editing","permalink":"https://silenwang.github.io/en/tags/Image-Editing/"},{"name":"git","slug":"git","permalink":"https://silenwang.github.io/en/tags/git/"},{"name":"docker","slug":"docker","permalink":"https://silenwang.github.io/en/tags/docker/"},{"name":"devcontainer","slug":"devcontainer","permalink":"https://silenwang.github.io/en/tags/devcontainer/"},{"name":"Dependency Management","slug":"Dependency-Management","permalink":"https://silenwang.github.io/en/tags/Dependency-Management/"},{"name":"Data Science","slug":"Data-Science","permalink":"https://silenwang.github.io/en/tags/Data-Science/"},{"name":"Bioinformatics","slug":"Bioinformatics","permalink":"https://silenwang.github.io/en/tags/Bioinformatics/"},{"name":"Cross‑Platform","slug":"Cross‑Platform","permalink":"https://silenwang.github.io/en/tags/Cross%E2%80%91Platform/"},{"name":"taipy","slug":"taipy","permalink":"https://silenwang.github.io/en/tags/taipy/"},{"name":"callback","slug":"callback","permalink":"https://silenwang.github.io/en/tags/callback/"},{"name":"html","slug":"html","permalink":"https://silenwang.github.io/en/tags/html/"},{"name":"github action","slug":"github-action","permalink":"https://silenwang.github.io/en/tags/github-action/"},{"name":"CD","slug":"CD","permalink":"https://silenwang.github.io/en/tags/CD/"},{"name":"automated deployment","slug":"automated-deployment","permalink":"https://silenwang.github.io/en/tags/automated-deployment/"},{"name":"R","slug":"R","permalink":"https://silenwang.github.io/en/tags/R/"},{"name":"ggplot2","slug":"ggplot2","permalink":"https://silenwang.github.io/en/tags/ggplot2/"},{"name":"rasterization","slug":"rasterization","permalink":"https://silenwang.github.io/en/tags/rasterization/"},{"name":"version control","slug":"version-control","permalink":"https://silenwang.github.io/en/tags/version-control/"},{"name":"subtree","slug":"subtree","permalink":"https://silenwang.github.io/en/tags/subtree/"},{"name":"submodule","slug":"submodule","permalink":"https://silenwang.github.io/en/tags/submodule/"},{"name":"scattermap","slug":"scattermap","permalink":"https://silenwang.github.io/en/tags/scattermap/"},{"name":"Tampermonkey","slug":"Tampermonkey","permalink":"https://silenwang.github.io/en/tags/Tampermonkey/"},{"name":"script","slug":"script","permalink":"https://silenwang.github.io/en/tags/script/"},{"name":"golang","slug":"golang","permalink":"https://silenwang.github.io/en/tags/golang/"},{"name":"Debug","slug":"Debug","permalink":"https://silenwang.github.io/en/tags/Debug/"},{"name":"maftools","slug":"maftools","permalink":"https://silenwang.github.io/en/tags/maftools/"},{"name":"Single-cell","slug":"Single-cell","permalink":"https://silenwang.github.io/en/tags/Single-cell/"},{"name":"Open-source Software","slug":"Open-source-Software","permalink":"https://silenwang.github.io/en/tags/Open-source-Software/"},{"name":"Software Maintenance","slug":"Software-Maintenance","permalink":"https://silenwang.github.io/en/tags/Software-Maintenance/"},{"name":"RISC-V","slug":"RISC-V","permalink":"https://silenwang.github.io/en/tags/RISC-V/"},{"name":"SAS","slug":"SAS","permalink":"https://silenwang.github.io/en/tags/SAS/"},{"name":"Linux","slug":"Linux","permalink":"https://silenwang.github.io/en/tags/Linux/"},{"name":"hexo","slug":"hexo","permalink":"https://silenwang.github.io/en/tags/hexo/"},{"name":"Volantis","slug":"Volantis","permalink":"https://silenwang.github.io/en/tags/Volantis/"},{"name":"blog","slug":"blog","permalink":"https://silenwang.github.io/en/tags/blog/"},{"name":"internationalization","slug":"internationalization","permalink":"https://silenwang.github.io/en/tags/internationalization/"},{"name":"FydeOS","slug":"FydeOS","permalink":"https://silenwang.github.io/en/tags/FydeOS/"},{"name":"podman","slug":"podman","permalink":"https://silenwang.github.io/en/tags/podman/"},{"name":"Bioconductor","slug":"Bioconductor","permalink":"https://silenwang.github.io/en/tags/Bioconductor/"},{"name":"dependency management","slug":"dependency-management","permalink":"https://silenwang.github.io/en/tags/dependency-management/"},{"name":"scanpy","slug":"scanpy","permalink":"https://silenwang.github.io/en/tags/scanpy/"},{"name":"loom","slug":"loom","permalink":"https://silenwang.github.io/en/tags/loom/"},{"name":"adata","slug":"adata","permalink":"https://silenwang.github.io/en/tags/adata/"},{"name":"Single-cell analysis","slug":"Single-cell-analysis","permalink":"https://silenwang.github.io/en/tags/Single-cell-analysis/"},{"name":"Differential abundance analysis","slug":"Differential-abundance-analysis","permalink":"https://silenwang.github.io/en/tags/Differential-abundance-analysis/"},{"name":"Milo","slug":"Milo","permalink":"https://silenwang.github.io/en/tags/Milo/"},{"name":"DevPod","slug":"DevPod","permalink":"https://silenwang.github.io/en/tags/DevPod/"},{"name":"DevOps","slug":"DevOps","permalink":"https://silenwang.github.io/en/tags/DevOps/"},{"name":"Docker","slug":"Docker","permalink":"https://silenwang.github.io/en/tags/Docker/"},{"name":"Podman","slug":"Podman","permalink":"https://silenwang.github.io/en/tags/Podman/"},{"name":"Debugging","slug":"Debugging","permalink":"https://silenwang.github.io/en/tags/Debugging/"},{"name":"Typescript","slug":"Typescript","permalink":"https://silenwang.github.io/en/tags/Typescript/"},{"name":"Javascript","slug":"Javascript","permalink":"https://silenwang.github.io/en/tags/Javascript/"},{"name":"Frontend Development","slug":"Frontend-Development","permalink":"https://silenwang.github.io/en/tags/Frontend-Development/"},{"name":"Monitoring","slug":"Monitoring","permalink":"https://silenwang.github.io/en/tags/Monitoring/"},{"name":"Uptime-Kuma","slug":"Uptime-Kuma","permalink":"https://silenwang.github.io/en/tags/Uptime-Kuma/"},{"name":"Website Maintenance","slug":"Website-Maintenance","permalink":"https://silenwang.github.io/en/tags/Website-Maintenance/"},{"name":"Service Monitoring","slug":"Service-Monitoring","permalink":"https://silenwang.github.io/en/tags/Service-Monitoring/"},{"name":"circlize","slug":"circlize","permalink":"https://silenwang.github.io/en/tags/circlize/"},{"name":"Genome","slug":"Genome","permalink":"https://silenwang.github.io/en/tags/Genome/"},{"name":"Mutation","slug":"Mutation","permalink":"https://silenwang.github.io/en/tags/Mutation/"},{"name":"CNV","slug":"CNV","permalink":"https://silenwang.github.io/en/tags/CNV/"},{"name":"ffmpeg","slug":"ffmpeg","permalink":"https://silenwang.github.io/en/tags/ffmpeg/"},{"name":"Taipy","slug":"Taipy","permalink":"https://silenwang.github.io/en/tags/Taipy/"},{"name":"chromeos","slug":"chromeos","permalink":"https://silenwang.github.io/en/tags/chromeos/"},{"name":"terminal","slug":"terminal","permalink":"https://silenwang.github.io/en/tags/terminal/"},{"name":"libapp","slug":"libapp","permalink":"https://silenwang.github.io/en/tags/libapp/"},{"name":"terminal app","slug":"terminal-app","permalink":"https://silenwang.github.io/en/tags/terminal-app/"},{"name":"SteamOS","slug":"SteamOS","permalink":"https://silenwang.github.io/en/tags/SteamOS/"},{"name":"Bazzite","slug":"Bazzite","permalink":"https://silenwang.github.io/en/tags/Bazzite/"},{"name":"Gaming","slug":"Gaming","permalink":"https://silenwang.github.io/en/tags/Gaming/"},{"name":"Steam","slug":"Steam","permalink":"https://silenwang.github.io/en/tags/Steam/"},{"name":"Hangzhou","slug":"Hangzhou","permalink":"https://silenwang.github.io/en/tags/Hangzhou/"},{"name":"career","slug":"career","permalink":"https://silenwang.github.io/en/tags/career/"},{"name":"job-hopping","slug":"job-hopping","permalink":"https://silenwang.github.io/en/tags/job-hopping/"},{"name":"bioinformatics","slug":"bioinformatics","permalink":"https://silenwang.github.io/en/tags/bioinformatics/"},{"name":"openfyde","slug":"openfyde","permalink":"https://silenwang.github.io/en/tags/openfyde/"},{"name":"theia","slug":"theia","permalink":"https://silenwang.github.io/en/tags/theia/"},{"name":"fydeos","slug":"fydeos","permalink":"https://silenwang.github.io/en/tags/fydeos/"},{"name":"github","slug":"github","permalink":"https://silenwang.github.io/en/tags/github/"},{"name":"python","slug":"python","permalink":"https://silenwang.github.io/en/tags/python/"},{"name":"pytest","slug":"pytest","permalink":"https://silenwang.github.io/en/tags/pytest/"},{"name":"playwright","slug":"playwright","permalink":"https://silenwang.github.io/en/tags/playwright/"},{"name":"linux","slug":"linux","permalink":"https://silenwang.github.io/en/tags/linux/"},{"name":"github actions","slug":"github-actions","permalink":"https://silenwang.github.io/en/tags/github-actions/"},{"name":"rlang","slug":"rlang","permalink":"https://silenwang.github.io/en/tags/rlang/"},{"name":"object","slug":"object","permalink":"https://silenwang.github.io/en/tags/object/"},{"name":"box","slug":"box","permalink":"https://silenwang.github.io/en/tags/box/"},{"name":"fydetab duo","slug":"fydetab-duo","permalink":"https://silenwang.github.io/en/tags/fydetab-duo/"},{"name":"jupyter","slug":"jupyter","permalink":"https://silenwang.github.io/en/tags/jupyter/"},{"name":"monocle3","slug":"monocle3","permalink":"https://silenwang.github.io/en/tags/monocle3/"},{"name":"singlecell","slug":"singlecell","permalink":"https://silenwang.github.io/en/tags/singlecell/"},{"name":"xeus","slug":"xeus","permalink":"https://silenwang.github.io/en/tags/xeus/"},{"name":"Complaining","slug":"Complaining","permalink":"https://silenwang.github.io/en/tags/Complaining/"},{"name":"chromebook","slug":"chromebook","permalink":"https://silenwang.github.io/en/tags/chromebook/"},{"name":"sinularity","slug":"sinularity","permalink":"https://silenwang.github.io/en/tags/sinularity/"},{"name":"container","slug":"container","permalink":"https://silenwang.github.io/en/tags/container/"},{"name":"rstudio","slug":"rstudio","permalink":"https://silenwang.github.io/en/tags/rstudio/"},{"name":"singularity","slug":"singularity","permalink":"https://silenwang.github.io/en/tags/singularity/"},{"name":"codespace","slug":"codespace","permalink":"https://silenwang.github.io/en/tags/codespace/"},{"name":"chatGPT","slug":"chatGPT","permalink":"https://silenwang.github.io/en/tags/chatGPT/"},{"name":"flask","slug":"flask","permalink":"https://silenwang.github.io/en/tags/flask/"},{"name":"DingTalk","slug":"DingTalk","permalink":"https://silenwang.github.io/en/tags/DingTalk/"},{"name":"Daily","slug":"Daily","permalink":"https://silenwang.github.io/en/tags/Daily/"},{"name":"GitHub","slug":"GitHub","permalink":"https://silenwang.github.io/en/tags/GitHub/"},{"name":"Playing games","slug":"Playing-games","permalink":"https://silenwang.github.io/en/tags/Playing-games/"},{"name":"Idle Gaming","slug":"Idle-Gaming","permalink":"https://silenwang.github.io/en/tags/Idle-Gaming/"},{"name":"qq","slug":"qq","permalink":"https://silenwang.github.io/en/tags/qq/"},{"name":"Pandas","slug":"Pandas","permalink":"https://silenwang.github.io/en/tags/Pandas/"},{"name":"nps","slug":"nps","permalink":"https://silenwang.github.io/en/tags/nps/"},{"name":"clash","slug":"clash","permalink":"https://silenwang.github.io/en/tags/clash/"},{"name":"2023","slug":"2023","permalink":"https://silenwang.github.io/en/tags/2023/"},{"name":"https","slug":"https","permalink":"https://silenwang.github.io/en/tags/https/"},{"name":"cloud","slug":"cloud","permalink":"https://silenwang.github.io/en/tags/cloud/"},{"name":"firewall","slug":"firewall","permalink":"https://silenwang.github.io/en/tags/firewall/"},{"name":"iptables","slug":"iptables","permalink":"https://silenwang.github.io/en/tags/iptables/"},{"name":"Graph Database","slug":"Graph-Database","permalink":"https://silenwang.github.io/en/tags/Graph-Database/"},{"name":"Marp","slug":"Marp","permalink":"https://silenwang.github.io/en/tags/Marp/"},{"name":"papers","slug":"papers","permalink":"https://silenwang.github.io/en/tags/papers/"},{"name":"zotero","slug":"zotero","permalink":"https://silenwang.github.io/en/tags/zotero/"},{"name":"ggplot","slug":"ggplot","permalink":"https://silenwang.github.io/en/tags/ggplot/"},{"name":"ggpubr","slug":"ggpubr","permalink":"https://silenwang.github.io/en/tags/ggpubr/"},{"name":"survminer","slug":"survminer","permalink":"https://silenwang.github.io/en/tags/survminer/"},{"name":"ploty","slug":"ploty","permalink":"https://silenwang.github.io/en/tags/ploty/"},{"name":"dash","slug":"dash","permalink":"https://silenwang.github.io/en/tags/dash/"},{"name":"Scrapy","slug":"Scrapy","permalink":"https://silenwang.github.io/en/tags/Scrapy/"},{"name":"Crawler","slug":"Crawler","permalink":"https://silenwang.github.io/en/tags/Crawler/"},{"name":"graphviz","slug":"graphviz","permalink":"https://silenwang.github.io/en/tags/graphviz/"},{"name":"drawio","slug":"drawio","permalink":"https://silenwang.github.io/en/tags/drawio/"},{"name":"ssh","slug":"ssh","permalink":"https://silenwang.github.io/en/tags/ssh/"},{"name":"openssh","slug":"openssh","permalink":"https://silenwang.github.io/en/tags/openssh/"},{"name":"manjaro","slug":"manjaro","permalink":"https://silenwang.github.io/en/tags/manjaro/"},{"name":"mongoDB","slug":"mongoDB","permalink":"https://silenwang.github.io/en/tags/mongoDB/"},{"name":"pymongo","slug":"pymongo","permalink":"https://silenwang.github.io/en/tags/pymongo/"},{"name":"flasgger","slug":"flasgger","permalink":"https://silenwang.github.io/en/tags/flasgger/"},{"name":"android","slug":"android","permalink":"https://silenwang.github.io/en/tags/android/"},{"name":"phone","slug":"phone","permalink":"https://silenwang.github.io/en/tags/phone/"},{"name":"Mokee","slug":"Mokee","permalink":"https://silenwang.github.io/en/tags/Mokee/"},{"name":"mSINGS","slug":"mSINGS","permalink":"https://silenwang.github.io/en/tags/mSINGS/"},{"name":"Flask","slug":"Flask","permalink":"https://silenwang.github.io/en/tags/Flask/"},{"name":"gitee","slug":"gitee","permalink":"https://silenwang.github.io/en/tags/gitee/"},{"name":"Monster Hunter","slug":"Monster-Hunter","permalink":"https://silenwang.github.io/en/tags/Monster-Hunter/"},{"name":"Apktools","slug":"Apktools","permalink":"https://silenwang.github.io/en/tags/Apktools/"},{"name":"crack","slug":"crack","permalink":"https://silenwang.github.io/en/tags/crack/"},{"name":"Web Crawler","slug":"Web-Crawler","permalink":"https://silenwang.github.io/en/tags/Web-Crawler/"},{"name":"requests","slug":"requests","permalink":"https://silenwang.github.io/en/tags/requests/"},{"name":"selenium","slug":"selenium","permalink":"https://silenwang.github.io/en/tags/selenium/"},{"name":"snakemake","slug":"snakemake","permalink":"https://silenwang.github.io/en/tags/snakemake/"},{"name":"Playing Games","slug":"Playing-Games","permalink":"https://silenwang.github.io/en/tags/Playing-Games/"},{"name":"dockerfile","slug":"dockerfile","permalink":"https://silenwang.github.io/en/tags/dockerfile/"},{"name":"systemd","slug":"systemd","permalink":"https://silenwang.github.io/en/tags/systemd/"},{"name":"GPU Passthrough","slug":"GPU-Passthrough","permalink":"https://silenwang.github.io/en/tags/GPU-Passthrough/"},{"name":"Manjaro","slug":"Manjaro","permalink":"https://silenwang.github.io/en/tags/Manjaro/"},{"name":"kvm","slug":"kvm","permalink":"https://silenwang.github.io/en/tags/kvm/"},{"name":"Syncthing","slug":"Syncthing","permalink":"https://silenwang.github.io/en/tags/Syncthing/"},{"name":"githook","slug":"githook","permalink":"https://silenwang.github.io/en/tags/githook/"},{"name":"Decorators","slug":"Decorators","permalink":"https://silenwang.github.io/en/tags/Decorators/"},{"name":"qc","slug":"qc","permalink":"https://silenwang.github.io/en/tags/qc/"},{"name":"multiqc","slug":"multiqc","permalink":"https://silenwang.github.io/en/tags/multiqc/"},{"name":"machine learning","slug":"machine-learning","permalink":"https://silenwang.github.io/en/tags/machine-learning/"},{"name":"documentation","slug":"documentation","permalink":"https://silenwang.github.io/en/tags/documentation/"},{"name":"gitbook","slug":"gitbook","permalink":"https://silenwang.github.io/en/tags/gitbook/"},{"name":"mkdocs","slug":"mkdocs","permalink":"https://silenwang.github.io/en/tags/mkdocs/"},{"name":"bash","slug":"bash","permalink":"https://silenwang.github.io/en/tags/bash/"},{"name":"caddy","slug":"caddy","permalink":"https://silenwang.github.io/en/tags/caddy/"},{"name":"parallel computing","slug":"parallel-computing","permalink":"https://silenwang.github.io/en/tags/parallel-computing/"},{"name":"Structural Variants","slug":"Structural-Variants","permalink":"https://silenwang.github.io/en/tags/Structural-Variants/"},{"name":"Structural Variation","slug":"Structural-Variation","permalink":"https://silenwang.github.io/en/tags/Structural-Variation/"},{"name":"itertools","slug":"itertools","permalink":"https://silenwang.github.io/en/tags/itertools/"},{"name":"Parallel Computing","slug":"Parallel-Computing","permalink":"https://silenwang.github.io/en/tags/Parallel-Computing/"},{"name":"Multiprocessing","slug":"Multiprocessing","permalink":"https://silenwang.github.io/en/tags/Multiprocessing/"},{"name":"teamviewer","slug":"teamviewer","permalink":"https://silenwang.github.io/en/tags/teamviewer/"},{"name":"plugin","slug":"plugin","permalink":"https://silenwang.github.io/en/tags/plugin/"},{"name":"argparse","slug":"argparse","permalink":"https://silenwang.github.io/en/tags/argparse/"},{"name":"Argument Parsing","slug":"Argument-Parsing","permalink":"https://silenwang.github.io/en/tags/Argument-Parsing/"},{"name":"single cell","slug":"single-cell","permalink":"https://silenwang.github.io/en/tags/single-cell/"},{"name":"gogs","slug":"gogs","permalink":"https://silenwang.github.io/en/tags/gogs/"},{"name":"vps","slug":"vps","permalink":"https://silenwang.github.io/en/tags/vps/"},{"name":"steam","slug":"steam","permalink":"https://silenwang.github.io/en/tags/steam/"},{"name":"syntax","slug":"syntax","permalink":"https://silenwang.github.io/en/tags/syntax/"},{"name":"sge","slug":"sge","permalink":"https://silenwang.github.io/en/tags/sge/"},{"name":"adb","slug":"adb","permalink":"https://silenwang.github.io/en/tags/adb/"},{"name":"minicap","slug":"minicap","permalink":"https://silenwang.github.io/en/tags/minicap/"},{"name":"sqlite","slug":"sqlite","permalink":"https://silenwang.github.io/en/tags/sqlite/"},{"name":"deepin","slug":"deepin","permalink":"https://silenwang.github.io/en/tags/deepin/"},{"name":"de","slug":"de","permalink":"https://silenwang.github.io/en/tags/de/"},{"name":"Regular Expressions","slug":"Regular-Expressions","permalink":"https://silenwang.github.io/en/tags/Regular-Expressions/"},{"name":"Perl","slug":"Perl","permalink":"https://silenwang.github.io/en/tags/Perl/"},{"name":"Bash","slug":"Bash","permalink":"https://silenwang.github.io/en/tags/Bash/"},{"name":"linuxmint","slug":"linuxmint","permalink":"https://silenwang.github.io/en/tags/linuxmint/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://silenwang.github.io/en/tags/ubuntu/"},{"name":"Talse of Destiny 2","slug":"Talse-of-Destiny-2","permalink":"https://silenwang.github.io/en/tags/Talse-of-Destiny-2/"},{"name":"Black History Plan","slug":"Black-History-Plan","permalink":"https://silenwang.github.io/en/tags/Black-History-Plan/"},{"name":"Video game","slug":"Video-game","permalink":"https://silenwang.github.io/en/tags/Video-game/"},{"name":"Melody's Escape","slug":"Melody-s-Escape","permalink":"https://silenwang.github.io/en/tags/Melody-s-Escape/"},{"name":"Idle game","slug":"Idle-game","permalink":"https://silenwang.github.io/en/tags/Idle-game/"},{"name":"Black history plan","slug":"Black-history-plan","permalink":"https://silenwang.github.io/en/tags/Black-history-plan/"},{"name":"Bilibili Merchandise","slug":"Bilibili-Merchandise","permalink":"https://silenwang.github.io/en/tags/Bilibili-Merchandise/"},{"name":"Small TV","slug":"Small-TV","permalink":"https://silenwang.github.io/en/tags/Small-TV/"},{"name":"Mechanical Heart Desk Mat","slug":"Mechanical-Heart-Desk-Mat","permalink":"https://silenwang.github.io/en/tags/Mechanical-Heart-Desk-Mat/"},{"name":"Extravagant Spending","slug":"Extravagant-Spending","permalink":"https://silenwang.github.io/en/tags/Extravagant-Spending/"},{"name":"Transistor","slug":"Transistor","permalink":"https://silenwang.github.io/en/tags/Transistor/"},{"name":"transistor","slug":"transistor","permalink":"https://silenwang.github.io/en/tags/transistor/"}]}